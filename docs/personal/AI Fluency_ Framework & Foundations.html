<!DOCTYPE html>
<!-- saved from url=(0063)https://anthropic.skilljar.com/ai-fluency-framework-foundations -->
<html class="js no-htmlimports no-flash sizes no-proximity transferables adownload no-ambientlight no-applicationcache audio audioloop webaudio no-batteryapi no-battery-api no-lowbattery blobconstructor blob-constructor canvas canvasblending todataurljpeg todataurlpng todataurlwebp canvaswinding canvastext contenteditable no-contextmenu cookies cors customelements crypto getrandomvalues cssall cssanimations appearance backdropfilter backgroundblendmode backgroundcliptext bgpositionshorthand bgpositionxy bgrepeatround bgrepeatspace backgroundsize bgsizecover borderimage borderradius boxshadow boxsizing csscalc checked csschunit csscolumns csscolumns-width csscolumns-span csscolumns-fill csscolumns-gap csscolumns-rule csscolumns-rulecolor csscolumns-rulestyle csscolumns-rulewidth csscolumns-breakbefore csscolumns-breakafter csscolumns-breakinside no-cssgridlegacy cssgrid cubicbezierrange no-displayrunin no-display-runin displaytable display-table ellipsis cssescape cssexunit supports cssfilters flexbox flexboxlegacy no-flexboxtweener flexwrap fontface generatedcontent cssgradients no-hairline hsla cssinvalid lastchild cssmask mediaqueries multiplebgs nthchild objectfit object-fit opacity no-overflowscrolling csspointerevents csspositionsticky csspseudoanimations csstransitions no-csspseudotransitions cssreflections no-regions cssremunit cssresize rgba no-cssscrollbar scrollsnappoints shapes siblinggeneral subpixelfont target textalignlast textshadow csstransforms csstransformslevel2 csstransforms3d preserve3d userselect cssvalid no-cssvhunit cssvmaxunit cssvminunit cssvwunit willchange no-wrapflow customprotocolhandler customevent no-dart dataview classlist no-createelementattrs no-createelement-attrs dataset documentfragment hidden no-microdata mutationobserver passiveeventlisteners bdi datalistelem details outputelem picture progressbar meter ruby template no-time texttrackapi track unknownelements emoji es5array es5date es5function es5object strictmode es5string json es5syntax es5undefined es5 es6array es6collections no-contains generators es6math es6number es6object promises es6string devicemotion deviceorientation oninput eventlistener filereader filesystem no-capture fileinput fileinputdirectory formattribute formvalidation localizednumber placeholder no-requestautocomplete fullscreen gamepads geolocation hashchange no-hiddenscroll history no-ie8compat sandbox no-seamless srcdoc imgcrossorigin srcset inputformaction input-formaction inputformenctype input-formenctype inputformmethod no-inputformtarget no-input-formtarget inputsearchevent intl ligatures olreversed mathml hovermq pointermq messagechannel beacon no-lowbandwidth eventsource fetch xhrresponsetype xhrresponsetypearraybuffer xhrresponsetypeblob xhrresponsetypedocument xhrresponsetypejson xhrresponsetypetext xhr2 notification pagevisibility performance pointerevents pointerlock postmessage queryselector quotamanagement requestanimationframe raf scriptasync scriptdefer serviceworker speechrecognition speechsynthesis localstorage sessionstorage no-websqldatabase no-stylescoped svg svgasimg svgclippaths svgfilters svgforeignobject inlinesvg smil templatestrings textareamaxlength no-touchevents typedarrays unicode no-unicoderange bloburls urlparser urlsearchparams no-userdata vibrate video videocrossorigin videoloop videopreload no-vml no-webintents webanimations webgl peerconnection datachannel getusermedia websockets websocketsbinary no-framed sharedworkers webworkers no-xdomainrequest exiforientation apng webp webpalpha webpanimation webplossless no-jpeg2000 no-jpegxr webp-alpha webp-animation webp-lossless datauri audiopreload blobworkers dataworkers indexeddb csshyphens softhyphens softhyphensfind indexeddb-deletedatabase no-videoautoplay" lang="en-US" style=""><!--<![endif]--><head prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# skilljar: http://ogp.me/ns/fb/skilljar#"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    
        
    

    

    <script type="text/javascript" id="www-widgetapi-script" src="./AI Fluency_ Framework &amp; Foundations_files/www-widgetapi.js" async=""></script><script src="./AI Fluency_ Framework &amp; Foundations_files/sdk.js" async="" crossorigin="anonymous"></script><script id="facebook-jssdk" src="./AI Fluency_ Framework &amp; Foundations_files/sdk(1).js"></script><script id="twitter-wjs" src="./AI Fluency_ Framework &amp; Foundations_files/widgets.js"></script><script src="./AI Fluency_ Framework &amp; Foundations_files/iframe_api"></script><script async="" src="./AI Fluency_ Framework &amp; Foundations_files/analytics.js"></script><script type="importmap">
        {
            "imports": {
                "vue": "/static/js/vendor/vue/vue@3.4.15.min.3a7e0323bd7d.js",
                "vue-shadow-dom": "/static/js/vendor/vue/vue-shadow-dom@4.2.0.c6ed52f5c4de.mjs"
            }
        }
    </script>

    
        <link rel="search" type="application/opensearchdescription+xml" href="https://anthropic.skilljar.com/domain/open-search" title="Skilljar">
    
    
        <link rel="shortcut icon" href="https://cc.sj-cdn.net/instructor/4hdejjwplbrm-anthropic/themes/3gufixqhei80k/favicon.1749519148.ico">
    

    
    
        <link href="./AI Fluency_ Framework &amp; Foundations_files/css2" rel="stylesheet">
    
    <meta name="viewport" id="viewport" content="width=device-width, initial-scale=1.0">
    
        <meta name="global-title" content="Anthropic Courses">
    
    
    
    






    <meta property="og:title" content="AI Fluency: Framework &amp; Foundations">
    <meta name="twitter:title" content="AI Fluency: Framework &amp; Foundations">







    <meta property="og:description" content="Learn to collaborate with AI systems effectively, efficiently, ethically, and safely">
    <meta name="twitter:description" content="Learn to collaborate with AI systems effectively, efficiently, ethically, and safely">
    <meta name="description" content="Learn to collaborate with AI systems effectively, efficiently, ethically, and safely">






    <meta property="og:image" content="https://cc.sj-cdn.net/instructor/4hdejjwplbrm-anthropic/courses/17owe4fx9adox/promo-image.1753132690.svg">
    <meta name="twitter:image" content="https://cc.sj-cdn.net/instructor/4hdejjwplbrm-anthropic/courses/17owe4fx9adox/promo-image.1753132690.svg">






    <meta name="keywords" content="">





<meta property="fb:app_id" content="571325372910940">


    <meta property="og:url" content="https://anthropic.skilljar.com/ai-fluency-framework-foundations">



    <meta property="og:type" content="skilljar:online_course">



    <meta property="og:site_name" content="Anthropic">





<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@skilljar">


    
        <link rel="canonical" href="https://anthropic.skilljar.com/ai-fluency-framework-foundations">
    

    <title>AI Fluency: Framework &amp; Foundations</title>

    
    
        
        
            <link rel="stylesheet" href="./AI Fluency_ Framework &amp; Foundations_files/sj_course_platform_v2.1c1268e38895.css">
        
        
        <link href="./AI Fluency_ Framework &amp; Foundations_files/prism.47d40f251583.css" rel="stylesheet">
    

    
    
        
            
            <link rel="stylesheet" href="./AI Fluency_ Framework &amp; Foundations_files/3gufixqhei80k.css">
        
    

    
    

    
    

    <script src="./AI Fluency_ Framework &amp; Foundations_files/index.093d21a2d769.js" type="module" async=""></script>

    

    <script>
        window.SKILLJAR_DASHBOARD_GLOBALS = {
            TIME_ZONE: 'America/Los_Angeles',
        };

    </script>

    
    <script>
        
        window.renderElement = function(tagName, attributes, children) {
            var elem = document.createElement(tagName);
            if (attributes) {
              Object.keys(attributes).forEach(function (attributeKey) {
                elem[attributeKey] = attributes[attributeKey];
              });
            }

            if (children) {
              children
                .filter(function(child) {
                  return !!child;
                })
                .forEach(function(child) {
                  elem.appendChild(child);
                });
            }

            return elem;
        }
    </script>

    
        
        <script src="./AI Fluency_ Framework &amp; Foundations_files/modernizr.min.026065b74e62.js"></script>
    

    
    
        
            <script src="./AI Fluency_ Framework &amp; Foundations_files/jquery.min.8fb584de777d.js"></script>
        
    

    
        <script src="./AI Fluency_ Framework &amp; Foundations_files/jquery.min.8fb584de777d.js"></script>
    

    <script src="./AI Fluency_ Framework &amp; Foundations_files/prism.15b0a331cd32.js"></script>
    <script src="./AI Fluency_ Framework &amp; Foundations_files/sanitize-html.e6b3f066533c.js"></script>

    
        
            <script>
                console.sentryCaptureException = function() {};
                console.sentryCaptureMessage = function() {};
            </script>
        
    

    
        
            




<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');


    ga('create', {
        trackingId: 'UA\u002D40340039\u002D1',
        cookieDomain: 'auto',
        name: 'skilljarTracker',
        
        
    });



    
        ga('skilljarTracker.set', 'dimension1', 'false');
    
    
    ga('skilljarTracker.set', 'anonymizeIp', true);
    ga('skilljarTracker.send', 'pageview');

</script>




<!-- Google tag (gtag.js) -->

    <script async="" src="./AI Fluency_ Framework &amp; Foundations_files/js"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-9WXYK3PMB6', {
            'groups': 'SKILLJAR',
            'anonymize_ip': true,
            'cookie_domain': 'auto',
            
        });

        gtag('set', {
            'send_to': 'SKILLJAR',
            'dimension1': 'false'
        });
    </script>


        
    

    
        




 



<script>
    
    var skilljarThemeVersionMajor = 2;
    var isSkilljarTestUser = false;
    
    var isSkilljarFirstPageviewSinceAuth = false;
    
    var isSkilljarFirstPageviewSinceNewDomainMembership = false;
    

    
    
        var skilljarCourse = {
            id: '17owe4fx9adox',
            
                publishedCourseId: '3gyr99bvbs6cs',
                tags: [
                    
                ],
            
            title: 'AI Fluency: Framework \u0026 Foundations',
            short_description: 'Learn to collaborate with AI systems effectively, efficiently, ethically, and safely',
            long_description_html: '\u003Cp class\u003D\u0022paragraph\u002Dm tight\u0022\u003EAnthropic has partnered with academic experts Prof. Joseph Feller (University College Cork) and Prof. Rick Dakan (Ringling College) to launch an AI fluency course that teaches practical skills for effective, efficient, ethical, and safe AI interaction.\u003C/p\u003E\u000D\u000A\u003Cp class\u003D\u0022paragraph\u002Dm tight\u0022\u003EThis course has something for everyone, whether you\u0027re new to Claude or a seasoned AI practitioner.\u003C/p\u003E\u000D\u000A\u003Ch4 id\u003D\u0022about\u002Dthis\u002Dcourse\u0022 class\u003D\u0022Body_reading\u002Dcolumn__t7kGM display\u002Dsans\u002Dxs post\u002Dsubsection\u0022\u003EAbout this course\u003C/h4\u003E\u000D\u000A\u003Cul class\u003D\u0022Body_reading\u002Dcolumn__t7kGM paragraph\u002Dm post\u002Dtext\u0022\u003E\u000D\u000A\u003Cli\u003E\u003Ca href\u003D\u0022https://www\u002Dcdn.anthropic.com/7e9692bba414a91a562af2a64b7e99d7946de590.pdf\u0022 target\u003D\u0022_blank\u0022 rel\u003D\u0022noopener noreferrer\u0022\u003EAbout this course\u003C/a\u003E\u003C/li\u003E\u000D\u000A\u003Cli\u003E\u003Ca href\u003D\u0022https://www\u002Dcdn.anthropic.com/e5f2470543977625bf82bbf74cd7898f47da4ae1.pdf\u0022 target\u003D\u0022_blank\u0022 rel\u003D\u0022noopener noreferrer\u0022\u003EMeet your instructors\u003C/a\u003E\u003C/li\u003E\u000D\u000A\u003Cli\u003E\u003Ca href\u003D\u0022https://www\u002Dcdn.anthropic.com/0e37fa9da01fab7a5478a2194d352027794c1b89.pdf\u0022 target\u003D\u0022_blank\u0022 rel\u003D\u0022noopener noreferrer\u0022\u003EHow we used AI in building the course\u003C/a\u003E\u003C/li\u003E\u000D\u000A\u003C/ul\u003E\u000D\u000A\u003Ch4 id\u003D\u0022certificate\u002Dof\u002Dcompletion\u0022 class\u003D\u0022Body_reading\u002Dcolumn__t7kGM display\u002Dsans\u002Dxs post\u002Dsubsection\u0022\u003ECertificate of completion\u003C/h4\u003E\u000D\u000A\u003Cp\u003EAfter finishing the course, you will have the opportunity to take a final assessment and receive a certificate of completion.\u003C/p\u003E'
        };
    
    
    
    
    
    
    
    
    
    
    
        var skilljarUpcomingEvents = [];
    
</script>

    

    
    
      
        
      
    

    
        

<script>
    var sjlpPluralizationIndex = function (n) { return ((n != 1) ? 1 : 0) };
    var sjlpLanguagePack = {
    };
</script>

        
    <script>
        var sjlpStripeLanguageCode = 'en';
    </script>

    

    
    
        <meta property="og:type" content="website">
<meta property="og:url" content="https://anthropic.skilljar.com/">
<meta property="og:title" content="Anthropic Courses">
<meta property="og:description" content="Learn to build with Claude through Anthropic&#39;s comprehensive courses and training programs.">
<meta property="og:image" content="https://cdn.sanity.io/images/4zrzovbb/website/c4bd33e7c8e809a2f9a9a5896ee13961e2a738ec-2400x1260.png">
<meta property="og:image:width" content="2400">
<meta property="og:image:height" content="1260">
<meta property="og:site_name" content="Anthropic Courses">

<!-- Twitter Card -->
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:url" content="https://anthropic.skilljar.com/">
<meta name="twitter:title" content="Anthropic Courses">
<meta name="twitter:description" content="Learn to build with Claude AI
through Anthropic&#39;s comprehensive courses and training programs.">
<meta name="twitter:image" content="https://cdn.sanity.io/images/4zrzovbb/website/c4bd33e7c8e809a2f9a9a5896ee13961e2a738ec-2400x1260.png">

<style>
@charset "UTF-8";
/* Font imports */
@font-face {
  font-family: "Styrene A";
  src: url("https://assets.anthropic.com/m/1230f9b0f8df18e5/original/StyreneALC-Medium.otf") format("opentype");
  font-weight: 500;
  font-style: normal;
  font-display: swap;
  font-feature-settings: "pnum" on, "lnum" on, "liga" on;
}
@font-face {
  font-family: "Styrene B";
  src: url("https://assets.anthropic.com/m/78f19c4a1fe4b2b6/original/StyreneBLC-Medium.otf") format("opentype");
  font-weight: 500;
  font-style: normal;
  font-display: swap;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  font-feature-settings: "tnum" on, "lnum" on;
}
@font-face {
  font-family: "Styrene B";
  src: url("https://assets.anthropic.com/m/615ef9f8465bfb6f/original/StyreneBLC-Regular.otf") format("opentype");
  font-weight: 400;
  font-style: normal;
  font-display: swap;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  font-feature-settings: "tnum" on, "lnum" on;
}
@font-face {
  font-family: "Copernicus";
  src: url("https://assets.anthropic.com/m/5a8c34e711888067/original/Copernicus-Book.otf") format("opentype");
  font-weight: 400;
  font-style: normal;
  font-display: swap;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  font-feature-settings: "tnum" on, "lnum" on;
}
@font-face {
  font-family: "Copernicus";
  src: url("https://assets.anthropic.com/m/531ea702ef721982/original/Copernicus-Semibold.otf") format("opentype");
  font-weight: 600;
  font-style: normal;
  font-display: swap;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  font-feature-settings: "tnum" on, "lnum" on;
}
@font-face {
  font-family: "Tiempos";
  src: url("https://assets.anthropic.com/m/758199899f64927d/original/TiemposText-Medium.otf") format("opentype");
  font-weight: 500;
  font-style: normal;
  font-display: swap;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  font-feature-settings: "tnum" on, "lnum" on;
}
@font-face {
  font-family: "Tiempos";
  src: url("https://assets.anthropic.com/m/557ce81381fd50f4/original/TiemposText-Regular.otf") format("opentype");
  font-weight: 400;
  font-style: normal;
  font-display: swap;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  font-feature-settings: "tnum" on, "lnum" on;
}
#catalog-courses {
  gap: 16px;
}

.sj-ribbon-wrapper,
.sj-course-ribbon-wrapper {
  overflow: visible !important;
  position: relative;
  top: -16px;
  right: 32px;
}

body.sj-page-catalog .catalog-header h1 {
  letter-spacing: -1.92px;
  font-size: 74px !important;
  margin-top: 80px;
}

a.coursebox-container.course.theme-color-border-hover.theme-no-hover {
  padding: 32px;
  border-radius: 16px;
  border: none;
  position: relative;
}
a.coursebox-container.course.theme-color-border-hover.theme-no-hover:hover {
  background-color: #F0EEE6;
}
a.coursebox-container.course.theme-color-border-hover.theme-no-hover .sj-ribbon-wrapper:not(.hide) {
  overflow: visible !important;
  float: right;
  margin-left: 16px;
}
@media (max-width: 768px) {
  a.coursebox-container.course.theme-color-border-hover.theme-no-hover .sj-ribbon-wrapper:not(.hide) {
    float: none;
    position: absolute;
    top: 2px;
    right: 5px;
    height: unset;
    width: unset;
  }
}
a.coursebox-container.course.theme-color-border-hover.theme-no-hover .sj-ribbon-wrapper:not(.hide) .sj-ribbon {
  display: block;
  height: 24px;
  padding: 3px;
  align-items: center;
  gap: 10px;
  border-radius: 12.5px;
  border: 1px solid var(--border-border-strong, #141413);
  background: transparent;
  transform: none;
  text-align: center;
}
a.coursebox-container.course.theme-color-border-hover.theme-no-hover .sj-ribbon-wrapper:not(.hide) .sj-ribbon .sj-ribbon-text {
  font-size: 14px;
  font-weight: 400;
  color: var(--text-text-color, #141413);
  transform: none;
  white-space: nowrap;
}

.coursebox-text {
  color: var(--text-text-color, #141413);
  font-variant-numeric: lining-nums proportional-nums;
  /* display-serif/s-500 */
  font-family: var(--typography-display-serif, Copernicus);
  font-size: 24px !important;
  font-style: normal;
  font-weight: 500;
  line-height: 120%;
  /* 28.8px */
  letter-spacing: -0.54px;
  margin-bottom: 4px;
}

.list-view a.course .coursebox-text {
  margin: 0;
  display: inline;
}
@media (max-width: 768px) {
  .list-view a.course .coursebox-text {
    display: block;
    margin-top: 30px;
    margin-bottom: 10px;
    padding: 0;
  }
}

.list-view a.course .coursebox-text-description {
  font-family: "tiempos", serif;
  font-size: 20px;
  line-height: 1.4 !important;
  margin: 0;
  padding: 0;
}

a.coursebox-container.course .coursebox-image {
  background: transparent;
  width: 124px;
  height: 124px;
  margin-right: 24px !important;
}
@media (max-width: 768px) {
  a.coursebox-container.course .coursebox-image {
    margin-bottom: 24px;
    display: flex;
    margin-right: 0 !important;
    width: 100%;
  }
}
a.coursebox-container.course .coursebox-image img {
  width: 100%;
  height: 100%;
  object-fit: contain;
}

.alert-box.warning {
  font-size: 1.35rem !important;
  line-height: 1.4;
}

.alert-box .close {
  top: 28px;
}

.clp__container {
  font-family: "Styrene B", -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
  background: #faf9f6;
  color: #2c2b25;
  line-height: 1.6;
  font-weight: 400;
}

.clp__hero {
  padding: 4rem 2rem;
}
@media (max-width: 640px) {
  .clp__hero {
    padding: 2rem 1rem;
  }
}

.clp__hero-content {
  max-width: 1200px;
  margin: 0 auto;
  display: grid;
  grid-template-columns: 1fr 1fr;
  gap: 5rem;
  align-items: center;
}
@media (max-width: 968px) {
  .clp__hero-content {
    grid-template-columns: 1fr;
    gap: 3rem;
  }
}

.clp__breadcrumb {
  font-size: 0.875rem;
  color: #7c7968;
  letter-spacing: 0.3px;
  margin-bottom: 1.5rem;
}
@media (max-width: 640px) {
  .clp__breadcrumb {
    margin-bottom: 1rem;
  }
}

.clp__breadcrumb-link {
  color: #7c7968;
  text-decoration: none;
  transition: color 0.2s ease;
}
.clp__breadcrumb-link:hover {
  color: #2c2b25;
  text-decoration: underline;
}

.clp__breadcrumb-separator {
  color: #9a9788;
}

.clp__title {
  font-family: "Copernicus", Georgia, serif;
  font-size: 3rem;
  line-height: 1.15;
  margin-bottom: 1.5rem;
  color: #2c2b25;
  font-weight: 400;
  letter-spacing: -0.02em;
}
@media (max-width: 968px) {
  .clp__title {
    font-size: 2.5rem;
  }
}
@media (max-width: 640px) {
  .clp__title {
    font-size: 2rem;
    margin-bottom: 1rem;
  }
}

.clp__subtitle {
  font-size: 1.125rem;
  color: #5e5b4e;
  margin-bottom: 2.5rem;
  line-height: 1.6;
}
@media (max-width: 640px) {
  .clp__subtitle {
    font-size: 1rem;
    margin-bottom: 1.5rem;
  }
}

.clp__hero-actions {
  display: flex;
  align-items: center;
  margin-bottom: 1rem;
}
@media (max-width: 640px) {
  .clp__hero-actions {
    margin-bottom: 0.75rem;
  }
}

.clp__enroll-btn {
  background: #2c2b25;
  color: white;
  padding: 0.875rem 2rem;
  border: none;
  border-radius: 8px;
  font-size: 1rem;
  cursor: pointer;
  transition: opacity 0.2s ease;
  font-weight: 500;
  font-family: inherit;
  display: inline-block;
  text-decoration: none;
  line-height: 1;
}
.clp__enroll-btn:hover {
  opacity: 0.85;
  color: white;
  text-decoration: none;
}

.clp__free-tag {
  display: inline-block;
  background: rgba(44, 43, 37, 0.08);
  padding: 0.25rem 0.75rem;
  border-radius: 4px;
  font-size: 0.875rem;
  margin-left: 1rem;
  color: #7c7968;
  font-weight: 400;
}

.clp__signin-text {
  color: #7c7968;
  font-size: 0.95rem;
}

.clp__signin-link {
  color: #2c2b25;
  text-decoration: none;
  font-weight: 500;
}
.clp__signin-link:hover {
  text-decoration: underline;
}

.clp__hero-right {
  display: flex;
  flex-direction: column;
}

.clp__hero-content .clp__video-wrapper .dp-promo-image-wrapper.columns {
  width: 100%;
  height: auto;
  margin: 0;
  padding: 0;
  border-radius: 8px;
  overflow: hidden;
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08);
  width: 100% !important;
}
.clp__hero-content .clp__video-wrapper .dp-promo-image-wrapper.columns .video-max {
  max-width: 100% !important;
}

.clp__video-container {
  background: #2c2b25;
  border-radius: 8px;
  overflow: hidden;
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08);
  position: relative;
}

.clp__video-title {
  position: absolute;
  top: 24px;
  left: 24px;
  color: white;
  font-size: 0.875rem;
  font-weight: 500;
  text-transform: uppercase;
  letter-spacing: 0.5px;
  opacity: 0.9;
  z-index: 1;
}

.clp__video-placeholder {
  width: 100%;
  min-height: 350px;
  background: #000;
  display: flex;
  align-items: center;
  justify-content: center;
}

.clp__play-button {
  width: 64px;
  height: 64px;
  background: rgba(255, 255, 255, 0.9);
  border-radius: 50%;
  display: flex;
  align-items: center;
  justify-content: center;
  cursor: pointer;
  transition: transform 0.2s ease;
}
.clp__play-button:hover {
  transform: scale(1.05);
}
.clp__play-button::after {
  content: "▶";
  font-size: 24px;
  color: #2c2b25;
  margin-left: 4px;
}

.clp__main-content {
  max-width: 1200px;
  margin: 0 auto;
  padding: 3rem 2rem 0 2rem;
}
@media (max-width: 640px) {
  .clp__main-content {
    padding: 2rem 1rem;
  }
}

.clp__topics-section {
  margin-bottom: 3rem;
}

.clp__section-title {
  font-family: "Copernicus", Georgia, serif;
  font-size: 2.25rem;
  margin: 3.5rem 0 2rem;
  color: #2c2b25;
  font-weight: 600;
  letter-spacing: -0.02em;
}
@media (max-width: 640px) {
  .clp__section-title {
    font-size: 1.75rem;
    margin-bottom: 2rem;
  }
}

.clp__topics-grid {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(320px, 1fr));
  gap: 2rem;
}
@media (max-width: 968px) {
  .clp__topics-grid {
    grid-template-columns: 1fr;
  }
}

.clp__topic-card {
  background: #f0eee6;
  border: 1px solid #e5e2d9;
  border-radius: 8px;
  padding: 2rem;
  transition: box-shadow 0.2s ease;
}
@media (max-width: 640px) {
  .clp__topic-card {
    padding: 1.5rem;
  }
}
.clp__topic-card:hover {
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.06);
}
.clp__topic-card:nth-child(1) {
  border-top: 3px solid #b4c6d4;
}
.clp__topic-card:nth-child(2) {
  border-top: 3px solid #c5bfd9;
}
.clp__topic-card:nth-child(3) {
  border-top: 3px solid #b5c5c0;
}

.clp__topic-title {
  font-family: "Styrene A", -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
  font-size: 1.25rem;
  margin-bottom: 1.25rem;
  color: #2c2b25;
  font-weight: 500;
}

.clp__topic-list {
  list-style: none;
  padding: 0;
  margin: 0;
}

.clp__topic-item {
  margin-bottom: 0.5rem;
  padding-left: 1.25rem;
  position: relative;
  color: #5e5b4e;
  font-size: 1rem;
  line-height: 1.5;
}
.clp__topic-item::before {
  content: "•";
  position: absolute;
  left: 0;
  color: #9a9788;
}

.clp__sections-container {
  margin-bottom: 3rem;
}

.clp__section-block {
  background: #f0eee6;
  border-radius: 8px;
  padding: 2.5rem;
  margin-bottom: 2.5rem;
}
@media (max-width: 640px) {
  .clp__section-block {
    padding: 1.5rem;
    margin-bottom: 1.5rem;
  }
}

.clp__section-header {
  display: flex;
  justify-content: space-between;
  align-items: start;
  margin-bottom: 2rem;
}
@media (max-width: 640px) {
  .clp__section-header {
    margin-bottom: 1rem;
  }
}

.clp__section-name {
  font-size: 1.5rem;
  margin-bottom: 0.5rem;
  color: #2c2b25;
  font-weight: 600;
}
@media (max-width: 640px) {
  .clp__section-name {
    font-size: 1.25rem;
  }
}

.clp__lesson-count {
  color: #7c7968;
  font-size: 0.9375rem;
}

.clp__section-description {
  color: #5e5b4e;
  margin-bottom: 2.5rem;
  line-height: 1.75;
  font-size: 1rem;
}
@media (max-width: 640px) {
  .clp__section-description {
    margin-bottom: 1.5rem;
    font-size: 0.9375rem;
  }
}

.clp__preview-label {
  display: block;
  font-size: 0.8125rem;
  color: #9a9788;
  margin-bottom: 0.5rem;
  font-weight: 400;
}

.clp__image-placeholders {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(220px, 1fr));
  place-items: center;
  gap: 1.25rem;
  margin-top: 0;
}
@media (max-width: 640px) {
  .clp__image-placeholders {
    gap: 0.75rem;
    margin-top: 1rem;
  }
}

.clp__screenshot-wrapper {
  max-height: 188px;
  aspect-ratio: 16/9;
  border-radius: 6px;
  overflow: hidden;
  border: 1px solid rgba(0, 0, 0, 0.08);
  background: rgba(0, 0, 0, 0.02);
}

.clp__screenshot-image {
  width: 100%;
  height: 100%;
  object-fit: cover;
  display: block;
  image-rendering: -webkit-optimize-contrast;
  image-rendering: crisp-edges;
  -webkit-font-smoothing: antialiased;
}

.clp__screenshot-clickable {
  cursor: pointer;
  transition: opacity 0.2s ease;
}
.clp__screenshot-clickable:hover {
  opacity: 0.8;
}

.clp__image-placeholder {
  aspect-ratio: 16/9;
  background: rgba(0, 0, 0, 0.05);
  border-radius: 6px;
  display: flex;
  align-items: center;
  justify-content: center;
  color: #9a9788;
  font-size: 0.875rem;
  border: 1px solid rgba(0, 0, 0, 0.08);
}

.clp__two-column-layout {
  display: grid;
  grid-template-columns: 2fr 1fr;
  gap: 5rem;
  margin-bottom: 6rem;
}
@media (max-width: 968px) {
  .clp__two-column-layout {
    grid-template-columns: 1fr;
  }
}

.clp__course-details {
  background: #f0eee6;
  border-radius: 8px;
  padding: 3.5rem;
}
@media (max-width: 640px) {
  .clp__course-details {
    padding: 2rem 1.5rem;
  }
}

.clp__details-title {
  font-family: "Copernicus", Georgia, serif;
  font-size: 2rem;
  margin-bottom: 2.5rem;
  color: #2c2b25;
  font-weight: 400;
  letter-spacing: -0.02em;
}
@media (max-width: 640px) {
  .clp__details-title {
    font-size: 1.5rem;
    margin-bottom: 1.5rem;
  }
}

.clp__details-subtitle {
  font-size: 1.25rem;
  margin-top: 2.5rem;
  margin-bottom: 1.25rem;
  color: #2c2b25;
  font-weight: 600;
}
@media (max-width: 640px) {
  .clp__details-subtitle {
    font-size: 1.125rem;
    margin-top: 1.5rem;
    margin-bottom: 0.75rem;
  }
}

.clp__course-details .clp__details-text {
  color: #5e5b4e;
  line-height: 1.75;
  margin-bottom: 1.25rem;
}
.clp__course-details .clp__details-list {
  margin-left: 1.5rem;
  color: #5e5b4e;
  line-height: 1.8;
}
.clp__course-details .clp__details-list > li {
  color: #5e5b4e;
}

.clp__curriculum-sidebar {
  background: #f0eee6;
  border-radius: 8px;
  padding: 2.5rem;
  height: fit-content;
  border: 1px solid #e5e2d9;
}

.clp__curriculum-title {
  font-size: 1.5rem;
  margin-bottom: 2rem;
  color: #2c2b25;
  font-weight: 600;
}

.clp__module {
  margin-bottom: 2rem;
}

.clp__module-title {
  font-weight: 600;
  font-size: 1rem;
  margin-bottom: 0.625rem;
  color: #2c2b25;
}

.clp__lesson {
  margin-left: 1rem;
  padding: 0.375rem 0;
  color: #7c7968;
  font-size: 0.9375rem;
  line-height: 1.5;
}

.clp__stats-container {
  display: flex;
  gap: 1.5rem;
  margin-top: 1rem;
  padding-top: 1rem;
  border-top: 1px solid #e5e2d9;
  justify-content: center;
}
@media (max-width: 640px) {
  .clp__stats-container {
    gap: 1.25rem;
    flex-wrap: wrap;
  }
}

.clp__stat-item {
  display: flex;
  align-items: baseline;
  gap: 0.375rem;
  font-size: 0.8125rem;
}

.clp__stat-value {
  font-weight: 600;
  color: #2c2b25;
  font-size: 0.875rem;
}

.clp__stat-label {
  color: #7c7968;
  font-weight: 400;
  font-size: 0.8125rem;
}

.clp__share-buttons-inline {
  display: flex;
  gap: 0.75rem;
  margin-top: 1rem;
}
@media (max-width: 640px) {
  .clp__share-buttons-inline {
    flex-direction: column;
    align-items: flex-start;
  }
}

.clp__share-btn-inline {
  display: inline-flex;
  align-items: center;
  gap: 0.4rem;
  padding: 0.4rem 0.75rem;
  border: 1px solid rgba(0, 0, 0, 0.08);
  border-radius: 6px;
  background: rgba(255, 255, 255, 0.3);
  color: #5e5b4e;
  text-decoration: none;
  font-size: 0.875rem;
  transition: all 0.2s ease;
}
.clp__share-btn-inline span {
  font-weight: 400;
}
.clp__share-btn-inline:hover {
  background: rgba(255, 255, 255, 0.6);
  border-color: rgba(0, 0, 0, 0.12);
  color: #2c2b25;
}

.clp__share-icon-inline {
  width: 16px;
  height: 16px;
}

.clp__instructors-container {
  margin-bottom: 3rem;
}

.clp__instructors-title {
  font-family: "Copernicus", Georgia, serif;
  font-size: 2.25rem;
  margin: 3.5rem 0 2rem;
  color: #2c2b25;
  font-weight: 600;
  letter-spacing: -0.02em;
}
@media (max-width: 640px) {
  .clp__instructors-title {
    font-size: 1.75rem;
    margin-bottom: 2rem;
  }
}

.clp__instructors-card {
  background: #f0eee6;
  border-radius: 8px;
  padding: 2.5rem;
}
@media (max-width: 640px) {
  .clp__instructors-card {
    padding: 1.5rem;
  }
}

.clp__instructors-grid {
  display: flex;
  flex-direction: column;
  gap: 2.5rem;
}
@media (max-width: 640px) {
  .clp__instructors-grid {
    gap: 2rem;
  }
}

.clp__instructor-item {
  display: flex;
  gap: 2rem;
  align-items: flex-start;
}
@media (max-width: 640px) {
  .clp__instructor-item {
    flex-direction: column;
    gap: 1.25rem;
  }
}

.clp__instructor-avatar {
  width: 168px;
  height: 168px;
  border-radius: 12px;
  object-fit: cover;
  flex-shrink: 0;
  padding-top: 15px;
}
@media (max-width: 640px) {
  .clp__instructor-avatar {
    width: 100px;
    height: 100px;
    border-radius: 8px;
    padding-top: 0;
  }
}

.clp__instructor-info {
  flex: 1;
  padding-top: 0.5rem;
}

.clp__instructor-name {
  font-size: 1.5rem;
  font-weight: 600;
  color: #2c2b25;
  margin-bottom: 0.75rem;
}
@media (max-width: 640px) {
  .clp__instructor-name {
    font-size: 1.25rem;
    margin-bottom: 0.5rem;
  }
}

.clp__instructor-description {
  color: #5e5b4e;
  line-height: 1.75;
  font-size: 1rem;
}
@media (max-width: 640px) {
  .clp__instructor-description {
    font-size: 0.9375rem;
    line-height: 1.6;
  }
}

.clp__extra-container {
  margin-bottom: 3rem;
}

.clp__extra-title {
  font-family: "Copernicus", Georgia, serif;
  font-size: 2.25rem;
  margin: 3.5rem 0 2rem;
  color: #2c2b25;
  font-weight: 600;
  letter-spacing: -0.02em;
}
@media (max-width: 640px) {
  .clp__extra-title {
    font-size: 1.75rem;
    margin-bottom: 2rem;
  }
}

.clp__extra-card {
  background: #f0eee6;
  border-radius: 8px;
  padding: 2.5rem;
}
@media (max-width: 640px) {
  .clp__extra-card {
    padding: 1.5rem;
  }
}

.clp__extra-content {
  color: #5e5b4e;
  line-height: 1.75;
  font-size: 1rem;
}
@media (max-width: 640px) {
  .clp__extra-content {
    font-size: 0.9375rem;
    line-height: 1.6;
  }
}
.clp__extra-content p {
  margin-bottom: 1.25rem;
}
.clp__extra-content p:last-child {
  margin-bottom: 0;
}
.clp__extra-content ul {
  margin: 1.25rem 0;
  padding-left: 1.5rem;
}
.clp__extra-content ul li {
  margin-bottom: 0.5rem;
  line-height: 1.75;
}

.clp__modal-overlay {
  position: fixed;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  background: rgba(0, 0, 0, 0.9);
  display: flex;
  align-items: center;
  justify-content: center;
  z-index: 1000;
  opacity: 0;
  animation: fadeIn 0.3s ease forwards;
}

@keyframes fadeIn {
  to {
    opacity: 1;
  }
}
.clp__modal-content {
  background: white;
  border-radius: 12px;
  max-width: 90vw;
  max-height: 90vh;
  width: 800px;
  display: flex;
  flex-direction: column;
  position: relative;
  overflow: hidden;
  box-shadow: 0 20px 40px rgba(0, 0, 0, 0.3);
}

.clp__modal-close {
  position: absolute;
  top: 1rem;
  right: 1rem;
  padding: 0;
  background: rgba(255, 255, 255, 0.9);
  border: none;
  border-radius: 50%;
  width: 40px;
  height: 40px;
  font-weight: bold;
  cursor: pointer;
  display: flex;
  align-items: center;
  justify-content: center;
  z-index: 10;
  transition: background 0.2s ease;
  color: black;
  line-height: 1;
}
.clp__modal-close:focus {
  background: rgba(255, 255, 255, 0.9);
  border-color: unset;
  color: black;
  outline: none !important;
}
.clp__modal-close:focus:before {
  outline: none !important;
  box-shadow: none !important;
  border: unset !important;
}
.clp__modal-close:hover {
  color: rgba(0, 0, 0, 0.4);
  background: rgba(255, 255, 255, 0.9);
  border-color: unset;
}

.clp__modal-header {
  padding: 2rem 2rem 0.5rem;
  border-bottom: 1px solid #e5e2d9;
}
.clp__modal-header .clp__preview-label {
  margin-top: 1rem;
}

.clp__modal-title {
  font-family: "Copernicus", Georgia, serif;
  font-size: 1.5rem;
  margin: 0 0 0.5rem;
  color: #2c2b25;
  font-weight: 600;
}

.clp__modal-description {
  color: #5e5b4e;
  margin: 0;
  line-height: 1.6;
  font-size: 0.95rem;
}

.clp__modal-body {
  position: relative;
  flex: 1;
  display: flex;
  align-items: center;
  min-height: 400px;
  overflow: auto;
}
@media (max-width: 640px) {
  .clp__modal-body {
    min-height: auto;
  }
}

.clp__modal-image-container {
  flex: 1;
  display: flex;
  align-items: center;
  justify-content: center;
  padding: 1rem;
}

.clp__modal-image {
  max-width: 100%;
  max-height: 100%;
  object-fit: contain;
  border-radius: 8px;
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
}

.clp__modal-nav {
  position: absolute;
  top: 50%;
  transform: translateY(-50%);
  background: rgba(0, 0, 0, 0.5);
  border: none;
  border-radius: 50%;
  width: 40px;
  height: 40px;
  font-size: 32px;
  font-weight: bold;
  cursor: pointer;
  display: flex;
  align-items: center;
  justify-content: center;
  transition: all 0.2s ease;
  z-index: 10;
  color: white;
  line-height: 1;
  min-width: 40px;
  min-height: 40px;
  padding: 0;
}
.clp__modal-nav span {
  display: block;
  line-height: 1;
  transform: translateY(-3px);
}
.clp__modal-nav:hover {
  background: rgba(0, 0, 0, 0.9);
  transform: translateY(-50%) scale(1.05);
  color: white;
  border-color: unset;
  outline: none !important;
}
.clp__modal-nav.clp__modal-prev {
  left: 1rem;
}
.clp__modal-nav.clp__modal-next {
  right: 1rem;
}
.clp__modal-nav:focus {
  background: rgba(0, 0, 0, 0.5);
}
.clp__modal-nav:focus:before {
  outline: none !important;
  box-shadow: none !important;
  border: unset !important;
}

.clp__modal-footer {
  padding: 1rem 2rem;
  border-top: 1px solid #e5e2d9;
  background: #faf9f6;
}

.clp__modal-footer-content {
  display: flex;
  justify-content: space-between;
  align-items: center;
  gap: 2rem;
}
@media (max-width: 640px) {
  .clp__modal-footer-content {
    flex-direction: column;
    gap: 1rem;
    text-align: center;
  }
}

.clp__modal-enroll {
  display: flex;
  align-items: center;
  gap: 1rem;
}
@media (max-width: 640px) {
  .clp__modal-enroll {
    flex-direction: column;
    gap: 0.5rem;
  }
}

.clp__modal-enroll-text {
  margin: 0;
  font-size: 0.875rem;
  color: #7c7968;
  font-weight: 500;
}

.clp__modal-enroll-btn {
  background: #2c2b25;
  color: white;
  padding: 0.75rem 1.5rem;
  border: none;
  border-radius: 6px;
  font-size: 0.875rem;
  cursor: pointer;
  transition: opacity 0.2s ease;
  font-weight: 500;
  font-family: inherit;
  display: inline-block;
  text-decoration: none;
  line-height: 1;
}
.clp__modal-enroll-btn:hover {
  opacity: 0.85;
  color: white;
  text-decoration: none;
}

.clp__modal-counter {
  font-size: 0.875rem;
  color: #7c7968;
  font-weight: 500;
}

@media (max-width: 768px) {
  .clp__modal-content {
    width: 95vw;
    max-height: 95vh;
  }
  .clp__modal-header {
    padding: 1.5rem 1.5rem 1rem;
  }
  .clp__modal-title {
    font-size: 1.25rem;
  }
  .clp__modal-description {
    font-size: 0.9rem;
  }
  .clp__modal-nav {
    width: 40px;
    height: 40px;
    font-size: 16px;
  }
  .clp__modal-footer {
    padding: 0.75rem 1.5rem;
  }
}
#lesson-main .lesson-top {
  display: flex;
  flex-direction: row;
  justify-content: space-between;
  align-items: center;
  overflow: visible;
}
#lesson-main .lesson-top h2 {
  margin: 0;
}
#lesson-main .lp__chat-combo {
  position: relative;
  display: flex;
  align-items: stretch;
  justify-content: flex-end;
}
#lesson-main .lp__chat-button-main {
  border-radius: 0.75rem 0 0 0.75rem;
  padding: 0 0.75rem;
  border: 1px solid rgba(0, 0, 0, 0.08);
  border-right: 0;
  margin: 0;
  background: rgba(255, 255, 255, 0.3);
  color: #5e5b4e;
  font-size: 0.875rem;
  font-weight: 500;
  cursor: pointer;
  transition: background-color 0.1s ease;
}
#lesson-main .lp__chat-button-main:hover {
  background: rgba(255, 255, 255, 0.6);
  border-color: rgba(0, 0, 0, 0.12);
  color: #2c2b25;
}
#lesson-main .lp__chat-button-main:focus {
  background: rgba(255, 255, 255, 0.8);
  border-color: rgba(0, 0, 0, 0.16);
  color: #2c2b25;
  outline: none !important;
}
#lesson-main .lp__chat-button-main:focus:before {
  outline: none !important;
  box-shadow: none !important;
  border: unset !important;
}
#lesson-main .lp__chat-button-main .lp__chat-button-content {
  display: flex;
  align-items: center;
  gap: 0.5rem;
}
#lesson-main .lp__chat-button-main .lp__chat-icon {
  width: 1rem;
  height: 1rem;
  flex-shrink: 0;
}
#lesson-main .lp__chat-button-dropdown {
  border-radius: 0 0.75rem 0.75rem 0;
  border: 1px solid rgba(0, 0, 0, 0.08);
  padding: 0.19rem 0.5rem;
  margin: 0;
  background: rgba(255, 255, 255, 0.3);
  cursor: pointer;
  transition: background-color 0.1s ease;
  display: flex;
  align-items: center;
  justify-content: center;
  aspect-ratio: 1;
}
#lesson-main .lp__chat-button-dropdown:hover {
  background: rgba(255, 255, 255, 0.6);
  border-color: rgba(0, 0, 0, 0.12);
}
#lesson-main .lp__chat-button-dropdown:focus {
  background: rgba(255, 255, 255, 0.8);
  border-color: rgba(0, 0, 0, 0.16);
  outline: none !important;
}
#lesson-main .lp__chat-button-dropdown:focus:before {
  outline: none !important;
  box-shadow: none !important;
  border: unset !important;
}
#lesson-main .lp__chat-button-dropdown .lp__dropdown-arrow {
  color: #9a9788;
  transition: transform 0.2s ease, color 0.1s ease;
  transform: rotate(90deg);
}
#lesson-main .lp__chat-button-dropdown:hover .lp__dropdown-arrow {
  color: #7c7968;
}
#lesson-main .lp__chat-button-dropdown.lp__dropdown-open .lp__dropdown-arrow {
  transform: rotate(270deg);
}
#lesson-main .lp__chat-dropdown-menu {
  position: absolute;
  top: 100%;
  right: 0;
  background: #faf9f6;
  border: 1px solid rgba(0, 0, 0, 0.08);
  border-radius: 0.75rem;
  box-shadow: 0 4px 16px rgba(0, 0, 0, 0.1);
  z-index: 50;
  opacity: 0;
  visibility: hidden;
  transform: translateY(-4px);
  transition: all 0.15s ease;
  margin-top: 0.25rem;
  min-width: max-content;
  padding: 0.25rem;
  display: flex;
  flex-direction: column;
  max-height: 420px;
  overflow-y: auto;
}
#lesson-main .lp__chat-dropdown-menu.lp__chat-dropdown-open {
  opacity: 1;
  visibility: visible;
  transform: translateY(0);
}
#lesson-main .lp__chat-dropdown-item {
  display: flex;
  align-items: center;
  gap: 0.25rem;
  padding: 0.375rem;
  cursor: pointer;
  border-radius: 0.75rem;
  transition: background-color 0.1s ease;
}
#lesson-main .lp__chat-dropdown-item:hover {
  background: rgba(255, 255, 255, 0.6);
}
#lesson-main .lp__chat-dropdown-item .lp__chat-icon-container {
  border: 1px solid rgba(0, 0, 0, 0.08);
  border-radius: 0.5rem;
  padding: 0.375rem;
  display: flex;
  align-items: center;
  justify-content: center;
  background: rgba(255, 255, 255, 0.3);
}
#lesson-main .lp__chat-dropdown-item .lp__chat-icon {
  width: 1rem;
  height: 1rem;
  flex-shrink: 0;
  color: #5e5b4e;
}
#lesson-main .lp__chat-dropdown-item .lp__chat-text {
  display: flex;
  flex-direction: column;
  padding-left: 0.25rem;
}
#lesson-main .lp__chat-dropdown-item .lp__chat-title {
  font-weight: 500;
  font-size: 0.875rem;
  color: #2c2b25;
  display: flex;
  align-items: center;
  gap: 0.25rem;
  line-height: 1.2;
}
#lesson-main .lp__chat-dropdown-item .lp__chat-subtitle {
  font-size: 0.75rem;
  color: #7c7968;
  line-height: 1.3;
}
#lesson-main .lp__chat-dropdown-item .lp__external-link {
  width: 0.75rem;
  height: 0.75rem;
  color: #7c7968;
  flex-shrink: 0;
}

#skilljar-content:has(#login-content) {
  display: flex;
  flex-direction: column;
  justify-content: flex-start;
  align-items: center;
}

.quiz .question .next-prev {
  margin-top: 10px;
}
.quiz .question .question-text,
.quiz .question .form-answers {
  width: 100%;
}
.quiz .question #id_answer {
  display: flex;
  flex-direction: column;
  gap: 8px;
  margin-top: 16px;
}
.quiz .question #id_answer > div {
  position: relative;
}
.quiz .question #id_answer label {
  display: flex;
  align-items: center;
  gap: 12px;
  padding: 14px 16px;
  background: rgba(255, 255, 255, 0.3);
  border: 1px solid rgba(0, 0, 0, 0.08);
  border-radius: 12px;
  cursor: pointer;
  transition: all 0.2s ease;
  font-family: "Tiempos", serif;
  font-size: 17px;
  font-weight: 400;
  line-height: 1.5;
  letter-spacing: -0.1px;
  color: rgb(20, 20, 19);
  margin: 0;
}
.quiz .question #id_answer label:hover {
  background: rgba(255, 255, 255, 0.6);
  border-color: rgba(0, 0, 0, 0.12);
}
.quiz .question #id_answer input[type=radio] {
  position: static;
  margin: 0;
  cursor: pointer;
  flex-shrink: 0;
  width: 18px;
  min-width: 18px;
  height: 18px;
  min-height: 18px;
}
.quiz .question #id_answer input[type=radio]:checked + label,
.quiz .question #id_answer label:has(input[type=radio]:checked) {
  background: rgba(255, 255, 255, 0.9);
  border-color: rgba(0, 0, 0, 0.2);
  box-shadow: 0 1px 3px rgba(0, 0, 0, 0.08);
}

.footer_contain {
  background-color: #141413;
  color: #fff;
  padding: 120px 0 60px;
  width: 100vw;
  margin-left: calc(-50vw + 50%);
  font-family: "Styrene B", -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
}

.u-container-full {
  width: 100%;
  margin: 0 auto;
  padding: 5rem 60px 0 60px;
}

.footer_grid {
  display: grid;
  grid-template-columns: auto 1fr;
  gap: 100px;
  margin-bottom: 80px;
}

.footer_grid_logo {
  color: #fff;
}

.footer_grid_logo svg {
  width: 48px;
  height: auto;
}

.footer_grid_content {
  display: grid;
  grid-template-columns: repeat(6, 1fr);
  gap: 50px;
}

.footer_group_wrap {
  display: flex;
  flex-direction: column;
  gap: 24px;
}

.footer_group_block {
  display: flex;
  flex-direction: column;
  gap: 16px;
}

.footer_group_title {
  color: #f0eee6;
  font-size: 18px;
  font-weight: 500;
  margin: 0;
  margin-bottom: 20px;
  font-family: "Styrene B", -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
}

.footer_group_list {
  list-style: none;
  margin: 0;
  padding: 0;
  display: flex;
  flex-direction: column;
  gap: 8px;
}

.w-list-unstyled {
  list-style: none;
}

.footer_group_item {
  margin: 0;
}

.footer_link_wrap {
  display: inline-block;
  text-decoration: none;
  color: #b0aea5;
  background: none;
  border: none;
  padding: 0;
  cursor: pointer;
  font-family: inherit;
  font-size: inherit;
}

.w-inline-block {
  display: inline-block;
}

.footer_link_text {
  font-size: 16px;
  line-height: 1.5;
  transition: color 0.2s ease;
  font-family: "Styrene B", -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
}

.footer_link_wrap:hover .footer_link_text {
  color: #fff;
}

.u-mb-0 {
  margin-bottom: 0 !important;
}

.footer_bottom_wrap {
  border-top: 1px solid #3d3d3a;
  padding-top: 32px;
}

.footer_bottom_contain {
  display: flex;
  justify-content: space-between;
  align-items: center;
  flex-wrap: wrap;
  gap: 20px;
}

.footer_bottom_text {
  color: #87867f;
  font-size: 14px;
}

.footer_bottom_list {
  display: flex;
  list-style: none;
  margin: 0;
  padding: 0;
  gap: 16px;
}

.footer_bottom_item {
  margin: 0;
}

.footer_bottom_link_wrap {
  display: inline-block;
  color: #87867f;
  text-decoration: none;
  transition: color 0.2s ease;
}

.footer_bottom_link_wrap:hover {
  color: #fff;
}

.u-icon-32 {
  width: 32px;
  height: 32px;
  display: flex;
  align-items: center;
  justify-content: center;
}

.u-icon-32 svg {
  width: 24px;
  height: 24px;
  fill: currentColor;
}

@media (max-width: 1200px) {
  .u-container-full {
    padding: 5rem 32px 0 32px;
  }
  .footer_grid_content {
    grid-template-columns: repeat(3, 1fr);
    gap: 32px;
  }
  .footer_grid {
    gap: 60px;
  }
}
@media (max-width: 768px) {
  .footer_contain {
    padding: 60px 0 32px;
  }
  .b-faq-list h2.h3 {
    margin: 0px 0 24px 0;
  }
  .block-content {
    margin-top: 24px !important;
  }
  .u-container-full {
    padding: 5rem 24px 0 24px;
  }
  .footer_grid {
    grid-template-columns: 1fr;
    gap: 40px;
    margin-bottom: 40px;
  }
  .footer_grid_content {
    grid-template-columns: repeat(2, 1fr);
    gap: 32px;
  }
  .footer_bottom_contain {
    flex-direction: column;
    align-items: center;
    text-align: center;
    gap: 16px;
  }
}
@media (max-width: 480px) {
  .footer_grid_content {
    grid-template-columns: 1fr;
    gap: 24px;
  }
  .footer_group_wrap {
    gap: 16px;
  }
}
/* ========== ORIGINAL FAQ STYLES BELOW ========== */
/* Typography */
.h3 {
  font-size: 32px;
  font-weight: 500;
  line-height: 1.15;
  letter-spacing: -0.64px;
  margin: 0 0 48px 0;
  font-family: "Styrene A", -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
}

.h4 {
  font-size: var(--type-scales-display-s, 24px);
  font-style: normal;
  font-weight: 500;
  line-height: 115%; /* 27.6px */
  letter-spacing: -0.48px;
  margin: 0;
  font-family: "Styrene A", -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
}

.text-b2 {
  font-size: 18px;
  line-height: 1.5;
  margin: 0;
  font-family: "Styrene A", -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
}

.text-b2 p {
  font-size: 20px !important;
  line-height: 1.5;
}

/* FAQ Component Styles */
.b-faq-list {
  padding-bottom: 96px;
  padding-top: 48px;
  position: relative;
}

.b-faq-list .page-wrapper {
  max-width: 1200px;
  margin: 0 auto;
  padding-left: 48px;
  padding-right: 48px;
}

.block-content {
  margin-top: 48px;
}

.faq-container {
  border-top: 1px solid #b0aea5;
  position: relative;
  width: 100%;
}

.faq-container + .faq-container {
  margin-top: 12px;
}

.faq-input {
  cursor: pointer;
  height: 100%;
  left: 0;
  opacity: 0;
  position: absolute;
  top: 0;
  width: 100%;
  z-index: 3;
  margin: 0;
}

.faq-label {
  display: flex;
  padding: 12px 0px;
  justify-content: space-between;
  align-items: center;
  align-self: stretch;
  cursor: pointer;
  position: relative;
  z-index: 1;
}

.faq-title {
  flex: 1;
}

.faq-icon {
  color: #141413;
  display: flex;
  justify-content: flex-end;
  align-items: center;
  flex-shrink: 0;
}

.icon-more {
  display: inline-block;
  width: 24px;
  height: 24px;
}

.icon-less {
  display: none;
  width: 24px;
  height: 24px;
}

.faq-container input[type=checkbox] + span + label {
  transition: color 0.1s cubic-bezier(0.165, 0.84, 0.44, 1);
}

.faq-container input[type=checkbox] + span + label svg path {
  transition: fill 0.1s cubic-bezier(0.165, 0.84, 0.44, 1);
}

.faq-container input[type=checkbox]:hover + span + label,
.faq-container input[type=checkbox]:focus-visible + span + label {
  color: #87867f;
}

.faq-container input[type=checkbox]:hover + span + label svg path,
.faq-container input[type=checkbox]:focus-visible + span + label svg path {
  fill: #87867f;
}

.faq-container input[type=checkbox] + span + label + article {
  display: grid;
  max-height: 0;
  overflow: hidden;
  transition: padding 0.2s cubic-bezier(0.165, 0.84, 0.44, 1) 0.3s, height 0.5s cubic-bezier(0.165, 0.84, 0.44, 1), max-height 0.5s cubic-bezier(0.165, 0.84, 0.44, 1);
}

.faq-container input[type=checkbox]:checked + span + label + article {
  max-height: 10000px;
  padding-bottom: 24px;
  padding-top: 24px;
  transition: padding 0.2s cubic-bezier(1, 0, 0, 1), height 1s cubic-bezier(1, 0, 0, 1), max-height 1s cubic-bezier(1, 0, 0, 1);
}

.faq-post {
  opacity: 0;
  transform: translateY(-10px);
}

.faq-container input[type=checkbox]:checked + span + label + article .faq-post {
  opacity: 1;
  transform: translateY(0);
  transition: opacity 0.2s cubic-bezier(0.165, 0.84, 0.44, 1), transform 0.3s cubic-bezier(0.165, 0.84, 0.44, 1);
  transition-delay: 0.2s;
}

.faq-container input[type=checkbox]:checked + span + label .icon-more {
  display: none;
}

.faq-container input[type=checkbox]:checked + span + label .icon-less {
  display: inline-block;
}

.faq-content {
  position: relative;
  z-index: 1;
}

/* Mobile responsive adjustments */
@media (max-width: 699px) {
  .faq-label {
    display: flex !important;
    justify-content: space-between;
    align-items: center;
    grid-template-columns: none;
    gap: 0;
  }
  .faq-title {
    padding-right: 16px;
    grid-column: unset;
    flex: 1;
  }
  .faq-icon {
    width: auto;
    grid-column: unset;
    flex-shrink: 0;
    justify-content: flex-end;
  }
  .s\:grid-12 {
    display: flex !important;
    flex-direction: row !important;
    justify-content: space-between;
    align-items: center;
    gap: 0;
  }
}
/* Tablet and larger breakpoints */
@media (min-width: 768px) {
  .b-faq-list .page-wrapper {
    padding-left: 64px;
    padding-right: 64px;
  }
  .h3 {
    font-size: 36px;
    margin: 0 0 64px 0;
  }
  .text-b2 {
    font-size: 20px;
  }
  .block-content {
    margin-top: 64px;
  }
  .b-faq-list {
    padding-bottom: 128px;
    padding-top: 64px;
  }
  .faq-container + .faq-container {
    margin-top: 16px;
  }
  .faq-container input[type=checkbox]:checked + span + label + article {
    padding-bottom: 0;
    padding-top: 0;
  }
}
@media (min-width: 1200px) {
  .b-faq-list .page-wrapper {
    padding-left: 80px;
    padding-right: 80px;
  }
  .h3 {
    font-size: 48px;
    margin: 0 0 24px 0;
  }
  .block-content {
    margin-top: 24px;
  }
  .b-faq-list {
    padding-bottom: 160px;
    padding-top: 80px;
  }
}
#details-pane-inner #details-pane-content #details-pane-summary-content.content .resource__container {
  display: flex;
  flex-direction: column;
  gap: 1rem;
  margin-top: 1.5rem;
}
#details-pane-inner #details-pane-content #details-pane-summary-content.content .resource__wrapper {
  display: flex;
  align-items: flex-start;
  padding: 1.3rem;
  border-radius: 12px;
  gap: 2rem;
  transition: all 0.2s ease;
  max-width: 800px;
}
#details-pane-inner #details-pane-content #details-pane-summary-content.content .resource__image {
  flex-shrink: 0;
  width: 33%;
  max-width: 230px;
  border-radius: 8px;
  display: flex;
  align-items: center;
  justify-content: center;
}
#details-pane-inner #details-pane-content #details-pane-summary-content.content .resource__image img {
  width: 100%;
  height: 100%;
  max-width: 210px;
  max-height: 210px;
  object-fit: contain;
}
#details-pane-inner #details-pane-content #details-pane-summary-content.content .resource__content {
  display: flex;
  flex-direction: column;
  flex: 1;
  gap: 1rem;
  padding-top: 10px;
}
#details-pane-inner #details-pane-content #details-pane-summary-content.content .resource__text_wrapper {
  display: flex;
  flex-direction: column;
  gap: 0.5rem;
}
#details-pane-inner #details-pane-content #details-pane-summary-content.content .resource__header {
  font-family: "Styrene A", -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
  font-size: 20px;
  font-weight: 500;
  margin: 0 0 0.5rem 0;
  color: rgb(20, 20, 19);
  line-height: 1.3;
}
#details-pane-inner #details-pane-content #details-pane-summary-content.content .resource__text {
  font-family: "Tiempos", Georgia, "Times New Roman", serif;
  font-size: 20px;
  margin: 0;
  color: rgb(20, 20, 19);
  line-height: 1.5;
}
#details-pane-inner #details-pane-content #details-pane-summary-content.content .resource__link {
  flex-shrink: 0;
  font-family: "Styrene B", -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
  font-size: 16px;
  font-weight: 500;
  padding: 0.75rem 1.5rem;
  background: rgba(25, 25, 25, 0.1);
  color: rgb(20, 20, 19);
  border-radius: 8px;
  text-decoration: none;
  transition: all 0.2s ease;
  display: inline-flex;
  align-items: center;
  justify-content: center;
  white-space: nowrap;
  border: none;
  cursor: pointer;
  align-self: flex-start;
  margin-top: 0.5rem;
}
#details-pane-inner #details-pane-content #details-pane-summary-content.content .resource__link span {
  color: rgb(20, 20, 19);
  text-decoration: none;
  font-weight: 400;
}
#details-pane-inner #details-pane-content #details-pane-summary-content.content .resource__link:hover {
  background: rgba(25, 25, 25, 0.2);
  text-decoration: none;
}
#details-pane-inner #details-pane-content #details-pane-summary-content.content .resource__link:focus {
  outline: none;
  text-decoration: none;
}
@media (max-width: 1024px) {
  #details-pane-inner #details-pane-content #details-pane-summary-content.content .resource__wrapper {
    flex-direction: column;
    align-items: flex-start;
    padding: 1.5rem;
  }
  #details-pane-inner #details-pane-content #details-pane-summary-content.content .resource__image {
    width: 100%;
    max-width: 200px;
    align-self: center;
  }
  #details-pane-inner #details-pane-content #details-pane-summary-content.content .resource__content {
    width: 100%;
  }
  #details-pane-inner #details-pane-content #details-pane-summary-content.content .resource__link {
    width: 100%;
    text-align: center;
    justify-content: center;
  }
}

h4.sj-text-details-pane-summary {
  display: none !important;
}

.lesson-fullscreen .lesson-top {
  width: 100%;
  max-width: 720px;
  margin: auto;
}

#lesson-main #lesson-main-inner .lesson-top {
  display: none !important;
}

#details-pane-summary-content,
.course-text-content:not(:has(#tutorial-container)):not(:has(.start-quiz)) {
  max-width: 720px;
  margin: auto;
}

#details-pane #details-pane-summary-content .lesson-description-content,
#lesson-main #lesson-main-inner #lesson-main-content .course-text-content {
  margin-top: 32px;
}
#details-pane #details-pane-summary-content .lesson-description-content iframe,
#lesson-main #lesson-main-inner #lesson-main-content .course-text-content iframe {
  width: min(960px, 100vw - 40px) !important;
  aspect-ratio: 16/9;
  height: auto !important;
  position: relative;
  left: 50%;
  transform: translateX(-50%);
}
@media (max-width: 1200px) {
  #details-pane #details-pane-summary-content .lesson-description-content iframe,
  #lesson-main #lesson-main-inner #lesson-main-content .course-text-content iframe {
    width: 100% !important;
    left: 0;
    transform: none;
  }
}
@media (max-width: 1200px) {
  .lesson-fullscreen #details-pane #details-pane-summary-content .lesson-description-content iframe,
  .lesson-fullscreen #lesson-main #lesson-main-inner #lesson-main-content .course-text-content iframe {
    width: min(960px, 100vw - 40px) !important;
    left: 50%;
    transform: translateX(-50%);
  }
}
#details-pane #details-pane-summary-content .lesson-description-content img,
#lesson-main #lesson-main-inner #lesson-main-content .course-text-content img {
  margin-bottom: 10px;
}
#details-pane #details-pane-summary-content .lesson-description-content h2,
#lesson-main #lesson-main-inner #lesson-main-content .course-text-content h2 {
  font-family: "Styrene A", sans-serif;
  font-size: 32px;
  font-weight: 500 !important;
  line-height: 115%;
  letter-spacing: -0.88px;
  color: rgb(20, 20, 19);
  background-color: transparent;
  margin-top: 64px;
  margin-bottom: 32px;
  text-decoration: none;
  text-align: start;
  text-wrap: pretty;
}
#details-pane #details-pane-summary-content .lesson-description-content > p:first-child,
#lesson-main #lesson-main-inner #lesson-main-content .course-text-content > p:first-child {
  margin-top: 4rem;
  margin-bottom: 2rem;
}
#details-pane #details-pane-summary-content .lesson-description-content h2#what-youll-learn,
#details-pane #details-pane-summary-content .lesson-description-content h2#what-youll-learn > span, #details-pane #details-pane-summary-content .lesson-description-content > p:first-child > span,
#lesson-main #lesson-main-inner #lesson-main-content .course-text-content h2#what-youll-learn,
#lesson-main #lesson-main-inner #lesson-main-content .course-text-content h2#what-youll-learn > span,
#lesson-main #lesson-main-inner #lesson-main-content .course-text-content > p:first-child > span {
  font-family: "Styrene A", sans-serif;
  font-feature-settings: "pnum" on, "lnum" on, "liga" on;
  font-size: 32px !important;
  font-weight: 500;
  line-height: 115%;
  letter-spacing: -0.88px;
  margin-top: 4rem;
  margin-bottom: 2rem;
  text-wrap: pretty;
}
#details-pane #details-pane-summary-content .lesson-description-content h3,
#lesson-main #lesson-main-inner #lesson-main-content .course-text-content h3 {
  font-family: "Styrene A", sans-serif;
  font-size: 24px;
  font-weight: 500;
  line-height: 27.6px;
  letter-spacing: -0.48px;
  color: rgb(20, 20, 19);
  background-color: transparent;
  margin-top: 32px;
  margin-bottom: 8px;
  text-decoration: none;
  text-align: start;
}
#details-pane #details-pane-summary-content .lesson-description-content h4,
#lesson-main #lesson-main-inner #lesson-main-content .course-text-content h4 {
  font-family: "Styrene A", sans-serif;
  font-size: 20px;
  font-weight: 500;
  line-height: 23px;
  letter-spacing: -0.4px;
  color: rgb(20, 20, 19);
  background-color: transparent;
  margin-top: 32px;
  margin-bottom: 8px;
  text-decoration: none;
  text-align: start;
}
#details-pane #details-pane-summary-content .lesson-description-content p,
#lesson-main #lesson-main-inner #lesson-main-content .course-text-content p {
  font-family: "Tiempos", serif;
  font-size: 20px;
  font-weight: 400;
  line-height: 31px;
  letter-spacing: -0.1px;
  color: rgb(20, 20, 19);
  background-color: transparent;
  margin-bottom: 16px;
  text-decoration: none;
  text-align: start;
}
#details-pane #details-pane-summary-content .lesson-description-content a,
#lesson-main #lesson-main-inner #lesson-main-content .course-text-content a {
  font-family: "Styrene B", sans-serif;
  font-size: 16px;
  font-weight: 400;
  line-height: 20px;
  letter-spacing: -0.08px;
  color: rgb(20, 20, 19);
  background-color: rgba(25, 25, 25, 0.1);
  text-decoration: none;
  text-align: start;
}
#details-pane #details-pane-summary-content .lesson-description-content strong,
#lesson-main #lesson-main-inner #lesson-main-content .course-text-content strong {
  font-family: "Tiempos", serif;
  font-size: 20px;
  font-weight: 500;
  line-height: 28px;
  letter-spacing: -0.1px;
  color: rgb(20, 20, 19);
  background-color: transparent;
  text-decoration: none;
  text-align: left;
}
#details-pane #details-pane-summary-content .lesson-description-content em,
#lesson-main #lesson-main-inner #lesson-main-content .course-text-content em {
  font-family: "Tiempos", serif;
  font-size: 20px;
  font-weight: 400;
  font-style: italic;
  line-height: 31px;
  letter-spacing: -0.1px;
  color: rgb(20, 20, 19);
  background-color: transparent;
  text-decoration: none;
  text-align: start;
}
#details-pane #details-pane-summary-content .lesson-description-content ul,
#lesson-main #lesson-main-inner #lesson-main-content .course-text-content ul {
  font-family: "Tiempos", serif;
  font-size: 20px;
  font-weight: 400;
  line-height: 155%;
  letter-spacing: -0.1px;
  color: rgb(20, 20, 19);
  background-color: transparent;
  margin-bottom: 16px;
  margin-left: 38px;
  list-style-type: disc;
  list-style-position: outside;
}
#details-pane #details-pane-summary-content .lesson-description-content ol,
#lesson-main #lesson-main-inner #lesson-main-content .course-text-content ol {
  font-family: "Tiempos", serif;
  font-size: 20px;
  font-weight: 400;
  line-height: 155%;
  letter-spacing: -0.1px;
  color: rgb(20, 20, 19);
  background-color: transparent;
  margin-bottom: 16px;
  margin-left: 38px;
  list-style-type: decimal;
  list-style-position: outside;
}
#details-pane #details-pane-summary-content .lesson-description-content li,
#lesson-main #lesson-main-inner #lesson-main-content .course-text-content li {
  font-family: "Tiempos", serif;
  font-size: 20px;
  font-weight: 400;
  line-height: 140%;
  letter-spacing: -0.1px;
  color: rgb(20, 20, 19);
  background-color: transparent;
  margin-bottom: 4px;
  display: list-item;
}
#details-pane #details-pane-summary-content .lesson-description-content ul > li,
#lesson-main #lesson-main-inner #lesson-main-content .course-text-content ul > li {
  font-family: "Tiempos", serif;
  font-size: 20px;
  font-weight: 400;
  line-height: 140%;
  letter-spacing: -0.1px;
  color: rgb(20, 20, 19);
  background-color: transparent;
  margin-bottom: 4px;
  display: list-item;
}
#details-pane #details-pane-summary-content .lesson-description-content ol > li,
#lesson-main #lesson-main-inner #lesson-main-content .course-text-content ol > li {
  font-family: "Tiempos", serif;
  font-size: 20px;
  font-weight: 400;
  line-height: 140%;
  letter-spacing: -0.1px;
  color: rgb(20, 20, 19);
  background-color: transparent;
  margin-bottom: 4px;
  display: list-item;
}
#details-pane #details-pane-summary-content .lesson-description-content ul ul,
#lesson-main #lesson-main-inner #lesson-main-content .course-text-content ul ul {
  font-family: "Tiempos", serif;
  font-size: 20px;
  font-weight: 400;
  line-height: 155%;
  letter-spacing: -0.1px;
  color: rgb(20, 20, 19);
  background-color: transparent;
  margin-top: 0;
  margin-bottom: 16px;
  margin-left: 0;
  padding-left: 32px;
  padding-bottom: 0;
  list-style-type: circle;
}
#details-pane #details-pane-summary-content .lesson-description-content ol ul,
#lesson-main #lesson-main-inner #lesson-main-content .course-text-content ol ul {
  font-family: "Tiempos", serif;
  font-size: 20px;
  font-weight: 400;
  line-height: 155%;
  letter-spacing: -0.1px;
  color: rgb(20, 20, 19);
  background-color: transparent;
  margin-top: 0;
  margin-bottom: 16px;
  margin-left: 0;
  padding-left: 32px;
  padding-bottom: 0;
  list-style-type: circle;
}
#details-pane #details-pane-summary-content .lesson-description-content ol ol,
#lesson-main #lesson-main-inner #lesson-main-content .course-text-content ol ol {
  font-family: "Tiempos", serif;
  font-size: 20px;
  font-weight: 400;
  line-height: 155%;
  letter-spacing: -0.1px;
  color: rgb(20, 20, 19);
  background-color: transparent;
  margin-top: 0;
  margin-bottom: 16px;
  margin-left: 0;
  padding-left: 32px;
  padding-bottom: 0;
  list-style-type: lower-alpha;
}
#details-pane #details-pane-summary-content .lesson-description-content ul ol,
#lesson-main #lesson-main-inner #lesson-main-content .course-text-content ul ol {
  font-family: "Tiempos", serif;
  font-size: 20px;
  font-weight: 400;
  line-height: 155%;
  letter-spacing: -0.1px;
  color: rgb(20, 20, 19);
  background-color: transparent;
  margin-top: 0;
  margin-bottom: 16px;
  margin-left: 0;
  padding-left: 32px;
  padding-bottom: 0;
  list-style-type: decimal;
}
#details-pane #details-pane-summary-content .lesson-description-content ul ul > li,
#lesson-main #lesson-main-inner #lesson-main-content .course-text-content ul ul > li {
  font-family: "Tiempos", serif;
  font-size: 20px;
  font-weight: 400;
  line-height: 140%;
  letter-spacing: -0.1px;
  color: rgb(20, 20, 19);
  background-color: transparent;
  margin-bottom: 4px;
}
#details-pane #details-pane-summary-content .lesson-description-content ol ul > li,
#lesson-main #lesson-main-inner #lesson-main-content .course-text-content ol ul > li {
  font-family: "Tiempos", serif;
  font-size: 20px;
  font-weight: 400;
  line-height: 140%;
  letter-spacing: -0.1px;
  color: rgb(20, 20, 19);
  background-color: transparent;
  margin-bottom: 4px;
}
#details-pane #details-pane-summary-content .lesson-description-content pre,
#lesson-main #lesson-main-inner #lesson-main-content .course-text-content pre {
  background-color: #f0eee6;
  border-radius: 8px;
  padding: 24px;
  margin-top: 16px;
  margin-bottom: 16px;
  overflow-x: auto;
  line-height: 1.5 !important;
}
#details-pane #details-pane-summary-content .lesson-description-content pre code,
#lesson-main #lesson-main-inner #lesson-main-content .course-text-content pre code {
  font-family: "SF Mono", "Monaco", "Menlo", monospace;
  font-size: 16px;
  font-weight: 400;
  line-height: 1.5 !important;
  letter-spacing: normal;
  color: rgb(20, 20, 19);
  background-color: transparent;
  white-space: pre;
  display: block;
}

/* PROD_STYLES. Don't remove this comment */
/* Base Overrides */
#skilljar-content.reveal {
  opacity: 100 !important;
}

#skilljar-content {
  opacity: 0 !important;
}

.cp-certificate {
  font-size: 26px;
  text-align: left;
}

#header .header-center-img {
  max-height: 16px;
  max-width: 100%;
}

a.header-link.focus-link-v2 {
  font-family: "Styrene B";
}

.lp-left-nav .course-title {
  font-size: 24px !important;
}

#details-pane {
  background: #faf9f5 !important;
}
@media only screen and (max-width: 767px) {
  #details-pane {
    margin-top: 0 !important;
  }
}

.course-text-content {
  background: transparent;
  font-family: "Tiempos", serif;
}

#lesson-main-content .course-text-content code {
  padding: 2px 4px;
  font-weight: normal;
  color: #a10d0d;
  font-size: 0.9em;
}

/* Base Overrides */
#cp-content #curriculum-list .lesson-row .title {
  overflow: visible;
  left: 44px;
  margin-top: 7px;
}

#cp-content #curriculum-list .lesson-row .bullet {
  top: 7px;
}

.row .row {
  margin-right: 0px;
  margin-left: 0px;
}

.lp-left-nav .section-title:first-of-type {
  padding-top: 16px;
}

.v1.sj-page-detail.sj-page-detail-course #skilljar-content {
  padding-top: 120px;
}

.v1.sj-page-login #skilljar-content {
  padding-top: 80px;
}

.v1.sj-page-curriculum #skilljar-content {
  padding-top: 80px;
}

body {
  display: flex;
  padding: 0, var(--spacing-system-module-stack-landing, 160px) var(--widths-page-margin, 80px);
  flex-direction: column;
  align-items: center;
  gap: var(--spacing-system-stack-gap-l, 48px);
  align-self: stretch;
  background-color: #faf9f5 !important;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

/* Layout Components */
.dp-row-flex-v2 {
  display: flex !important;
  justify-content: space-between !important;
  align-items: center !important;
}

/* Course Curriculum Page Specific Styles */
.v1.sj-page-curriculum {
  /* Layout Components - Create a CSS Grid */
  /* Make containers transparent to layout */
  /* Position elements using grid areas */
  /* Breadcrumb text before the grid */
  /* Course description styling */
}
.v1.sj-page-curriculum .dp-row-flex-v2 {
  display: grid !important;
  grid-template-columns: 1fr 300px !important;
  grid-template-areas: "title image" "description lessons" "description progress" "description button" ". ." !important;
  column-gap: 80px !important;
  row-gap: 0 !important;
  align-items: start !important;
}
.v1.sj-page-curriculum .large-8.push-4.columns.sj-summary.cp-summary-wrapper,
.v1.sj-page-curriculum .large-4.pull-8.columns.cp-promo-image-wrapper {
  display: contents !important;
}
.v1.sj-page-curriculum .dp-row-flex-v2 h1.break-word {
  grid-area: title !important;
  margin: 0 !important;
  padding: 0 !important;
  text-align: left !important;
}
.v1.sj-page-curriculum .large-10.large-centered.columns.text-center.cp-summary-row-v2::before {
  content: "Anthropic Academy  /  Courses" !important;
  display: block !important;
  color: var(--text-text-color, #141413) !important;
  font-variant-numeric: lining-nums proportional-nums;
  /* detail/m */
  font-family: var(--typography-detail, "Styrene B LC") !important;
  font-size: var(--type-scales-detail-m, 16px) !important;
  font-style: normal !important;
  font-weight: 400 !important;
  line-height: 125% !important;
  /* 20px */
  letter-spacing: -0.08px !important;
  margin-top: 100px !important;
  margin-bottom: 40px !important;
  text-align: left !important;
  width: 100% !important;
  max-width: 1200px !important;
  margin-left: 0;
  margin-right: auto !important;
}
.v1.sj-page-curriculum .course-description {
  grid-area: description !important;
  display: block !important;
  margin: 0 !important;
  padding: 0 !important;
  font-family: "Tiempos", serif;
  font-size: 24px;
  line-height: 1.5;
  max-width: 700px;
  text-align: left !important;
}
.v1.sj-page-curriculum .dp-row-flex-v2 .cp-promo-image {
  grid-area: image !important;
  display: flex !important;
  justify-content: center !important;
  align-items: center !important;
  margin: 0 auto !important;
  width: 97px !important;
  height: 96px !important;
}
.v1.sj-page-curriculum .dp-row-flex-v2 .cp-promo-image img {
  width: 100% !important;
  height: 100% !important;
  object-fit: contain !important;
}
.v1.sj-page-curriculum .dp-row-flex-v2 .cp-lessons {
  grid-area: lessons !important;
  text-align: center !important;
  margin: 0 !important;
  padding: 0 !important;
}
.v1.sj-page-curriculum .dp-row-flex-v2 .progress-bar {
  grid-area: progress !important;
  margin: 0 !important;
  border-radius: 12px !important;
  border: 1px solid var(--text-text-agate, #B0AEA5) !important;
  background: var(--background-secondary-background, #F0EEE6) !important;
  max-width: 300px;
}
.v1.sj-page-curriculum .dp-row-flex-v2 #resume-button {
  grid-area: button !important;
  display: flex !important;
  justify-content: center !important;
  margin: 0 !important;
}

.columns.text-center.push-3.large-6.dp-summary-wrapper {
  margin-left: 0 !important;
  width: 60% !important;
  max-width: none !important;
  text-align: left !important;
  float: none !important;
  padding: 0 !important;
  flex: 0 0 60% !important;
}

.columns.large-6.dp-promo-image-wrapper {
  width: 40% !important;
  flex: 0 0 40% !important;
  display: flex !important;
  justify-content: center !important;
  align-items: center !important;
}

/* Desktop order (768px and up) */
@media only screen and (min-width: 768px) {
  .v1.sj-page-detail.sj-page-detail-course .columns.text-center.push-3.large-6.dp-summary-wrapper {
    order: 1 !important;
  }
  .v1.sj-page-detail.sj-page-detail-course .columns.large-6.dp-promo-image-wrapper {
    order: 2 !important;
  }
}
.push-3,
.large-7 {
  all: unset;
}

.large-7 {
  display: flex;
  width: 802px;
  flex-direction: column;
  align-items: flex-start;
  margin-left: 20px;
}

/* Typography */
.main-container,
h1,
h2,
h3,
h4,
h5,
h6,
.v1.sj-page-curriculum,
.v1.sj-page-lesson {
  font-family: "Copernicus", serif;
}

.header-links-container a {
  font-size: 16px;
}

#header .header-links-container {
  padding-right: 0px;
}

#header .header-links-container .header-link {
  margin-right: 26px;
}

.main-container,
p,
ul,
li,
.v1.sj-page-curriculum,
.v1.sj-page-lesson {
  font-family: "Tiempos", serif;
  font-size: 20px;
  line-height: 1.7;
  font-weight: 400;
  color: #141413;
  letter-spacing: -0.015em;
}

.main-container strong,
p strong,
ul strong,
li strong,
.v1.sj-page-curriculum strong,
.v1.sj-page-lesson strong {
  font-weight: 500;
}

.sj-course-info-wrapper h2 {
  font-family: "Tiempos" !important;
  font-weight: 400;
  font-size: 24px;
  line-height: 1.4;
  font-style: italic;
}

.columns h2,
.columns h3,
.columns h2 span,
.columns h3 span,
.columns h4 span {
  color: #141413;
  font-family: "Copernicus";
  font-size: 26px;
  font-style: normal;
  font-weight: 600;
  line-height: 140%;
  letter-spacing: -0.025em;
  margin-top: 2em;
}

.lp-color-scheme-light .lp-left-nav .course-title {
  color: #141413;
  font-family: "Copernicus";
  font-size: 26px !important;
  font-style: normal;
  font-weight: 600 !important;
  line-height: 140%;
  letter-spacing: -0.025em;
  margin-top: 2em;
}

.columns h3:first-of-type {
  margin-top: 0px;
}

h1.break-word {
  letter-spacing: -1.92px;
  font-family: "Copernicus", serif;
  font-weight: 500;
  line-height: 110%;
  margin-bottom: var(--spacer-02);
  font-size: 72px !important;
}

/* Curriculum page specific h1 override */
.v1.sj-page-curriculum h1.break-word {
  align-self: stretch;
  color: var(--text-text-color, #141413);
  font-variant-numeric: lining-nums proportional-nums;
  font-family: var(--typography-display-serif, Copernicus);
  font-size: 54px !important;
  font-style: normal;
  font-weight: 400;
  line-height: 120%;
  /* 64.8px */
  letter-spacing: -1.62px;
  margin-bottom: 0;
}

/* Buttons */
.purchase-button-wrapper {
  display: flex;
  width: 180px;
  flex-direction: column;
  align-items: flex-start;
  gap: 12px;
  margin-top: 0;
}

#purchase-button {
  display: flex;
  width: 180px;
  margin: 0;
  top: 15px;
  height: var(--spacing-system-button-height, 48px);
  padding: 0px var(--spacing-system-button-padding, 32px);
  justify-content: center;
  align-items: center;
  gap: var(--spacers-spacer-01, 4px);
  border-radius: 12px;
  background: var(--button-button-color, #141413);
}

#resume-button {
  display: flex;
  justify-content: center;
  align-items: center;
}

.button.resume-button {
  display: flex;
  height: 40px;
  padding: 0px var(--spacing-system-button-padding, 32px);
  justify-content: center;
  align-items: center;
  gap: var(--spacers-spacer-01, 4px);
  border-radius: var(--border-radius-border-radius-small, 12px);
  background: var(--button-button-color, #141413);
  border: none;
  width: auto;
}

/* Progress Bar */
.progress-bar {
  height: 12px !important;
  width: 100%;
  max-width: 436px !important;
  border-radius: 12px;
  border: 1px solid var(--text-text-agate, #B0AEA5);
  background: var(--background-secondary-background, #F0EEE6);
  position: relative;
  overflow: hidden;
}

.progress-bar-inner {
  height: 100% !important;
  background: #B0AEA5 !important;
  border-radius: 12px;
  transition: width 0.3s ease;
}

.sj-page-curriculum #resume-button.row a {
  margin: 24px;
  font-family: "Styrene B";
  font-size: 16px;
  letter-spacing: 0.015em;
  padding: 16px 24px 18px;
}

#cp-content h2,
#cp-content .h2-style,
#cp-content h3,
#cp-content .h3-style {
  font-size: 24px;
}

/* Lesson Counter */
.cp-lessons {
  color: #000;
  text-align: left;
  font-variant-numeric: lining-nums proportional-nums;
  /* detail/s */
  font-family: var(--typography-detail, "Styrene B LC");
  font-size: var(--type-scales-detail-s, 14px);
  font-style: normal;
  font-weight: 400;
  line-height: 125%;
  /* 17.5px */
  letter-spacing: -0.07px;
  margin: 0 !important;
  height: auto !important;
}

/* Lesson completion text */
.sj-text-x-of-y-lessons-completed span {
  color: #000 !important;
  text-align: center !important;
  font-variant-numeric: lining-nums proportional-nums !important;
  /* detail/s */
  font-family: var(--typography-detail, "Styrene B LC") !important;
  font-size: var(--type-scales-detail-s, 14px) !important;
  font-style: normal !important;
  font-weight: 400 !important;
  line-height: 125% !important;
  /* 17.5px */
  letter-spacing: -0.07px !important;
}

/* Curriculum Section */
.sj-curriculum-wrapper {
  display: flex;
  width: 40% !important;
  padding: var(--spacing-system-card-padding, 32px);
  flex-direction: column;
  align-items: flex-start;
  border-radius: var(--border-radius-border-radius, 16px);
  background: var(--background-secondary-background, #f0eee6);
  margin-left: 0 !important;
  flex: 0 0 40% !important;
  align-self: flex-start !important;
}

.sj-curriculum-wrapper h3 {
  font-family: "Styrene A", -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
  font-size: 20px !important;
  font-weight: 500;
  line-height: 1.2;
  letter-spacing: 0.02em;
  color: #141413;
  margin-bottom: 16px;
  padding-left: 0;
}

#curriculum-list .lesson-modular,
#curriculum-list .lesson-video,
#curriculum-list .lesson-quiz,
#curriculum-list .lesson-pdf {
  height: 44px;
  padding: 5px 0px 10px 7px;
  border-bottom: 1px solid var(--border-border-subtle, #d1cfc5);
}

.section-container.tabs > section > .title a,
.section-container.tabs > .section > .title a {
  font-family: "Styrene B";
  font-size: 16px;
  letter-spacing: 0.015em;
}

.section-container.tabs section.active > .title a,
.section-container.tabs .section.active > .title a {
  font-weight: 500;
}

/* Course Info */
.sj-course-info-wrapper {
  margin: 0 !important;
  background: transparent !important;
  display: flex !important;
  flex-direction: column !important;
  align-items: flex-start !important;
  width: 100% !important;
}

.sj-course-info-wrapper h2 {
  font-size: 24px !important;
  font-weight: 400 !important;
  line-height: 1.5 !important;
  color: #141413 !important;
  margin: 32px 0 24px 0 !important;
  background-color: transparent !important;
  max-width: 100% !important;
}

/* Header and Navigation */
#header,
.header-link,
.header-left .left-nav-toggle {
  color: black !important;
}

#header-left .fa-times {
  margin-top: 27px;
}

#dp-details {
  display: flex;
  padding-bottom: var(--spacers-spacer-12, 160px);
  margin: 40px auto !important;
  max-width: 1200px !important;
  padding: 0 !important;
  display: flex !important;
  justify-content: space-between !important;
  align-items: flex-start !important;
  gap: 48px !important;
}

.top-row-white-v2 {
  color: #141413 !important;
  padding-bottom: 24px;
}

/* Lessons */
.lesson-modular,
.lesson-video,
.lesson-quiz,
.lesson-pdf {
  display: flex;
  align-items: center;
  padding: 7px 12px;
  height: 44px;
  border-bottom: 1px solid var(--border-border-subtle, #d1cfc5);
}

#lp-left-nav .lesson {
  color: #141413 !important;
}

.lp-left-nav .lesson-row {
  padding: 0;
  width: 100%;
}

.lp-left-nav .lesson {
  padding: 16px;
  color: #141413;
  font-variant-numeric: lining-nums proportional-nums;
  font-family: "Tiempos Text";
  font-size: 18px;
  font-style: normal;
  font-weight: 400;
  line-height: 100%;
}

#lp-left-nav .lesson:hover {
  background: #DEDCD1;
}

#lp-left-nav .lesson:active {
  background: #DEDCD1;
}

.lesson-top h2 {
  font-weight: 600;
}

.lp-color-scheme-light #lp-wrapper #lesson-body h2 {
  font-weight: 600;
}

.fa-circle-o.unviewed,
.lp-left-nav .lesson-row:hover .unviewed {
  color: #5a697c;
}

.fa-check-circle:before {
  content: "\f058";
  color: #141413;
}

#lp-left-nav .lesson-active,
#lp-left-nav .current-item,
.lp-color-scheme-light .lp-left-nav .lesson-row.lesson-active {
  background: none;
  width: 100%;
}
#lp-left-nav .lesson-active .title,
#lp-left-nav .current-item .title,
.lp-color-scheme-light .lp-left-nav .lesson-row.lesson-active .title {
  font-weight: 500;
}
.lesson #lp-left-nav .lesson-active,
.lesson #lp-left-nav .current-item,
.lesson .lp-color-scheme-light .lp-left-nav .lesson-row.lesson-active {
  background: #DEDCD1;
}

.lp-left-nav .lesson-row .title {
  max-width: 90%;
  font-family: "Tiempos", serif;
  font-size: 18px;
  font-weight: 400;
  color: #141413;
  letter-spacing: -0.015em;
  line-height: 1.4;
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}

#curriculum-list-2 a {
  display: flex;
  height: 44px;
}

/* LESSON PAGES */
#cp-content #curriculum-list .lesson-section h3,
#cp-content #curriculum-list .lesson-section .h3-style,
h3.section-title {
  font-family: "Styrene B", -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
  font-size: 18px !important;
  font-weight: 500 !important;
  line-height: 1.2;
  letter-spacing: 0;
  color: #141413;
  margin-bottom: 0;
  padding-left: 0;
}

#cp-content #curriculum-list .lesson-row .bullet i {
  font-size: 1em;
}

/* Video Content Page */
#cp-content-2 .lesson-modular {
  height: 44px;
  padding: 11px 12px 7px 12px;
  gap: 6px;
  border-bottom: 1px solid var(--border-border-subtle, #d1cfc5) !important;
}

#returnToOverview {
  padding-bottom: 20px;
  font-size: 18px;
  text-decoration: underline;
  font-family: "Styrene B";
  color: #615f5b !important;
}

#lp-wrapper #lp-content #details-pane.columns #details-pane-inner #details-pane-summary-content {
  display: flex !important;
  flex-direction: column-reverse;
}

/* Background Color Overrides */
#lesson-main,
#lp-left-nav,
.white-bg,
.top-row-white-v2,
#skilljar-content,
#header,
.header,
.section-container.tabs > section.active > .title,
.section-container.tabs > .section.active > .title,
.section-container.tabs > section > .content,
.section-container.tabs > .section > .content,
.section-title {
  background-color: #faf9f5 !important;
}

#skilljar-content {
  min-height: 75% !important;
  margin-bottom: 0px !important;
}

.lp-color-scheme-light.lp-left-nav.section-title,
.lp-color-scheme-light .lp-left-nav .section-title {
  background: none !important;
}

body.cbp-spmenu-fixed.sj-page-lesson:not(.lesson-fullscreen) #lp-left-nav {
  width: 420px;
}

.cbp-spmenu-open #lp-wrapper,
body.cbp-spmenu-fixed.sj-page-lesson:not(.lesson-fullscreen) #lp-wrapper {
  margin-left: 420px;
}

#lp-left-nav,
.lp-color-scheme-light #lp-wrapper,
#dp-details [class*=curriculum],
#dp-details [id*=curriculum],
.course-outline,
.course-content,
.lessons-list {
  background-color: #f0eee6 !important;
}

.lp-color-scheme-light.lp-left-nav.section-title,
.lp-color-scheme-light .lp-left-nav .section-title {
  background: none !important;
  border-bottom: none;
  padding: 32px 16px 0px 16px;
}

/* Tooltips */
.tooltips {
  border-bottom: 1px solid var(--border-border-subtle, #d1cfc5);
  display: flex;
  padding: var(--spacers-spacer-04, 16px) var(--spacers-spacer-03, 12px) var(--spacers-spacer-04, 16px) 0px;
  align-items: center;
  align-self: stretch;
}

/* Utility Classes */
* {
  box-shadow: none !important;
}

.focus-link-v2 {
  color: black !important;
}

/* Sign-in Section */
.signin {
  color: #666 !important;
  font-size: 14px !important;
  margin-top: 10px !important;
  background-color: transparent !important;
  text-align: right !important;
}

.signin a {
  color: #000 !important;
  text-decoration: underline !important;
  font-weight: 400 !important;
}

/* Login form styles */
#login-content {
  max-width: 810px !important;
  padding-bottom: 48px !important;
}

/* FAQ section spacing on login page */
.v1.sj-page-login .b-faq-list {
  padding-top: 0 !important;
  margin-top: 0 !important;
}

/* Replace video icon with custom SVG */
.lesson-modular .type-icon sjwc-icon,
.lesson-video .type-icon sjwc-icon {
  display: none !important;
}

.lp-left-nav .lesson-row .type-icon {
  display: none !important;
}

.lesson-modular .type-icon,
.lesson-video .type-icon {
  width: 15px;
  height: 12px;
  margin-bottom: 2px;
  background-image: url('data:image/svg+xml;charset=UTF-8,%3Csvg xmlns="http://www.w3.org/2000/svg" width="11" height="12" viewBox="0 0 11 12" fill="none"%3E%3Cpath d="M9.6252 5.14956C10.2571 5.54045 10.2571 6.45955 9.6252 6.85044L2.02607 11.5512C1.35987 11.9633 0.5 11.4841 0.5 10.7007L0.500001 1.29926C0.500001 0.5159 1.35987 0.0367168 2.02607 0.448822L9.6252 5.14956Z" fill="%23D1CFC5"/%3E%3C/svg%3E');
  background-repeat: no-repeat;
  background-position: center;
  background-size: contain;
  display: inline-block;
  flex-shrink: 0;
}

/* Standardize icon container size for quiz and PDF */
.lesson-quiz .type-icon,
.lesson-pdf .type-icon {
  width: 15px !important;
  height: 12px !important;
  margin-bottom: 2px;
  display: inline-block !important;
  flex-shrink: 0;
}

.lesson-quiz .type-icon sjwc-icon,
.lesson-pdf .type-icon sjwc-icon {
  display: none !important;
  width: 15px !important;
  height: 12px !important;
}

.lesson-quiz .type-icon i,
.lesson-pdf .type-icon i {
  font-size: 14px !important;
  line-height: 12px !important;
  display: none !important;
}

ul.dp-curriculum .lesson-quiz .type-icon i,
ul.dp-curriculum .lesson-pdf .type-icon i {
  position: relative;
  top: -7px;
  left: -8px;
  display: none !important;
}

.lesson-modular .lesson-wrapper,
.lesson-video .lesson-wrapper,
.lesson-quiz .lesson-wrapper,
.lesson-pdf .lesson-wrapper {
  display: flex;
  align-items: center;
  flex: 1;
}

.lesson-modular .lesson-wrapper > div,
.lesson-video .lesson-wrapper > div,
.lesson-quiz .lesson-wrapper > div,
.lesson-pdf .lesson-wrapper > div {
  display: flex;
  align-items: center;
}

#cp-content #curriculum-list .lesson-row .type-icon {
  margin-top: 0px;
  display: none !important;
}

/* Hide Elements */
.social-media-wrapper {
  display: none !important;
}

.ep-footer,
#ep-footer {
  display: none;
}

.columns.sj-curriculum-wrapper.large-4 {
  vertical-align: top !important;
  margin-top: 0 !important;
  box-sizing: border-box;
  padding: 32px;
}

.columns.sj-curriculum-wrapper.large-4 h3 {
  font-family: "Styrene A", -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
  font-size: 20px;
  font-weight: 500;
  line-height: 1.2;
  letter-spacing: 0.02em;
  color: #141413;
  margin-bottom: 16px;
  padding-left: 0;
  padding-top: 0px;
  margin-top: 0;
}

.section.tooltips {
  margin-bottom: 5px;
  padding-bottom: 0;
}

/* Equal height columns with scrolling - best practices approach */
.row.hide-for-small.padded-side-bottom {
  display: flex !important;
  min-height: 100vh !important;
  /* Minimum height but can expand */
  align-items: flex-start !important;
  /* Align items to the top */
}

/* Left column - About this course */
.row.hide-for-small.padded-side-bottom > .large-7.columns {
  flex: 1 1 60% !important;
  overflow: visible !important;
  /* No scrolling */
  background-color: #faf9f5 !important;
  padding: 20px !important;
  padding-top: 0 !important;
  /* Align with right column */
  float: none !important;
  /* Remove float */
  margin-top: 0 !important;
  /* Ensure no top margin */
  margin-right: 48px;
}

/* Right column - Curriculum */
.row.hide-for-small.padded-side-bottom > .columns.sj-curriculum-wrapper.large-4 {
  flex: 0 0 auto !important;
  width: 469px !important;
  /* Limit height to viewport */
  overflow-y: auto !important;
  /* Enable vertical scrolling */
  overflow-x: hidden !important;
  float: none !important;
  /* Remove float */
  margin-left: 20px !important;
  margin-top: 0 !important;
  /* Ensure no top margin */
  align-self: flex-start !important;
  /* Align to top, don't stretch */
}

/* Curriculum navigation styling */
ul.dp-curriculum {
  margin: 0;
  padding: 0;
  list-style: none;
  width: 100%;
}

ul.dp-curriculum li.section {
  padding-left: 0;
  font-size: 18px;
  font-weight: 500;
  font-family: "Styrene B";
  padding-bottom: 4px;
  margin-bottom: 8px;
  color: #141413;
}

ul.dp-curriculum li.lesson-modular,
ul.dp-curriculum li.lesson-video,
ul.dp-curriculum li.lesson-quiz,
ul.dp-curriculum li.lesson-pdf {
  padding-left: 0;
  font-size: 18px;
  font-weight: 400;
  font-family: "Tiempos";
  color: #333333;
  display: flex;
  align-items: center;
}

li.section.tooltips {
  padding-left: 0;
  font-size: 16px;
  font-weight: 500;
  font-family: "Styrene B";
}

/* Lesson page specific layout */
.lesson-page {
  display: flex !important;
  height: 100vh !important;
}

/* Left navigation on lesson pages - make it scrollable */
.lesson-page #lp-left-nav {
  flex: 0 0 300px !important;
  overflow-y: auto !important;
  overflow-x: hidden !important;
  background-color: #f0eee6 !important;
}

/* Right content area on lesson pages - full height no scroll */
.lesson-page #lp-wrapper {
  flex: 1 !important;
  height: 100vh !important;
  display: flex !important;
  flex-direction: column !important;
  overflow: auto !important;
}

.lesson-page #lesson-body {
  flex: 1 !important;
  display: flex !important;
  flex-direction: column !important;
}

.lesson-page #lesson-main {
  flex: 1 !important;
  display: flex !important;
  flex-direction: column !important;
}

.lesson-page #lesson-main-inner {
  flex: 1 !important;
  display: flex !important;
  flex-direction: column !important;
}

.lesson-page #lesson-main-content {
  flex: 1 !important;
}

/* Video container should fit within available space */
.lesson-page .course-fixed-content-video {
  aspect-ratio: 16/9;
  width: 100%;
  max-height: calc(100vh - 60px - 49px) !important;
}
@media (max-width: 640px) {
  .lesson-page .course-fixed-content-video {
    max-height: 100%;
  }
}

.lesson-page .video-max {
  align-items: center !important;
}
.lesson-page .video-max .flex-video.widescreen {
  padding-bottom: 40px;
  margin-top: 0px !important;
  width: 100%;
  height: 100%;
  object-fit: cover;
}

body.lesson-fullscreen #lp-wrapper #lesson-body #details-pane.columns {
  display: block !important;
}

/* Footer should stay at bottom */
.lesson-page #lp-footer {
  flex: 0 0 auto !important;
  display: flex;
  justify-content: space-between;
  align-items: center;
}
.lesson-page #lp-footer:not(:has(.left)) {
  justify-content: flex-end;
}
.lesson-page #lp-footer .lesson-title-footer {
  font-weight: 500;
}
.lesson-page #lp-footer .button.next-lesson-link {
  border-radius: 8px;
  cursor: pointer;
  font-weight: 500;
  font-family: inherit;
  display: inline-block;
  text-decoration: none;
  line-height: 1;
  transition: all 0.2s ease;
  margin: 0;
  padding: 0.625rem 1.5rem;
  font-size: 0.875rem;
  background: #2c2b25;
  color: white;
  border: none;
}
.lesson-page #lp-footer .button.next-lesson-link:hover {
  opacity: 0.85;
  color: white;
  text-decoration: none;
}
.lesson-page #lp-footer .button.next-lesson-link:focus {
  background: #5e5b4e;
  border-color: unset;
  color: white;
  outline: none !important;
}
.lesson-page #lp-footer .button.next-lesson-link:focus:before {
  outline: none !important;
  box-shadow: none !important;
  border: unset !important;
}

/********* MOBILE STYLES **********/
@media only screen and (min-width: 768px) {
  #lp-wrapper #lp-content {
    height: unset;
  }
}
@media only screen and (max-width: 767px) {
  .lesson-page #lesson-body {
    padding: 0;
  }
  body.sj-page-catalog .catalog-header h1.break-word,
  body.sj-page-catalog .catalog-header h1,
  .sj-page-catalog .catalog-header h1.break-word,
  .sj-page-catalog .catalog-header h1 {
    font-size: 64px !important;
  }
  /* Fix body spacing for mobile */
  body {
    padding: 16px !important;
    gap: 24px !important;
  }
  /* Fix main container layout */
  .dp-row-flex-v2 {
    flex-direction: column !important;
    align-items: stretch !important;
  }
  /* Fix course info wrapper */
  .sj-course-info-wrapper {
    flex-direction: column !important;
    align-items: flex-start !important;
    gap: 16px !important;
  }
  .sj-course-info-wrapper h2 {
    font-size: 20px !important;
    width: 100% !important;
    margin-bottom: 16px !important;
  }
  button,
  .button {
    font-family: "Styrene B";
    font-size: 16px;
    letter-spacing: 0.015em;
  }
  /* Fix purchase button wrapper */
  .purchase-button-wrapper {
    width: 100% !important;
    align-items: flex-start !important;
    margin-bottom: 16px !important;
  }
  #purchase-button {
    width: 100% !important;
    top: 0 !important;
  }
  /* Fix already registered text */
  .signin {
    width: 100% !important;
    text-align: left !important;
    margin-top: 8px !important;
  }
  .main-container,
  h1,
  h2,
  h3,
  h4,
  h5,
  h6,
  .v1.sj-page-curriculum,
  .v1.sj-page-lesson {
    font-size: 22px !important;
  }
  .main-container,
  p,
  ul,
  li,
  .v1.sj-page-curriculum,
  .v1.sj-page-lesson {
    font-size: 18px;
    line-height: 1.6;
  }
  /* Fix curriculum wrapper */
  .sj-curriculum-wrapper {
    width: 100% !important;
    margin-left: 0 !important;
    margin-top: 24px !important;
    padding: 24px !important;
    box-sizing: border-box !important;
  }
  /* Fix row layout for mobile */
  .row.hide-for-small.padded-side-bottom {
    flex-direction: column !important;
    min-height: auto !important;
  }
  .row.hide-for-small.padded-side-bottom > .large-7.columns {
    flex: none !important;
    width: 100% !important;
    padding: 0 !important;
    margin-bottom: 24px !important;
  }
  .row.hide-for-small.padded-side-bottom > .columns.sj-curriculum-wrapper.large-4 {
    flex: none !important;
    width: 100% !important;
    max-height: none !important;
    margin-left: 0 !important;
    box-sizing: border-box !important;
    padding: 24px !important;
  }
  /* Fix columns layout */
  .columns,
  .column {
    width: 100% !important;
    margin-left: 0 !important;
    padding: 0 !important;
  }
  .large-7 {
    width: 100% !important;
    margin-left: 0 !important;
  }
  #dp-details {
    display: block !important;
  }
  .row.show-for-small {
    display: none !important;
  }
  /* Course page mobile adjustments */
  .v1.sj-page-detail.sj-page-detail-course {
    /* Main page mobile adjustments */
  }
  .v1.sj-page-detail.sj-page-detail-course .row.dp-row-flex-v2 {
    flex-direction: column !important;
    display: flex !important;
  }
  .v1.sj-page-detail.sj-page-detail-course .columns.text-center.push-3.large-6.dp-summary-wrapper {
    order: 10 !important;
    width: 100% !important;
    flex: none !important;
    margin-bottom: 24px !important;
  }
  .v1.sj-page-detail.sj-page-detail-course .columns.large-6.dp-promo-image-wrapper {
    order: -1 !important;
    width: 100% !important;
    flex: none !important;
    margin-bottom: 24px !important;
  }
  .v1.sj-page-detail.sj-page-detail-course .dp-promo-image img {
    max-width: 200px !important;
    height: auto !important;
  }
  .v1.sj-page-detail.sj-page-detail-course .sj-course-info-wrapper h2 {
    margin: 16px 0 16px 0 !important;
    font-size: 18px !important;
  }
  .v1.sj-page-detail.sj-page-detail-course h1.break-word {
    font-size: 36px !important;
  }
  .v1.sj-page-detail.sj-page-detail-course #skilljar-content {
    padding-top: 60px !important;
  }
  .v1.sj-page-detail.sj-page-detail-course h1.break-word {
    font-size: 48px !important;
  }
  /* Lesson row title - mobile only */
  #cp-content #curriculum-list .lesson-row .title {
    overflow: visible;
    left: 38px;
    margin-top: 10px;
    font-size: 16px;
  }
  .lp-left-nav .lesson-row .title {
    max-width: 90%;
    font-family: "Tiempos", serif;
    font-size: 16px;
    font-weight: 400;
    color: #141413;
    letter-spacing: -0.015em;
    line-height: 1.4;
  }
  /* Mobile layout for the curriculum page grid */
  .v1.sj-page-curriculum .dp-row-flex-v2 {
    display: flex !important;
    flex-direction: column !important;
    grid-template-columns: none !important;
    grid-template-areas: none !important;
    align-items: center !important;
    gap: 24px !important;
  }
  .v1.sj-page-curriculum .dp-row-flex-v2 h1.break-word {
    order: 1 !important;
    text-align: center !important;
  }
  .v1.sj-page-curriculum .course-description {
    order: 2 !important;
    text-align: center !important;
    margin-left: auto !important;
    margin-right: auto !important;
    padding: 0 20px !important;
  }
  .v1.sj-page-curriculum .dp-row-flex-v2 .cp-promo-image {
    order: 3 !important;
  }
  .v1.sj-page-curriculum .dp-row-flex-v2 .cp-lessons {
    order: 4 !important;
  }
  .v1.sj-page-curriculum .dp-row-flex-v2 .progress-bar {
    order: 5 !important;
    width: 100% !important;
  }
  .v1.sj-page-curriculum span.progress-bar-inner.button-background {
    background: #DEDCD1 !important;
  }
  .v1.sj-page-curriculum .dp-row-flex-v2 #resume-button {
    order: 6 !important;
  }
}
.dp-summary-wrapper,
.cp-summary-wrapper {
  border: none;
}

/* Breadcrumb for catalog page */
.catalog-center-width.one-col-full::before {
  content: "Anthropic Academy  /  Courses" !important;
  display: block !important;
  color: var(--text-text-color, #141413) !important;
  font-variant-numeric: lining-nums proportional-nums;
  font-family: var(--typography-detail, "Styrene B LC") !important;
  font-size: var(--type-scales-detail-m, 16px) !important;
  font-style: normal !important;
  font-weight: 400 !important;
  line-height: 125% !important;
  letter-spacing: -0.08px !important;
}
@media (min-width: 1025px) {
  .catalog-center-width.one-col-full::before {
    margin-top: 100px !important;
    text-align: left !important;
    width: 100% !important;
    max-width: 1200px !important;
    margin-left: auto !important;
    margin-right: auto !important;
  }
}

/* Catalog header h1 spacing */
.catalog-header h1 {
  margin-top: 48px !important;
}

/* Remove padding on catalog header */
.catalog-center-width .catalog-header {
  padding-left: 0 !important;
}

/* FAQ Data and Privacy heading */
.b-faq-list h2.h3 {
  font-size: 32px !important;
  line-height: 1.2 !important;
}

/* FAQ question titles */
.faqs-container .faq-title.h4 {
  font-size: 20px !important;
  line-height: 1.3 !important;
}

/* Mobile adjustments for FAQ */
@media only screen and (max-width: 767px) {
  .b-faq-list h2.h3 {
    font-size: 24px !important;
  }
  .faqs-container .faq-title.h4 {
    font-size: 18px !important;
  }
  .cbp-spmenu-open #lp-wrapper,
  body.cbp-spmenu-fixed.sj-page-lesson:not(.lesson-fullscreen) #lp-wrapper {
    margin-left: 0px;
  }
  #left-nav-button {
    font-size: 14px;
  }
  body.cbp-spmenu-fixed.sj-page-lesson:not(.lesson-fullscreen) #lp-left-nav {
    width: 300px;
  }
  #toggle-header-mobile-dropdown {
    font-size: 16px;
  }
  .flex-video.widescreen {
    padding-bottom: 11.25%;
  }
}
/* Breadcrumb for course detail page */
.v1.sj-page-detail.sj-page-detail-course h1.break-word::before {
  content: "Anthropic Academy  /  Courses" !important;
  display: block !important;
  color: var(--text-text-color, #141413) !important;
  font-variant-numeric: lining-nums proportional-nums;
  /* detail/m */
  font-family: var(--typography-detail, "Styrene B LC") !important;
  font-size: var(--type-scales-detail-m, 16px) !important;
  font-style: normal !important;
  font-weight: 400 !important;
  line-height: 125% !important;
  /* 20px */
  letter-spacing: -0.08px !important;
  margin-bottom: 48px !important;
  text-align: left !important;
}

/* Ensure h1 alignment matches breadcrumb */
.v1.sj-page-detail.sj-page-detail-course .dp-summary-wrapper h1.break-word {
  text-align: left !important;
}

@media (min-width: 1200px) {
  .v1.sj-page-login .b-faq-list,
  .v1.sj-page-register .b-faq-list {
    padding-top: 0px !important;
  }
}
.sj-text-downloads span {
  font-family: "Styrene A" !important;
  font-weight: normal !important;
}

h1,
.h1-style,
h2,
.h2-style,
h3,
.h3-style,
h4,
.h4-style,
h5,
h6 {
  margin-bottom: 0.25em;
}

#song-wrapper {
  position: relative;
}
#song-wrapper #song {
  position: absolute;
  top: -15px;
  right: 0px;
  font-size: 5px;
  cursor: pointer;
}

#karaoke-lyrics {
  position: fixed;
  bottom: 80px;
  left: 50%;
  transform: translateX(-50%);
  font-size: 36px;
  line-height: 1.6;
  text-align: center;
  max-width: 900px;
  width: 90%;
  transition: opacity 0.3s ease;
  background: rgba(255, 255, 255, 0.95);
  padding: 30px 40px;
  border-radius: 12px;
  box-shadow: 0 4px 20px rgba(0, 0, 0, 0.15);
}

#karaoke-lyrics.karaoke-hidden {
  opacity: 0;
  pointer-events: none;
}

.karaoke-line {
  margin: 15px 0;
  transition: all 0.3s ease;
}

.karaoke-line.karaoke-upcoming {
  color: #999;
  font-size: 28px;
}

.karaoke-line.karaoke-current {
  color: #d97959;
  font-weight: bold;
  font-size: 42px;
}

.karaoke-line.karaoke-past {
  color: #bbb;
  font-size: 28px;
}

</style>
    

    
    
    
<meta class="foundation-mq-small"><meta class="foundation-mq-medium"><meta class="foundation-mq-large"><style></style><style>
    #lp-wrapper #lp-content #details-pane.columns #details-pane-inner #details-pane-content #details-pane-summary-content {
      flex-direction: column !important;
    }
  </style><style type="text/css">.vidyard-player-container .play-button{position:absolute;width:16%;height:auto;border-radius:50%;border:none;cursor:pointer;opacity:.65;filter:alpha(opacity = 65);transition:opacity .2s linear;overflow:hidden;font-size:0;padding:0;min-width:20px;top:50%;left:50%;transform:translate(-50%,-50%);-webkit-appearance:initial!important;-moz-appearance:initial!important;appearance:initial!important}.vidyard-player-container .play-button .play-button-size{padding-top:100%;width:100%}.vidyard-player-container .play-button .arrow-size{position:absolute;top:50%;left:50%;width:35%;height:auto;margin:-25% 0 0 -12%;overflow:hidden}.vidyard-player-container .play-button .arrow-size-ratio{padding-top:150%;width:100%}.vidyard-player-container .play-button .arrow{position:absolute;top:50%;left:auto;right:0;bottom:auto;width:0;height:0;margin:-200px 0 -200px -300px;border:200px solid transparent;border-left:300px solid #fff;border-right:none}.vidyard-lightbox-thumbnail:hover .play-button{opacity:1;filter:alpha(opacity = 100);zoom:1}.vidyard-player-container{position:relative;height:100%;text-align:center}.vidyard-player-container img{height:100%}.vidyard-player-container .play-button{display:none}.vidyard-close-container{position:fixed;right:20px;top:20px;height:34px;width:34px;cursor:pointer;z-index:1000}.vidyard-close-container:focus{outline:1px dotted grey}.vidyard-close-x{position:absolute;height:100%;width:100%;color:#fff;font-size:2em;text-align:center;line-height:34px}.vidyard-close-x:hover{color:#ddd}.vidyard-close-x:hover:after,.vidyard-close-x:hover:before{background:#ddd}.vidyard-close-x:after,.vidyard-close-x:before{content:"";position:absolute;background:#fff;display:block;left:50%;top:50%;height:65%;width:2px;transition:all .2s;-ms-high-contrast-adjust:none}.vidyard-close-x:before{transform:translate(-50%,-50%) rotate(45deg);-ms-transform:translate(-50%,-50%) rotate(45deg)}.vidyard-close-x:after{transform:translate(-50%,-50%) rotate(-45deg);-ms-transform:translate(-50%,-50%) rotate(-45deg)}.vidyard-close-x.simple-close:after,.vidyard-close-x.simple-close:before{display:none}.vidyard-lightbox-thumbnail{width:100%;height:100%;margin:auto}.vidyard-lightbox-image{height:100%;left:0;position:absolute;top:0;width:100%}.vidyard-lightbox-centering{cursor:pointer;height:0;max-width:100%;overflow:hidden;padding-bottom:56.25%;position:relative}.vidyard-lightbox-content-backer{-webkit-transform:opacity 1s,filter 1s;-ms-transform:opacity 1s,filter 1s;transition:opacity 1s,filter 1s;background-color:#000;height:100%;width:100%;position:absolute}#vidyard-overlay-wrapper,.vidyard-lightbox-content-backer{filter:alpha(opacity = 0);opacity:0;top:0;right:0;bottom:0;left:0}#vidyard-overlay-wrapper{position:relative;box-sizing:border-box;display:none;transition:opacity .5s,filter .5s}#vidyard-overlay{top:0;right:0;bottom:0;left:0;opacity:.9;filter:alpha(opacity = 90);width:100%;height:100%;background-color:#000;z-index:800}#vidyard-content-fixed,#vidyard-overlay{position:fixed;box-sizing:border-box;display:none}#vidyard-content-fixed{opacity:1;z-index:900;text-align:center;top:5%;right:5%;bottom:5%;left:5%;width:90%}#vidyard-popbox{display:inline-block;position:absolute;left:50%;top:50%;-webit-transform:translate(-50%,-50%);-ms-transform:translate(-50%,-50%);transform:translate(-50%,-50%)}#vidyard-popbox-constraint{opacity:0;filter:alpha(opacity = 0);display:block;visibility:hidden}#vidyard-popbox-constraint.landscape{height:90vh}#vidyard-popbox-constraint.portrait{width:90vw}.vidyard-player-container div[class^=vidyard-iframe-]{z-index:1}.vidyard-player-container div[class^=vidyard-div-]{background-repeat:no-repeat;background-position:0 50%;background-size:100%}img.vidyard-player-embed{width:100%}img.vidyard-player-embed.inserted{position:absolute;top:0;left:0;z-index:0;max-width:100%!important}.vidyard-player-container.playlist-open{padding-right:319px;width:auto!important}.vidyard-player-container.playlist-open div[class^=vidyard-div-]{width:calc(100% + 319px);max-width:calc(100% + 319px)!important;background-size:calc(100% - 319px);background-color:#f5f9ff}.vidyard-player-container.playlist-open div[class^=vidyard-div-] img.vidyard-player-embed{width:calc(100% - 319px)!important}#backlink-icon{height:15px;width:15px;margin-right:6px;transition:.3s}#backlink{align-items:center;border-radius:4px;border:3px solid #ebeeff;display:inline-block;float:left;line-height:18px;margin:8px 0 0;outline:none;padding:1px 8px 1px 5px;position:relative;*zoom:1;font-family:Arial,Helvetica Neue,Helvetica,sans-serif;font-style:normal;font-weight:400;font-size:12px;text-decoration:none}#backlink:after,#backlink:before{content:" ";display:table}#backlink:after{clear:both}#backlink:link,#backlink:visited{background:#ebeeff;border-color:#ebeeff;color:#414dd4}#backlink:hover{background:#bfc2ff;color:#1b1a82;cursor:pointer}#backlink:focus,#backlink:hover{border-color:#bfc2ff}#backlink:active{background:#8f97ff;border-color:#8f97ff;color:#0c084d}#backlink-icon{float:left;height:18px;margin-right:5px;position:relative;width:18px}#backlink-text{float:left}</style><style type="text/css" data-fbcssmodules="css:fb.css.base css:fb.css.dialog css:fb.css.iframewidget">.fb_hidden{position:absolute;top:-10000px;z-index:10001}.fb_reposition{overflow:hidden;position:relative}.fb_invisible{display:none}.fb_reset{background:none;border:0px;border-spacing:0;color:#000;cursor:auto;direction:ltr;font-family:lucida grande,tahoma,verdana,arial,sans-serif;font-size:11px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;line-height:1;margin:0;overflow:visible;padding:0;text-align:left;text-decoration:none;text-indent:0;text-shadow:none;text-transform:none;visibility:visible;white-space:normal;word-spacing:normal}.fb_reset>div{overflow:hidden}@keyframes fb_transform{0%{opacity:0;transform:scale(.95)}to{opacity:1;transform:scale(1)}}.fb_animate{animation:fb_transform .3s forwards}

.fb_hidden{position:absolute;top:-10000px;z-index:10001}.fb_reposition{overflow:hidden;position:relative}.fb_invisible{display:none}.fb_reset{background:none;border:0px;border-spacing:0;color:#000;cursor:auto;direction:ltr;font-family:lucida grande,tahoma,verdana,arial,sans-serif;font-size:11px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;line-height:1;margin:0;overflow:visible;padding:0;text-align:left;text-decoration:none;text-indent:0;text-shadow:none;text-transform:none;visibility:visible;white-space:normal;word-spacing:normal}.fb_reset>div{overflow:hidden}@keyframes fb_transform{0%{opacity:0;transform:scale(.95)}to{opacity:1;transform:scale(1)}}.fb_animate{animation:fb_transform .3s forwards}

.fb_dialog{background:#525252b3;position:absolute;top:-10000px;z-index:10001}.fb_dialog_advanced{border-radius:8px;padding:10px}.fb_dialog_content{background:#fff;color:#373737}.fb_dialog_close_icon{background:url(https://connect.facebook.net/rsrc.php/v4/yq/r/IE9JII6Z1Ys.png) no-repeat scroll 0 0 transparent;cursor:pointer;display:block;height:15px;position:absolute;right:18px;top:17px;width:15px}.fb_dialog_mobile .fb_dialog_close_icon{left:5px;right:auto;top:5px}.fb_dialog_padding{background-color:transparent;position:absolute;width:1px;z-index:-1}.fb_dialog_close_icon:hover{background:url(https://connect.facebook.net/rsrc.php/v4/yq/r/IE9JII6Z1Ys.png) no-repeat scroll 0 -15px transparent}.fb_dialog_close_icon:active{background:url(https://connect.facebook.net/rsrc.php/v4/yq/r/IE9JII6Z1Ys.png) no-repeat scroll 0 -30px transparent}.fb_dialog_iframe{line-height:0}.fb_dialog_content .dialog_title{background:#6d84b4;border:1px solid #365899;color:#fff;font-size:14px;font-weight:700;margin:0}.fb_dialog_content .dialog_title>span{background:url(https://connect.facebook.net/rsrc.php/v4/yd/r/Cou7n-nqK52.gif) no-repeat 5px 50%;float:left;padding:5px 0 7px 26px}body.fb_hidden{height:100%;left:0;margin:0;overflow:visible;position:absolute;top:-10000px;transform:none;width:100%}.fb_dialog.fb_dialog_mobile.loading{background:url(https://connect.facebook.net/rsrc.php/v4/ya/r/3rhSv5V8j3o.gif) #fff no-repeat 50% 50%;min-height:100%;min-width:100%;overflow:hidden;position:absolute;top:0;z-index:10001}.fb_dialog.fb_dialog_mobile.loading.centered{background:none;height:auto;min-height:initial;min-width:initial;width:auto}.fb_dialog.fb_dialog_mobile.loading.centered #fb_dialog_loader_spinner{width:100%}.fb_dialog.fb_dialog_mobile.loading.centered .fb_dialog_content{background:none}.loading.centered #fb_dialog_loader_close{clear:both;color:#fff;display:block;font-size:18px;padding-top:20px}#fb-root #fb_dialog_ipad_overlay{background:#0006;inset:0;min-height:100%;position:absolute;width:100%;z-index:10000}#fb-root #fb_dialog_ipad_overlay.hidden{display:none}.fb_dialog.fb_dialog_mobile.loading iframe{visibility:hidden}.fb_dialog_mobile .fb_dialog_iframe{position:sticky;top:0}.fb_dialog_content .dialog_header{background:linear-gradient(from(#738aba),to(#2c4987));border-bottom:1px solid;border-color:#043b87;box-shadow:#fff 0 1px 1px -1px inset;color:#fff;font:700 14px Helvetica,sans-serif;text-overflow:ellipsis;text-shadow:rgba(0,30,84,.296875) 0px -1px 0px;vertical-align:middle;white-space:nowrap}.fb_dialog_content .dialog_header table{height:43px;width:100%}.fb_dialog_content .dialog_header td.header_left{font-size:12px;padding-left:5px;vertical-align:middle;width:60px}.fb_dialog_content .dialog_header td.header_right{font-size:12px;padding-right:5px;vertical-align:middle;width:60px}.fb_dialog_content .touchable_button{background:linear-gradient(from(#4267B2),to(#2a4887));background-clip:padding-box;border:1px solid #29487d;border-radius:3px;display:inline-block;line-height:18px;margin-top:3px;max-width:85px;padding:4px 12px;position:relative}.fb_dialog_content .dialog_header .touchable_button input{background:none;border:none;color:#fff;font:700 12px Helvetica,sans-serif;margin:2px -12px;padding:2px 6px 3px;text-shadow:rgba(0,30,84,.296875) 0px -1px 0px}.fb_dialog_content .dialog_header .header_center{color:#fff;font-size:16px;font-weight:700;line-height:18px;text-align:center;vertical-align:middle}.fb_dialog_content .dialog_content{background:url(https://connect.facebook.net/rsrc.php/v4/y9/r/jKEcVPZFk-2.gif) no-repeat 50% 50%;border:1px solid #4A4A4A;border-bottom:0;border-top:0;height:150px}.fb_dialog_content .dialog_footer{background:#f5f6f7;border:1px solid #4A4A4A;border-top-color:#ccc;height:40px}#fb_dialog_loader_close{float:left}.fb_dialog.fb_dialog_mobile .fb_dialog_close_icon{visibility:hidden}#fb_dialog_loader_spinner{animation:rotateSpinner 1.2s linear infinite;background-color:transparent;background-image:url(https://connect.facebook.net/rsrc.php/v4/yD/r/t-wz8gw1xG1.png);background-position:50% 50%;background-repeat:no-repeat;height:24px;width:24px}@keyframes rotateSpinner{0%{transform:rotate(0)}to{transform:rotate(360deg)}}

.fb_iframe_widget{display:inline-block;position:relative}.fb_iframe_widget span{display:inline-block;position:relative;text-align:justify}.fb_iframe_widget iframe{position:absolute}.fb_iframe_widget_fluid_desktop,.fb_iframe_widget_fluid_desktop span,.fb_iframe_widget_fluid_desktop iframe{max-width:100%}.fb_iframe_widget_fluid_desktop iframe{min-width:220px;position:relative}.fb_iframe_widget_lift{z-index:1}.fb_iframe_widget_fluid{display:inline}.fb_iframe_widget_fluid span{width:100%}
</style></head>
<body class="v1 sj-page-detail sj-page-detail-course  sj-nested-header-links sj-nested-footer-links" style="">
<div id="main-container" data-course="ai-fluency-framework-foundations" data-tags="">
    
    

    
        
        
    
















<header id="header">
    <div id="header-left" class="header-left">

        
        
            
                
                
                
                  <a href="https://anthropic.skilljar.com/" class="back-to-catalog  animated-button-pair animated-button-pair-stacked header-link focus-link-v2" title="Anthropic Courses" aria-label="Back to Anthropic Courses">
                      <i class="fa fa-arrow-left" aria-hidden="true"></i>
                      <span id="backToCatalogShortText">
                          
                      </span>
                  </a>
                
            
        


        <a href="https://anthropic.com/" class="header-logo-link focus-link-v2">

            
                <img class="header-center-img  " src="./AI Fluency_ Framework &amp; Foundations_files/header-logo.1749494808.png" alt="Go home">
            

        </a>

    </div>

    <div id="header-right">

        
            <div class="header-links-container headerheight align-vertical" aria-labelledby="header-links-navigation">
                <h2 id="header-links-navigation" class="hide">Header Navigation</h2>
                
                  


  
    <a href="https://www.anthropic.com/learn" target="_self" class="header-link header-link--base focus-link-v2">
      Anthropic Academy
    </a>
  

  
    <a href="https://anthropic.skilljar.com/" target="_self" class="header-link header-link--base focus-link-v2">
      Courses
    </a>
  


<script>
  document.addEventListener('DOMContentLoaded',  () => {
    const DROPDOWN_SELECTOR = '.header-link--dropdown';
    const MENU_SELECTOR = '.header-dropdown__menu';
    const MENU_LINK_SELECTOR = '.header-dropdown__link';
    const BUTTON_OPEN_CLASS = 'header-link--open';
    const dropdowns = document.querySelectorAll(DROPDOWN_SELECTOR);
    const closeAllDropdowns = () => {
      dropdowns.forEach(button => {
        button.classList.remove(BUTTON_OPEN_CLASS);
        button.setAttribute('aria-expanded', 'false');
      });
    }

    document.addEventListener('click', (e) => {
      if (![...dropdowns].some(dropdown => dropdown.contains(e.target))) {
        closeAllDropdowns();
      }
    });

    dropdowns.forEach(button => {
      const menu = button.querySelector(MENU_SELECTOR);
      const getIsDropdownOpen = () => button.classList.contains(BUTTON_OPEN_CLASS);
      const openDropdown = () => {
        button.classList.add(BUTTON_OPEN_CLASS);
        button.setAttribute('aria-expanded', 'true');
      };
      const closeDropdown = () => {
        button.classList.remove(BUTTON_OPEN_CLASS);
        button.setAttribute('aria-expanded', 'false');
      }

      button.addEventListener('mouseenter',  () => {
        closeAllDropdowns();
        if (!getIsDropdownOpen()) {
          openDropdown();
        }
      });

      button.addEventListener('mouseleave',  () => {
        if (getIsDropdownOpen()) {
          closeDropdown();
        }
      });

      button.addEventListener('keydown',  (e) => {
        if (e.code === 'Enter' || e.code === 'Space') {
          e.preventDefault();
          if (!getIsDropdownOpen()) {
            closeAllDropdowns();
            openDropdown();
          } else {
            closeDropdown();
          }
        } else if (e.code === 'Escape') {
          e.preventDefault();
          closeAllDropdowns();
          button.focus();
        } else if (e.code === 'ArrowDown') {
          e.preventDefault();
          if (!getIsDropdownOpen()) {
            closeAllDropdowns();
            openDropdown();

            const firstMenuItem = menu.querySelector(MENU_LINK_SELECTOR);
            if (firstMenuItem) {
              firstMenuItem.focus();
            }
          }
        }
      });

      menu.addEventListener('keydown',  (e) => {
        const menuItems = menu.querySelectorAll(MENU_LINK_SELECTOR);
        const currentIndex = Array.from(menuItems).findIndex(item => item === document.activeElement);

        if (e.code === 'ArrowDown') {
          e.preventDefault();
          if (currentIndex >= 0) {
            const nextIndex = (currentIndex + 1) % menuItems.length;
            menuItems[nextIndex].focus();
          } else if (menuItems.length > 0) {
            menuItems[0].focus();
          }
        } else if (e.code === 'ArrowUp') {
          e.preventDefault();
          if (currentIndex >= 0) {
            const prevIndex = (currentIndex - 1 + menuItems.length) % menuItems.length;
            menuItems[prevIndex].focus();
          } else if (menuItems.length > 0) {
            menuItems[menuItems.length - 1].focus();
          }
        } else if (e.code === 'Home') {
          e.preventDefault();
          menuItems[0]?.focus();
        } else if (e.code === 'End') {
          e.preventDefault();
          menuItems[menuItems.length - 1]?.focus();
        } else if (e.code === 'Escape') {
          e.preventDefault();
          closeAllDropdowns();
          button.focus();
        }
      });
    });
  });
</script>

                
            </div>
            <div class="headerheight align-vertical">
                <a id="toggle-header-mobile-dropdown" class="header-link focus-link-v2" tabindex="0" aria-label="Open menu">
                    <i class="fa fa-bars" aria-hidden="true"></i>
                    <i class="fa fa-times" aria-hidden="true"></i>
                </a>
            </div>
        

        

        <div class="headerheight align-vertical hide-mobile">

            
                <!-- Start Profile / Sign In -->
                
                    <a class="header-link login-link sj-text-sign-in focus-link-v2" href="https://anthropic.skilljar.com/auth/login?next=%2Fai-fluency-framework-foundations" rel="nofollow">
                        <span>Sign In</span>
                    </a>
                
                <!-- End Profile / Sign In -->
            

        </div>
    </div>

    <!-- Start Header Dropdowns -->
    
    <!-- End Header Dropdowns -->
</header>


    <div id="header-mobile-dropdown">
        
            


  
    <a class="header-link focus-link-v2" href="https://www.anthropic.com/learn" target="_self">
      Anthropic Academy
    </a>
  

  
    <a class="header-link focus-link-v2" href="https://anthropic.skilljar.com/" target="_self">
      Courses
    </a>
  


        

        
            <a class="header-link login-link sj-text-sign-in focus-link-v2" href="https://anthropic.skilljar.com/auth/login?next=%2Fai-fluency-framework-foundations" rel="nofollow">
                <span>Sign In</span>
            </a>
        
    </div>



<script>
  var activeDropdownIndex = -1;

  document.addEventListener('DOMContentLoaded', function () {
    var $dropdownButton = document.querySelector('.header-dropdown-button');
    if (document.querySelector('.header-dropdown-button')) {
      $dropdownButton.addEventListener('click', handleDropdownToggle);
      $dropdownButton.addEventListener('blur', handleDropdownBlur);
      $dropdownButton.addEventListener('focus', handleDropdownFocus);
      var $dropdownLinks = document.querySelectorAll('#header-drop a');
      for (var dropdownLink of $dropdownLinks) {
        dropdownLink.addEventListener('keydown', handleDropdownLinkBlur);
      }
    }

    var $toggleMobileMenu = document.getElementById('toggle-header-mobile-dropdown');
    if ($toggleMobileMenu) {
      $toggleMobileMenu.addEventListener('click', handleMobileMenuToggle);

    }

    $($toggleMobileMenu).keydown(function (e) {
        const key = e.key
        if (key === 'Enter') {
             handleMobileMenuToggle(key)
        }
    });
  });

  function handleDropdownLinkBlur(e) {
    setTimeout(function () {
      var $openDropdown = document.querySelector('#header-drop.open');
      if (!$openDropdown) {
        return;
      }

      if (!$(':focus').closest('#header-drop').length) {
        var $dropdownButton = document.querySelector('.header-dropdown-button');
        $dropdownButton.click();
      }
    }, 100);
  }

  function handleDropdownFocus(e) {
    setTimeout(function () {
      var $openDropdown = document.querySelector('#header-drop.open');
      if ($openDropdown) {
        return;
      }

      var $dropdownButton = document.querySelector('.header-dropdown-button');
      $dropdownButton.click();
    }, 100);
  }

  function handleDropdownToggle() {
    setTimeout(function () {
      activeDropdownIndex = -1;
      var $dropdownAnchors = document.querySelectorAll('#header-drop a');
      var $headerDropDown = document.querySelector('.header-dropdown-button')
      for (var anchor of $dropdownAnchors) {
        var $openDropdown = document.querySelector('#header-drop.open');
        if ($openDropdown) {
          document.addEventListener("keydown", handleDropdownUpDownNav);
          $headerDropDown ? $headerDropDown.ariaExpanded = true : null
        } else {
          $headerDropDown ? $headerDropDown.ariaExpanded = false : null
          document.removeEventListener("keydown", handleDropdownUpDownNav);

        }
      }
    }, 0);
  }

  function handleDropdownBlur() {
    setTimeout(function () {
      if (!$(':focus').closest('#header-drop').length) {
        var $openDropdown = document.querySelector('#header-drop.open');
        if ($openDropdown) {
          var $dropdownButton = document.querySelector('.header-dropdown-button');
          $dropdownButton.click();
        }
      }
    }, 0);

    setTimeout(function () {
      var $openDropdown = document.querySelector('#header-drop.open');
      if (!$openDropdown) {
        handleDropdownToggle();
      }
    }, 0);
  }

  function handleDropdownUpDownNav(e) {
    if (e.keyCode !== 38 && e.keyCode !== 40) {
      return;
    }

    e.preventDefault();
    var $dropdownAnchors = document.querySelectorAll('#header-drop a');

    if (e.keyCode === 40) {
      if (activeDropdownIndex < $dropdownAnchors.length - 1) {
        activeDropdownIndex = activeDropdownIndex + 1;
      }
    } else {
      if (activeDropdownIndex > 0) {
        activeDropdownIndex = activeDropdownIndex - 1;
      }
    }

    if (activeDropdownIndex === -1) {
      activeDropdownIndex = 0;
    }

    $dropdownAnchors[activeDropdownIndex].focus();

  }

  function handleMobileMenuToggle(key=null) {
    var $mainContainer = document.getElementById('main-container');
    var $mobileDropdown = document.getElementById('header-mobile-dropdown');
    var $toggleMobileMenu = document.getElementById('toggle-header-mobile-dropdown');

    if ($mainContainer.classList.toggle('show-header-mobile-dropdown') && key != 'Escape') {
      document.addEventListener('click', handleMobileMenuClose);
       $mobileDropdown.addEventListener('click', handleDropdownToggle);
       $toggleMobileMenu.setAttribute('aria-label',"Close menu")
    } else {
      document.removeEventListener('click', handleMobileMenuClose);
       $mobileDropdown.removeEventListener('click', handleDropdownToggle);
       $toggleMobileMenu.setAttribute('aria-label',"Open menu")
    }

    var $header = document.getElementById('header');
    var $leftNavButton = document.getElementById('left-nav-button');


    function handleMobileMenuClose(e) {
      // Close menu unless left-nav-button or a child of header or header-mobile-dropdown
      if (e.target === $leftNavButton || !$header.contains(e.target) &&
          !$mobileDropdown.contains(e.target)) {
        $mainContainer.classList.remove('show-header-mobile-dropdown');
        document.removeEventListener('click', handleMobileMenuClose);
      }
    }
  }
</script>



    <div id="skilljar-content" class="reveal"><div class="clp__container">
      
    <section class="clp__hero">
      <div class="clp__hero-content">
        <div class="clp__hero-text">
          <div class="clp__breadcrumb">
            <a href="https://www.anthropic.com/learn" class="clp__breadcrumb-link">Anthropic Academy</a>
            <span class="clp__breadcrumb-separator"> / </span>
            <a href="https://anthropic.skilljar.com/" class="clp__breadcrumb-link">Courses</a>
          </div>
          <h1 class="clp__title">AI Fluency: Framework &amp; Foundations</h1>
          <p class="clp__subtitle">Learn to collaborate with AI systems effectively, efficiently, ethically, and safely</p>
          
          <div class="clp__hero-actions">
            <a href="https://anthropic.skilljar.com/checkout/17mii0ye0vhrr" class="clp__enroll-btn">Enroll in Course</a>
            <span class="clp__free-tag">FREE</span>
          </div>
          
          <div class="clp__signin-text">
            Already registered? <a href="https://anthropic.skilljar.com/auth/login?next=%2Fai-fluency-framework-foundations" class="clp__signin-link">Sign In</a>
          </div>
          
          <div class="clp__share-buttons-inline">
            <a href="https://anthropic.skilljar.com/ai-fluency-framework-foundations#" class="clp__share-btn-inline" onclick="shareOnX(event)">
              <svg class="clp__share-icon-inline" viewBox="0 0 24 24" fill="currentColor">
                <path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"></path>
              </svg>
              <span>Share on X</span>
            </a>
            <a href="https://anthropic.skilljar.com/ai-fluency-framework-foundations#" class="clp__share-btn-inline" onclick="shareOnLinkedIn(event)">
              <svg class="clp__share-icon-inline" viewBox="0 0 24 24" fill="currentColor">
                <path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"></path>
              </svg>
              <span>Share on LinkedIn</span>
            </a>
          </div>
        </div>
        
        <div class="clp__hero-right">
          <div class="clp__video-wrapper"><div class="columns large-6 text-center dp-promo-image-wrapper">
                <div class="dp-promo-image">
                    
                        






<script>
    var videoJsArgs = {
        videoAspectRatio: 1.76,
        externalControlsHeight: 0,
        fullHeightSmall: false,
        disableVideoFastForward: false,
        
        highwaterSecond: 0,
        currentSecond: 0,
        playerCompleteCallback: undefined,
        autoPlay: false,
        
        
    };
</script>

<script src="./AI Fluency_ Framework &amp; Foundations_files/video.lesson.e9a303b78c0a.js"></script>
<script src="./AI Fluency_ Framework &amp; Foundations_files/base.videoplayer.7e778db6ef92.js"></script>


    <div class="video-max" style="max-width: 568px;">
        <div class="flex-video widescreen" style="margin-top: 0px;">
            

<iframe id="iframe-youtube-player" data-video-id="-UN9sNqQ0t4" width="640" height="360" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="AI Fluency: Framework &amp; Foundations Course Trailer" src="./AI Fluency_ Framework &amp; Foundations_files/-UN9sNqQ0t4.html" data-gtm-yt-inspected-8="true"></iframe>

<script src="./AI Fluency_ Framework &amp; Foundations_files/youtube.videoplayer.4f00efffaa38.js">
</script>

        </div>
    </div>





                    
                </div>
            </div></div>
          
    <div class="clp__stats-container">
      <div class="clp__stat-item">
        <span class="clp__stat-value">14</span>
        <span class="clp__stat-label">lectures</span>
      </div>
      <div class="clp__stat-item">
        <span class="clp__stat-value">1.1</span>
        <span class="clp__stat-label">hours of video</span>
      </div>
      
      <div class="clp__stat-item">
        <span class="clp__stat-value">1</span>
        <span class="clp__stat-label">quiz</span>
      </div>
      
      <div class="clp__stat-item">
        <span class="clp__stat-value">✓</span>
        <span class="clp__stat-label">Certificate of completion</span>
      </div>
    </div>
  
        </div>
      </div>
    </section>
  
      
    <div class="clp__main-content">
    
    <div>
      <div class="clp__course-details">
        <h2 class="clp__details-title">About this course</h2>
        
        <p class="clp__details-text"></p><p>At Anthropic, we believe that empowering people with AI, and ensuring that AI makes safe contributions to society, requires engaging with a wide range of human perspectives and experiences. Responsible AI development and engagement isn't something any single discipline or viewpoint can fully address. It demands collaborative approaches that span a wide range of technical, creative, business, scientific, and educational domains. That's why we partnered with educators who bring complementary expertise to create this foundational course on AI collaboration.</p><p>This course is the result of a long partnership between Anthropic and professors Rick Dakan from Ringling College of Art and Design and Joseph Feller from University College Cork. Rick and Joe developed the AI Fluency Framework in 2023-2024, based on their research exploring how AI tools like Claude were transforming creative and business processes. When we saw the framework, we immediately recognized a shared vision: helping people interact with AI effectively and responsibly, beyond just “cool prompts.” Their framework offered exactly the kind of multidisciplinary perspective we believe is essential for navigating AI’s impact on society. </p><p>The AI Fluency Framework they created — four interconnected competencies (Delegation, Description, Discernment, and Diligence) — enables more effective, efficient, ethical, and safe human-AI collaboration, regardless of which new AI models or tools emerge. We collaborated to develop this course based on their framework, bringing together our collective expertise in AI systems, education, creativity, and business innovation. The work was supported in part by the Higher Education Authority (Ireland) through the National Forum for the Enhancement of Teaching and Learning. </p><p>This framework has already informed undergraduate and postgraduate courses at both Ringling College and University College Cork, as well as staff training initiatives and community outreach events. Now, we’re excited to share these insights more broadly through this open online course.</p><p>Our goal is to make AI Fluency accessible and useful to everyone, no matter what stage of AI expertise you find yourself at. We hope you find it valuable in navigating the evolving landscape of AI collaboration.</p><p></p>
        
        
        
        
        
        
      </div>
      
      
    </div>
  
    
    <section class="clp__sections-container">
      <h2 class="clp__section-title">Course sections</h2>
      
      
        <div class="clp__section-block">
          <div class="clp__section-header">
            <div class="clp__section-info">
              <h3 class="clp__section-name">AI Fundamentals &amp; Framework</h3>
              <span class="clp__lesson-count">10 lessons</span>
            </div>
          </div>
          
          <p class="clp__section-description">Establish foundational understanding of generative AI systems and why developing AI fluency matters for effective collaboration. Introduces the 4D Framework as a structured approach to human-AI interaction, covering core capabilities and limitations of current AI technologies. Provides the conceptual grounding needed to approach AI tools strategically rather than reactively.</p>
          
          
            <span class="clp__preview-label">Course preview images</span>
            <div class="clp__image-placeholders">
              
      <div class="clp__screenshot-wrapper">
        <img src="./AI Fluency_ Framework &amp; Foundations_files/01-01-medium.webp" alt="Three ways to interact with ai" class="clp__screenshot-image clp__screenshot-clickable" loading="lazy" data-section-id="section-1" data-screenshot-id="s1-1" data-full-size-url="https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-framework-foundations/01-01.webp">
      </div>
    
      <div class="clp__screenshot-wrapper">
        <img src="./AI Fluency_ Framework &amp; Foundations_files/01-02-medium.webp" alt="key takeaways" class="clp__screenshot-image clp__screenshot-clickable" loading="lazy" data-section-id="section-1" data-screenshot-id="s1-2" data-full-size-url="https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-framework-foundations/01-02.webp">
      </div>
    
      <div class="clp__screenshot-wrapper">
        <img src="./AI Fluency_ Framework &amp; Foundations_files/01-03-medium.webp" alt="three pillars that make ai possible" class="clp__screenshot-image clp__screenshot-clickable" loading="lazy" data-section-id="section-1" data-screenshot-id="s1-3" data-full-size-url="https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-framework-foundations/01-03.webp">
      </div>
    
            </div>
          
        </div>
      
        <div class="clp__section-block">
          <div class="clp__section-header">
            <div class="clp__section-info">
              <h3 class="clp__section-name">Practical AI Skills</h3>
              <span class="clp__lesson-count">10 lessons</span>
            </div>
          </div>
          
          <p class="clp__section-description">Develop hands-on competencies for effective AI collaboration through the four core areas of the framework: delegation, description, discernment, and diligence. Learn systematic approaches to project planning with AI, crafting effective prompts, evaluating outputs critically, and iterating through the description-discernment loop. Emphasizes practical application across creative, business, and educational contexts.</p>
          
          
            <span class="clp__preview-label">Course preview images</span>
            <div class="clp__image-placeholders">
              
      <div class="clp__screenshot-wrapper">
        <img src="./AI Fluency_ Framework &amp; Foundations_files/02-01-medium.webp" alt="product descriptions" class="clp__screenshot-image clp__screenshot-clickable" loading="lazy" data-section-id="section-2" data-screenshot-id="s1-1" data-full-size-url="https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-framework-foundations/02-01.webp">
      </div>
    
      <div class="clp__screenshot-wrapper">
        <img src="./AI Fluency_ Framework &amp; Foundations_files/02-02-medium.webp" alt="foundational prompting tips" class="clp__screenshot-image clp__screenshot-clickable" loading="lazy" data-section-id="section-2" data-screenshot-id="s1-2" data-full-size-url="https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-framework-foundations/02-02.webp">
      </div>
    
      <div class="clp__screenshot-wrapper">
        <img src="./AI Fluency_ Framework &amp; Foundations_files/02-03-medium.webp" alt="creation diligence" class="clp__screenshot-image clp__screenshot-clickable" loading="lazy" data-section-id="section-2" data-screenshot-id="s1-3" data-full-size-url="https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-framework-foundations/02-03.webp">
      </div>
    
            </div>
          
        </div>
      
    </section>
  
    
    <section class="clp__instructors-container">
      <h2 class="clp__instructors-title">About the instructors</h2>
      
      <div class="clp__instructors-card">
        <div class="clp__instructors-grid">
          
            <div class="clp__instructor-item">
              <img src="./AI Fluency_ Framework &amp; Foundations_files/drew_bent_o-medium.webp" alt="Drew Bent" class="clp__instructor-avatar">
              <div class="clp__instructor-info">
                <h3 class="clp__instructor-name">Drew Bent</h3>
                <p class="clp__instructor-description">Drew leads education research at Anthropic. He previously co-founded the tutoring non-profit Schoolhouse.world with Sal Khan, which he ran from 2020-24 and now sits on the board. Prior to that, he wrote code at Khan Academy, taught high school math, and has been tutoring students for over a decade. Drew has degrees in physics &amp; CS from MIT, and an education master's from Stanford.</p>
              </div>
            </div>
          
            <div class="clp__instructor-item">
              <img src="./AI Fluency_ Framework &amp; Foundations_files/rick_dakan_o-medium.webp" alt="Rick Dakan" class="clp__instructor-avatar">
              <div class="clp__instructor-info">
                <h3 class="clp__instructor-name">Rick Dakan</h3>
                <p class="clp__instructor-description">Rick is the AI Coordinator and a professor at Ringling College of Art and Design in Sarasota, Florida where he teaches creative writing, interactive experience design, and AI courses. He also oversees the college's <i>Undergraduate Certificate in Artificial Intelligence</i> and the <i>Professional Certificate in Fundamentals of AI for Creatives</i>. He is a game designer and author of more than thirty games and books from video games and tabletop games to novels, nonfiction, and comics.</p>
              </div>
            </div>
          
            <div class="clp__instructor-item">
              <img src="./AI Fluency_ Framework &amp; Foundations_files/joseph_feller_o-medium.webp" alt="Joseph Feller" class="clp__instructor-avatar">
              <div class="clp__instructor-info">
                <h3 class="clp__instructor-name">Joseph Feller</h3>
                <p class="clp__instructor-description">Joseph is <i>Professor of Information Systems and Digital Transformation</i> at the Cork University Business School, University College Cork, Ireland. His current work focuses on AI-human hybrid creativity, innovation, and learning. His research has been published in <i>Information Systems Research</i>, <i>Journal of MIS</i>, <i>Journal of the AIS</i>, <i>Journal of Information Technology</i>, <i>Information Systems Journal</i>, <i>European Journal of Information Systems</i>, and <i>Journal of Strategic Information Systems</i>, and has been funded by the European Commission, Irish Research Council, Irish HEA, and other funding bodies.</p><p></p>
              </div>
            </div>
          
            <div class="clp__instructor-item">
              <img src="./AI Fluency_ Framework &amp; Foundations_files/maggie_vo_o-medium.webp" alt="Maggie Vo" class="clp__instructor-avatar">
              <div class="clp__instructor-info">
                <h3 class="clp__instructor-name">Maggie Vo</h3>
                <p class="clp__instructor-description">Maggie founded and leads Anthropic's education team. She has a varied applied research background from Harvard University, with a career and education that spans fields such as game design, organizational behavior, tech and consumer goods, and human behavioral psychology. Maggie has held previous roles at top consulting firms and innovative startups alike. Prior to Anthropic, she worked in AI strategy consulting.</p><p></p>
              </div>
            </div>
          
        </div>
      </div>
    </section>
  
    
    <section class="clp__extra-container">
      <h2 class="clp__extra-title">AI diligence statement</h2>
      
      <div class="clp__extra-card">
        <div class="clp__extra-content"><p>In the development of the AI Fluency: Framework and Foundations course, we engaged in extensive collaboration with Claude 3.7 from Anthropic.</p><p>The base content for this course came from:</p><ul><li>The AI Fluency Framework Practical Summary Document by Rick Dakan (at Ringling College of Art and Design) and Joseph Feller (at University College Cork) and related working documents and research notes</li><li>Slide decks and lecture transcripts from multiple university courses, guest lectures, and research talks delivered by Feller and/or Dakan</li><li>Technical/practical content provided by Maggie Vo and Drew Bent (Anthropic)</li></ul><p>Throughout this process, Claude assisted one or more of the human authors with structural development, resource and exercise design, and content drafting, critiquing, editing and rewriting. The human authors wrote, designed, edited and provided continual vision, expertise, critical judgment, and domain knowledge and made all final decisions about both content and approach.</p><p>All AI-generated and co-created content underwent thorough validation, editing, and curation by the human authors. The final materials accurately reflect the human authors' understanding, expertise, and intended pedagogical approach. While AI assistance was instrumental in producing these materials, the human authors maintain responsibility for the content.</p><p>This disclosure is made in the spirit of transparency advocated by the AI Fluency Framework and to acknowledge the evolving role of AI in educational content development and other creative and intellectual work.</p></div>
      </div>
    </section>
  
    </div>
  
    </div></div>
    








    <div id="sj-custom-footer" role="contentinfo">
        <div class="b-faq-list">
        <div class="page-wrapper">
            <h2 class="h3">Data and Privacy</h2>

            <div class="block-content">
                <div class="faqs-container">
                    
                    <div id="faq-container-1" class="faq-container">
                        <input id="faq-1" class="faq-input hidden-field" name="open-faqs" type="checkbox"><span class="custom checkbox"></span>
                        
                        <label for="faq-1" class="faq-label s:grid s:grid-12">
                            <div class="faq-title h4 s:col-start-1 s:col-span-10">
                                What is Skilljar and why am I logging into it?
                            </div>
                            <span class="faq-icon s:col-start-11 s:col-span-2">
                                <svg class="icon-more" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <line x1="12" y1="5" x2="12" y2="19"></line>
                                    <line x1="5" y1="12" x2="19" y2="12"></line>
                                </svg>
                                <svg class="icon-less" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <line x1="5" y1="12" x2="19" y2="12"></line>
                                </svg>
                            </span>
                        </label>

                        <article id="faq-content-1" class="faq-content s:grid s:grid-12">
                            <div class="faq-post s:col-start-1 s:col-span-10 text-b2">
                                <p>Skilljar is a learning management system that hosts our educational content. You're logging into it to access the Anthropic course materials. This separate platform allows us to provide interactive learning experiences, track your progress, and ensure you have access to all course resources in an organized way.</p>
                            </div>
                        </article>
                    </div>

                    <div id="faq-container-2" class="faq-container">
                        <input id="faq-2" class="faq-input hidden-field" name="open-faqs" type="checkbox"><span class="custom checkbox"></span>
                        
                        <label for="faq-2" class="faq-label s:grid s:grid-12">
                            <div class="faq-title h4 s:col-start-1 s:col-span-10">
                                What information does Skilljar collect about my learning activity?
                            </div>
                            <span class="faq-icon s:col-start-11 s:col-span-2">
                                <svg class="icon-more" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <line x1="12" y1="5" x2="12" y2="19"></line>
                                    <line x1="5" y1="12" x2="19" y2="12"></line>
                                </svg>
                                <svg class="icon-less" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <line x1="5" y1="12" x2="19" y2="12"></line>
                                </svg>
                            </span>
                        </label>

                        <article id="faq-content-2" class="faq-content s:grid s:grid-12">
                            <div class="faq-post s:col-start-1 s:col-span-10 text-b2">
                                <p>Skilljar collects basic learning analytics such as course progress, lesson completion status, quiz scores, and time spent on materials. This data helps us understand how you're progressing through the course and allows us to provide you with completion certificates. All data collection is focused on improving your learning experience.</p>
                            </div>
                        </article>
                    </div>

                    <div id="faq-container-3" class="faq-container">
                        <input id="faq-3" class="faq-input hidden-field" name="open-faqs" type="checkbox"><span class="custom checkbox"></span>
                        
                        <label for="faq-3" class="faq-label s:grid s:grid-12">
                            <div class="faq-title h4 s:col-start-1 s:col-span-10">
                                How is my Skilljar data different from my Anthropic account data?
                            </div>
                            <span class="faq-icon s:col-start-11 s:col-span-2">
                                <svg class="icon-more" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <line x1="12" y1="5" x2="12" y2="19"></line>
                                    <line x1="5" y1="12" x2="19" y2="12"></line>
                                </svg>
                                <svg class="icon-less" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <line x1="5" y1="12" x2="19" y2="12"></line>
                                </svg>
                            </span>
                        </label>

                        <article id="faq-content-3" class="faq-content s:grid s:grid-12">
                            <div class="faq-post s:col-start-1 s:col-span-10 text-b2">
                                <p>Skilljar only tracks your learning progress within this course platform, while your Anthropic account manages your access to the Anthropic Console and/or Claude AI services.</p>
                            </div>
                        </article>
                    </div>

                    <div id="faq-container-4" class="faq-container">
                        <input id="faq-4" class="faq-input hidden-field" name="open-faqs" type="checkbox"><span class="custom checkbox"></span>
                        
                        <label for="faq-4" class="faq-label s:grid s:grid-12">
                            <div class="faq-title h4 s:col-start-1 s:col-span-10">
                                Is Skilljar secure? Where is my data stored?
                            </div>
                            <span class="faq-icon s:col-start-11 s:col-span-2">
                                <svg class="icon-more" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <line x1="12" y1="5" x2="12" y2="19"></line>
                                    <line x1="5" y1="12" x2="19" y2="12"></line>
                                </svg>
                                <svg class="icon-less" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <line x1="5" y1="12" x2="19" y2="12"></line>
                                </svg>
                            </span>
                        </label>

                        <article id="faq-content-4" class="faq-content s:grid s:grid-12">
                            <div class="faq-post s:col-start-1 s:col-span-10 text-b2">
                                <p>Yes, Skilljar employs industry-standard security measures including data encryption, secure hosting, and regular security audits. Your learning data is stored on secure servers with appropriate access controls. Skilljar is SOC 2 compliant and follows best practices for data protection to ensure your information remains safe and private.</p>
                            </div>
                        </article>
                    </div>

                    <div id="faq-container-5" class="faq-container">
                        <input id="faq-5" class="faq-input hidden-field" name="open-faqs" type="checkbox"><span class="custom checkbox"></span>
                        
                        <label for="faq-5" class="faq-label s:grid s:grid-12">
                            <div class="faq-title h4 s:col-start-1 s:col-span-10">
                                How do I delete my learning activity or Skilljar account?
                            </div>
                            <span class="faq-icon s:col-start-11 s:col-span-2">
                                <svg class="icon-more" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <line x1="12" y1="5" x2="12" y2="19"></line>
                                    <line x1="5" y1="12" x2="19" y2="12"></line>
                                </svg>
                                <svg class="icon-less" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <line x1="5" y1="12" x2="19" y2="12"></line>
                                </svg>
                            </span>
                        </label>

                        <article id="faq-content-5" class="faq-content s:grid s:grid-12">
                            <div class="faq-post s:col-start-1 s:col-span-10 text-b2">
                                <p>You can request deletion of your learning activity by emailing <a href="mailto:education-support@anthropic.com">education-support@anthropic.com</a> or deletion of your entire Skilljar account by emailing <a href="mailto:support@skilljar.com">support@skilljar.com</a>. Your request will be processed in accordance with applicable privacy laws and our data retention policies. Note that some data may need to be retained for legitimate business purposes, such as compliance or security, but we'll delete all personal information where legally permissible.</p>
                            </div>
                        </article>
                    </div>

                    <div id="faq-container-6" class="faq-container">
                        <input id="faq-6" class="faq-input hidden-field" name="open-faqs" type="checkbox"><span class="custom checkbox"></span>
                        
                        <label for="faq-6" class="faq-label s:grid s:grid-12">
                            <div class="faq-title h4 s:col-start-1 s:col-span-10">
                                Do I need an Anthropic account to access the learning content?
                            </div>
                            <span class="faq-icon s:col-start-11 s:col-span-2">
                                <svg class="icon-more" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <line x1="12" y1="5" x2="12" y2="19"></line>
                                    <line x1="5" y1="12" x2="19" y2="12"></line>
                                </svg>
                                <svg class="icon-less" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <line x1="5" y1="12" x2="19" y2="12"></line>
                                </svg>
                            </span>
                        </label>

                        <article id="faq-content-6" class="faq-content s:grid s:grid-12">
                            <div class="faq-post s:col-start-1 s:col-span-10 text-b2">
                                <p>No, you don't need an Anthropic account to access this learning content. The course is hosted on Skilljar and only requires a Skilljar account for access. However, if you want to use Claude AI services after completing the course, you would need to create a separate Anthropic account at claude.ai.</p>
                            </div>
                        </article>
                    </div>

                </div>
            </div>
        </div>
    </div>


    <!-- Footer HTML matching the actual website structure -->
    <div class="footer_contain u-container-full">
      <nav class="footer_grid">
        <div class="footer_grid_logo">
          <svg width="48" viewBox="0 0 46 32" fill="none">
            <path d="M32.73 0H25.7846L38.4499 32H45.3953L32.73 0Z" fill="currentColor"></path>
            <path d="M12.6653 0L0 32H7.08167L9.67193 25.28H22.9219L25.5122 32H32.5939L19.9286 0H12.6653ZM11.9626 19.3371L16.2969 8.09143L20.6313 19.3371H11.9626Z" fill="currentColor"></path>
          </svg>
        </div>
      </nav>
      <div class="footer_bottom_wrap">
        <div class="footer_bottom_contain">
          <div class="footer_bottom_text">© <span data-year="true">2025</span> Anthropic PBC</div>
          <ul role="list" class="footer_bottom_list">
            <li class="footer_bottom_item">
              <a href="https://www.youtube.com/@anthropic-ai" target="_blank" class="footer_bottom_link_wrap w-inline-block">
                <div class="u-icon-32">
                  <svg xmlns="http://www.w3.org/2000/svg" width="100%" viewBox="0 0 32 32" fill="none">
                    <path d="M29.2184 9.4375C28.9596 8.06299 27.7263 7.06201 26.2951 6.74951C24.1533 6.3125 20.1896 6 15.901 6C11.615 6 7.58782 6.3125 5.44354 6.74951C4.01486 7.06201 2.77905 7.99951 2.52021 9.4375C2.25884 11 2 13.1875 2 16C2 18.8125 2.25884 21 2.58365 22.5625C2.84502 23.937 4.0783 24.938 5.50698 25.2505C7.78068 25.6875 11.6784 26 15.967 26C20.2556 26 24.1533 25.6875 26.427 25.2505C27.8557 24.938 29.089 24.0005 29.3504 22.5625C29.6092 21 29.934 18.749 30 16C29.868 13.1875 29.5432 11 29.2184 9.4375ZM12.3941 20.375V11.625L20.319 16L12.3941 20.375Z" fill="currentColor"></path>
                  </svg>
                </div>
              </a>
            </li>
            <li class="footer_bottom_item">
              <a aria-label="Visit our LinkedIn page" rel="noopener noreferrer" href="https://www.linkedin.com/company/anthropicresearch" target="_blank" class="footer_bottom_link_wrap w-inline-block">
                <div class="u-icon-32">
                  <svg xmlns="http://www.w3.org/2000/svg" width="100%" viewBox="0 0 32 32" fill="none">
                    <path d="M25.8182 4H6.18182C4.97636 4 4 4.97636 4 6.18182V25.8182C4 27.0236 4.97636 28 6.18182 28H25.8182C27.0236 28 28 27.0236 28 25.8182V6.18182C28 4.97636 27.0236 4 25.8182 4ZM11.5862 23.6364H8.368V13.2815H11.5862V23.6364ZM9.94436 11.8011C8.90691 11.8011 8.068 10.96 8.068 9.92473C8.068 8.88945 8.908 8.04945 9.94436 8.04945C10.9785 8.04945 11.8196 8.89055 11.8196 9.92473C11.8196 10.96 10.9785 11.8011 9.94436 11.8011ZM23.6407 23.6364H20.4247V18.6007C20.4247 17.3996 20.4029 15.8549 18.7524 15.8549C17.0778 15.8549 16.8204 17.1629 16.8204 18.5135V23.6364H13.6044V13.2815H16.6916V14.6964H16.7353C17.1651 13.8825 18.2145 13.024 19.78 13.024C23.0385 13.024 23.6407 15.1687 23.6407 17.9571V23.6364Z" fill="currentColor"></path>
                  </svg>
                </div>
              </a>
            </li>
            <li class="footer_bottom_item">
              <a aria-label="Visit our X (formerly Twitter) profile" rel="noopener noreferrer" href="https://x.com/AnthropicAI" target="_blank" class="footer_bottom_link_wrap w-inline-block">
                <div class="u-icon-32">
                  <svg xmlns="http://www.w3.org/2000/svg" width="100%" viewBox="0 0 32 32" fill="none">
                    <path d="M28 28L18.6145 14.0124L18.6305 14.0255L27.0929 4H24.265L17.3713 12.16L11.8968 4H4.48021L13.2425 17.0593L13.2414 17.0582L4 28H6.82792L14.4921 18.9215L20.5834 28H28ZM10.7763 6.18182L23.9449 25.8182H21.7039L8.52468 6.18182H10.7763Z" fill="currentColor"></path>
                  </svg>
                </div>
              </a>
            </li>
          </ul>
        </div>
      </div>
    </div>
    </div>

<footer id="ep-footer">
    
        <div id="footer-left" class="left">
            <ul>
                <li>© 2025</li>
                
                    



<script>
  document.addEventListener('DOMContentLoaded', () => {
    const DROPDOWN_SELECTOR = '.footer-item--dropdown';
    const MENU_SELECTOR = '.footer-item__menu';
    const MENU_LINK_SELECTOR = '.footer-item__menu-link';
    const BUTTON_OPEN_CLASS = 'footer-item--open';
    const dropdowns = document.querySelectorAll(DROPDOWN_SELECTOR);
    const closeAllDropdowns = () => {
      dropdowns.forEach(button => {
        button.classList.remove(BUTTON_OPEN_CLASS);
        button.setAttribute('aria-expanded', 'false');
      });
    };

    document.addEventListener('click', (e) => {
      if (![...dropdowns].some(dropdown => dropdown.contains(e.target))) {
        closeAllDropdowns();
      }
    });

    dropdowns.forEach(button => {
      const menu = button.querySelector(MENU_SELECTOR);
      const getIsDropdownOpen = () => button.classList.contains(BUTTON_OPEN_CLASS);
      const openDropdown = () => {
        button.classList.add(BUTTON_OPEN_CLASS);
        button.setAttribute('aria-expanded', 'true');
      };
      const closeDropdown = () => {
        button.classList.remove(BUTTON_OPEN_CLASS);
        button.setAttribute('aria-expanded', 'false');
      };

      button.addEventListener('mouseenter', () => {
        closeAllDropdowns();
        if (!getIsDropdownOpen()) {
          openDropdown();
        }
      });

      button.addEventListener('mouseleave', () => {
        if (getIsDropdownOpen()) {
          closeDropdown();
        }
      });

      button.addEventListener('keydown', (e) => {
        if (e.code === 'Enter' || e.code === 'Space') {
          e.preventDefault();
          if (!getIsDropdownOpen()) {
            closeAllDropdowns();
            openDropdown();
          } else {
            closeDropdown();
          }
        } else if (e.code === 'Escape') {
          e.preventDefault();
          closeAllDropdowns();
          button.focus();
        } else if (e.code === 'ArrowDown') {
          e.preventDefault();
          if (!getIsDropdownOpen()) {
            closeAllDropdowns();
            openDropdown();

            const firstMenuItem = menu.querySelector(MENU_LINK_SELECTOR);
            if (firstMenuItem) {
              firstMenuItem.focus();
            }
          }
        }
      });

      menu.addEventListener('keydown', (e) => {
        const menuItems = menu.querySelectorAll(MENU_LINK_SELECTOR);
        const currentIndex = Array.from(menuItems).findIndex(item => item === document.activeElement);

        if (e.code === 'ArrowDown') {
          e.preventDefault();
          if (currentIndex >= 0) {
            const nextIndex = (currentIndex + 1) % menuItems.length;
            menuItems[nextIndex].focus();
          } else if (menuItems.length > 0) {
            menuItems[0].focus();
          }
        } else if (e.code === 'ArrowUp') {
          e.preventDefault();
          if (currentIndex >= 0) {
            const prevIndex = (currentIndex - 1 + menuItems.length) % menuItems.length;
            menuItems[prevIndex].focus();
          } else if (menuItems.length > 0) {
            menuItems[menuItems.length - 1].focus();
          }
        } else if (e.code === 'Home') {
          e.preventDefault();
          menuItems[0]?.focus();
        } else if (e.code === 'End') {
          e.preventDefault();
          menuItems[menuItems.length - 1]?.focus();
        } else if (e.code === 'Escape') {
          e.preventDefault();
          closeAllDropdowns();
          button.focus();
        }
      });
    });
  });
</script>

                
            </ul>
        </div>

        <div class="right">

            

            
                <span id="powered-by">
                    <a href="http://www.skilljar.com/" target="_blank" class="focus-link-v2">
                        <span class="sj-text-powered-by"><span>powered by</span></span> <img title="Skilljar" src="./AI Fluency_ Framework &amp; Foundations_files/powered-by-logo.864fb1f2d98e.png" alt="Skilljar">
                    </a>
                </span>
            

        </div>

    
</footer>


<script>
    document.addEventListener('DOMContentLoaded', function () {
        const languagePackSelect = document.getElementById('languagePackSelect');
        
        if (languagePackSelect) {
            languagePackSelect.addEventListener('change', function () {
                languagePackSelect.disabled = true;
                const languageCode = languagePackSelect.value;
                const expirationDate = new Date();
                
                expirationDate.setFullYear(expirationDate.getFullYear() + 1);
                
                const secure = window.location.protocol === 'https:' ? '; Secure' : '';
                document.cookie = '=' + encodeURIComponent(languageCode) + 
                    '; expires=' + expirationDate.toUTCString() +
                    '; path=/' +
                    '; SameSite=Lax' +
                    secure;
                window.location.reload(true);
            });
        }
    });
</script>



    
        
            <script src="./AI Fluency_ Framework &amp; Foundations_files/plugins.min.ab527eb92395.js"></script>
            <script src="./AI Fluency_ Framework &amp; Foundations_files/scripts.min.b2481308ca5e.js"></script>
        

        
        
        

        
        
            <script>
                $(document).foundation(function () {
                    // Activates the modified custom selects on forms
                    // Just need to add the class awesome to the form to make it work
                    wrapSelects = function () {
                        $('form.awesome select:not(.is-wrapped)')
                                .addClass('is-wrapped')
                                .wrap('<div class="custom dropdown select" />')
                                .after('<span class="selector"></span>');
                    };

                    wrapSelects();
                });
            </script>
        

        
            
                <script>
                    $(document).ready(function () {
                        $('[data-track-click]').click(function (event) {
                            var ga_event_data = $(event.currentTarget).attr('data-track-click').split(',');
                            ga('skilljarTracker.send', 'event', ga_event_data[0], ga_event_data[1], ga_event_data[2], ga_event_data[3]);
                        });
                    });
                </script>
            

            
                <script>
                    $(document).ready(function () {
                        $('[data-track-click]').click(function (event) {
                            var ga_event_data = $(event.currentTarget).attr('data-track-click').split(',');

                            let event_name = ga_event_data[0];
                            let event_action = ga_event_data[1];
                            let event_label = ga_event_data[2];
                            let event_value = ga_event_data[3];

                            gtag('event', event_name, {
                                'send_to': 'SKILLJAR',
                                'action': event_action,
                                'label': event_label,
                                'value': event_value,
                            });
                        });
                    });
                </script>
            
        

        
            
        

    

    

    
    
    
        <script>
(function() {
window.__chatData = {
  "1p": "<notes>\n<critical>\nBelow are notes from a video course about working with the Claude language model.\nUse these notes as a resource to answer the user's question.\nWrite your answer as a standalone response - do not refer directly to these notes unless specifically requested by the user.\n</critical>\n\n<note title=\"Overview of Claude Models\">\nClaude has three model families optimized for different priorities:\n\nOpus = highest intelligence model for complex, multi-step tasks requiring deep reasoning and planning. Trade-off: higher cost and latency.\n\nSonnet = balanced model with good intelligence, speed, and cost efficiency. Strong coding abilities and precise code editing. Best for most practical use cases.\n\nHaiku = fastest model optimized for speed and cost efficiency. No reasoning capabilities like Opus/Sonnet. Best for real-time user interactions and high-volume processing.\n\nSelection framework: Intelligence priority → Opus. Speed priority → Haiku. Balanced requirements → Sonnet.\n\nCommon approach = use multiple models in same application based on specific task requirements rather than single model selection.\n\nAll models share core capabilities: text generation, coding, image analysis. Main difference is optimization focus.\n</note>\n\n<note title=\"Accessing the API\">\nAPI Access Flow = 5-step process from user input to response display\n\nStep 1: Client sends user text to developer's server (never access Anthropic API directly from client apps to keep API key secret)\n\nStep 2: Server makes request to Anthropic API using SDK (Python, TypeScript, JavaScript, Go, Ruby) or plain HTTP. Required parameters = API key + model name + messages list + max_tokens limit\n\nStep 3: Text generation process has 4 stages:\n- Tokenization = breaking input into tokens (words/word parts/symbols/spaces)\n- Embedding = converting tokens to number lists representing all possible word meanings\n- Contextualization = adjusting embeddings based on neighboring tokens to determine precise meaning\n- Generation = output layer produces probabilities for next word, model selects using probability + randomness, adds selected word, repeats process\n\nStep 4: Model stops when max_tokens reached or special end_of_sequence token generated\n\nStep 5: API returns response with generated text + usage counts + stop_reason to server, server sends to client for display\n\nToken = text chunk (word/part/symbol)\nEmbedding = numerical representation of word meanings\nContextualization = meaning refinement using neighboring words\nMax_tokens = generation length limit\nStop_reason = why model stopped generating\n</note>\n\n<note title=\"Making a Request\">\nMaking API Request to Anthropic = Process involving 4 setup steps and understanding message structure\n\nSetup Steps:\n1. Install packages = pip install anthropic python-dotenv in Jupyter notebook\n2. Store API key = Create .env file with ANTHROPIC_API_KEY=\"your_key\" (ignore in version control)\n3. Load environment variable = Use python-dotenv to securely load API key\n4. Create client = Initialize anthropic client and define model variable (claude-3-sonnet)\n\nAPI Request Structure:\n- Function = client.messages.create()\n- Required arguments = model, max_tokens, messages\n- Model = Name of Claude model to use\n- Max_tokens = Safety limit for generation length (not target length)\n- Messages = List containing conversation exchanges\n\nMessage Types:\n- User message = {\"role\": \"user\", \"content\": \"your text\"} (human-authored content)\n- Assistant message = Contains model-generated responses\n\nResponse Access:\n- Full response = Contains metadata and nested structure\n- Text only = message.content[0].text extracts just generated text\n\nExample request structure: client.messages.create(model=model, max_tokens=1000, messages=[{\"role\": \"user\", \"content\": \"What is quantum computing?\"}])\n</note>\n\n<note title=\"Multi-Turn Conversations\">\nMulti-Turn Conversations = conversations with multiple back-and-forth exchanges that maintain context.\n\nKey limitation: Anthropic API stores no messages. Each request is independent with no memory of previous exchanges.\n\nSolution requires two steps:\n1. Manually maintain message list in code\n2. Send entire conversation history with every follow-up request\n\nMessage structure = list of dictionaries with \"role\" (user/assistant) and \"content\" fields.\n\nConversation flow:\n- Send initial user message\n- Receive assistant response\n- Append assistant response to message history\n- Add new user message to history\n- Send complete history for context-aware follow-up\n\nHelper functions needed:\n- add_user_message(messages, text) = appends user message to history\n- add_assistant_message(messages, text) = appends assistant response to history  \n- chat(messages) = sends message history to API and returns response\n\nWithout message history = responses lack context and continuity. With complete history = Claude maintains conversation context and provides relevant follow-ups.\n</note>\n\n<note title=\"System Prompts\">\nSystem Prompts = technique to customize Claude's response style and tone by assigning it a specific role or behavior pattern.\n\nImplementation = pass system prompt as plain string to create function using system keyword argument.\n\nPurpose = control how Claude responds rather than what it responds. Example: math tutor role makes Claude give hints instead of direct answers.\n\nStructure = first line typically assigns role (\"You are a patient math tutor\"), followed by specific behavioral instructions.\n\nKey principle = system prompts guide response approach, not content. Same question gets different treatment based on assigned role.\n\nTechnical implementation = create params dictionary, conditionally add system key if prompt provided, pass params to create function with ** unpacking. Handle None case by excluding system parameter entirely.\n\nUse case example = Math tutor that gives guidance/hints rather than complete solutions, encouraging student thinking over direct answers.\n</note>\n\n<note title=\"Temperature\">\nTemperature = parameter (0-1) that controls randomness in Claude's text generation by influencing token selection probabilities.\n\nText generation process: Input text → tokenization → probability assignment to possible next tokens → token selection based on probabilities → repeat.\n\nTemperature effects:\n- Temperature 0 = deterministic output, always selects highest probability token\n- Higher temperature = increases chance of selecting lower probability tokens, more creative/unexpected outputs\n\nUsage guidelines:\n- Low temperature (near 0) = data extraction, factual tasks requiring consistency\n- High temperature (near 1) = creative tasks like brainstorming, writing, jokes, marketing\n\nImplementation: Add temperature parameter to model API calls. Higher values don't guarantee different outputs, just increase probability of variation.\n\nKey insight: Temperature directly manipulates the probability distribution of next token selection, making high-probability tokens more/less dominant in the selection process.\n</note>\n\n<note title=\"Response Streaming\">\nResponse Streaming = technique to display AI responses chunk-by-chunk as they're generated instead of waiting for complete response.\n\nProblem solved: AI responses can take 10-30 seconds. Users expect immediate feedback, not just spinners.\n\nHow it works:\n1. Server sends user message to Claude\n2. Claude immediately sends initial response (no text, just acknowledgment)\n3. Stream of events follows, each containing text chunks\n4. Server forwards chunks to frontend for real-time display\n\nEvent types:\n- message_start = initial acknowledgment\n- content_block_start = text generation begins\n- content_block_delta = contains actual text chunks (most important)\n- content_block_stop/message_stop = generation complete\n\nImplementation:\nBasic: client.messages.create(stream=True) returns event iterator\nSimplified: client.messages.stream() with text_stream property extracts just text\nFinal message: stream.get_final_message() assembles all chunks for storage\n\nKey benefits: Better UX through immediate response visibility, complete message capture for database storage.\n</note>\n\n<note title=\"Controlling Model Output\">\n**Controlling Model Output = Two key techniques beyond prompt modification**\n\n**Pre-filling Assistant Messages = Manually adding assistant message at end of conversation to steer response direction**\n\nHow it works:\n- Assemble messages list with user prompt + manual assistant message\n- Claude sees assistant message as already authored content\n- Claude continues response from exact end of pre-filled text\n- Response gets steered toward pre-filled direction\n\nKey point: Claude continues from exact endpoint of pre-fill, not complete sentences. Must stitch together pre-fill + generated response.\n\nExample: Pre-fill \"Coffee is better because\" → Claude continues with justification for coffee\n\n**Stop Sequences = Force Claude to halt generation when specific string appears**\n\nHow it works:\n- Provide stop sequence string in chat function\n- When Claude generates that exact string, response immediately stops\n- Generated stop sequence text not included in final output\n\nExample: Prompt \"count 1 to 10\" + stop sequence \"five\" → Output stops at \"four, \" (five not included)\n\nRefinement: Stop sequence \", five\" → Clean output \"one, two, three, four\"\n\nBoth techniques provide precise control over response direction and length without changing core prompts.\n</note>\n\n<note title=\"Structured Data\">\nStructured Data Generation = technique using assistant message prefilling + stop sequences to get raw output without Claude's natural explanatory headers/footers.\n\nProblem = Claude automatically adds markdown formatting, headers, commentary when generating JSON/code/structured content. Users often want just the raw data for copy/paste functionality.\n\nSolution Pattern:\n1. User message = request for structured data\n2. Assistant message prefill = opening delimiter (e.g., \"\\`\\`\\`json\")  \n3. Stop sequence = closing delimiter (e.g., \"\\`\\`\\`\")\n\nHow it works = Claude sees prefilled message, assumes it already started response, generates only the requested content, stops when hitting delimiter.\n\nResult = Raw structured data output with no extra formatting or commentary.\n\nApplication = Works for any structured data type (JSON, Python code, lists, etc.), not just JSON. Use whenever you need clean, parseable output without explanatory text.\n\nKey benefit = Output can be directly used/copied without manual selection or parsing of unwanted text.\n</note>\n\n<note title=\"Prompt Evaluation\">\nPrompt Engineering = techniques for writing/editing prompts to help Claude understand requests and desired responses.\n\nPrompt Evaluation = automated testing of prompts using objective metrics to measure effectiveness.\n\nThree paths after writing a prompt:\n1. Test once/twice, deploy to production (trap)\n2. Test with custom inputs, minor tweaks for corner cases (trap)  \n3. Run through evaluation pipeline for objective scoring (recommended)\n\nKey takeaway: Engineers commonly under-test prompts. Use evaluation pipelines to get objective performance scores before iterating and deploying prompts.\n</note>\n\n<note title=\"A Typical Eval Workflow\">\nTypical Eval Workflow = 6-step iterative process for prompt improvement\n\nStep 1: Write initial prompt draft - create baseline prompt to optimize\n\nStep 2: Create evaluation dataset - collection of test inputs (can be 3 examples or thousands, hand-written or LLM-generated)\n\nStep 3: Generate prompt variations - interpolate each dataset input into prompt template\n\nStep 4: Get LLM responses - feed each prompt variation to Claude, collect outputs\n\nStep 5: Grade responses - use grader system to score each response (e.g. 1-10 scale), average scores for overall prompt performance\n\nStep 6: Iterate - modify prompt based on scores, repeat entire process, compare versions\n\nKey points: No standard methodology exists. Many open-source/paid tools available. Can start simple with custom implementation. Grading complexity varies. Objective scoring enables systematic prompt improvement through A/B comparison.\n</note>\n\n<note title=\"Generating Test Datasets\">\nCustom prompt evaluation workflow = build prompt + generate test dataset + evaluate performance\n\nGoal = AWS code assistance prompt that outputs only Python, JSON config, or regex without explanations\n\nDataset generation approaches = manual assembly or automated with Claude (use faster models like Haiku for generation)\n\nDataset structure = array of JSON objects with task property describing user requests\n\nGeneration process = prompt Claude to create test cases → use pre-filling with assistant message \"\\`\\`\\`json\" → set stop sequence \"\\`\\`\\`\" → parse response as JSON → save to file\n\nKey implementation = generate_dataset() function that sends prompt to Claude, gets structured JSON response of test tasks, saves to dataset.json file for later evaluation use\n\nTest dataset enables systematic evaluation by running prompt against multiple input scenarios to measure performance consistency.\n</note>\n\n<note title=\"Running the Eval\">\nEval execution process = merging test cases with prompts, running through LLM, and grading outputs.\n\nTest case = individual record from dataset (JSON object).\n\nThree core functions:\n- run_prompt = merges test case with prompt, sends to Claude, returns output\n- run_test_case = calls run_prompt, grades result, returns summary dictionary \n- run_eval = loops through dataset, calls run_test_case for each, assembles results\n\nBasic prompt structure = \"Please solve the following task: [test_case_task]\" (v1 starting point).\n\nCurrent limitations = no output formatting instructions, hardcoded scoring (score=10), verbose Claude responses.\n\nRuntime = ~31 seconds with Haiku model for full dataset execution.\n\nOutput format = array of objects containing Claude output, original test case, and score.\n\nNext step = implement proper grading system to replace hardcoded scores.\n\nEval pipeline core = dataset + prompt + LLM + grader, with minimal code complexity.\n</note>\n\n<note title=\"Model Based Grading\">\nModel Based Grading = evaluation system that takes model outputs and assigns objective scores (typically 1-10 scale, 10 = highest quality)\n\nThree grader types:\n- Code graders = programmatic checks (length, word presence, syntax validation, readability scores)\n- Model graders = additional API call to evaluate original model output, highly flexible for quality/instruction-following assessment\n- Human graders = person evaluates responses, most flexible but time-consuming and tedious\n\nKey requirements: Must return objective signal (usually numerical score). Define evaluation criteria upfront.\n\nImplementation pattern for model graders:\n- Create detailed prompt requesting strengths/weaknesses/reasoning/score (not just score alone to avoid default middling scores)\n- Use JSON response format with pre-filled assistant message and stop sequences\n- Parse returned JSON for score and reasoning\n- Calculate average scores across test cases for final metric\n\nModel graders offer high flexibility but may be inconsistent. Still provides objective baseline for prompt optimization.\n</note>\n\n<note title=\"Code Based Grading\">\nCode Based Grading = automated validation system for LLM outputs containing code, JSON, or regex\n\nCore Implementation:\n- validate_json() = attempts JSON parsing, returns 10 if valid, 0 if error\n- validate_python() = attempts AST parsing, returns 10 if valid, 0 if error  \n- validate_regex() = attempts regex compilation, returns 10 if valid, 0 if error\n\nDataset Requirements:\n- Must include \"format\" key specifying expected output type (JSON/Python/RegEx)\n- Updated via prompt template modification for automated dataset generation\n\nPrompt Engineering:\n- Instruct model to respond only with raw code/JSON/regex\n- No comments, explanations, or commentary\n- Use pre-filled Assistant message with \\`\\`\\`code\\`\\`\\` blocks\n- Add stop sequences to extract clean output\n\nScoring System:\n- Final score = (model_score + syntax_score) / 2\n- Combines semantic evaluation with syntax validation\n- Enables measurement of both correctness and technical validity\n\nKey Limitation = requires known expected format for proper validator selection\n</note>\n\n<note title=\"Prompt Engineering\">\nPrompt Engineering = improving prompts to get more reliable, higher-quality outputs from language models.\n\nModule Structure: Start with initial poor prompt → Apply prompt engineering techniques step-by-step → Evaluate improvements after each technique → Observe performance gains over time.\n\nExample Goal: Generate one-day meal plan for athletes based on height, weight, physical goal, dietary restrictions.\n\nTechnical Setup:\n- Updated eval pipeline with flexible prompt evaluator class\n- Supports concurrency (adjust max_concurrent_tasks based on rate limits)\n- generate_dataset() method creates test cases with specified inputs\n- run_prompt() function processes each test case individually\n\nKey Components:\n- prompt_input_spec = dictionary defining required prompt inputs\n- extra_criteria = additional validation requirements for model grading\n- output.html = formatted evaluation report showing test case results and scores\n\nProcess: Write initial prompt → Interpolate test case inputs → Run evaluation → Apply engineering techniques → Re-evaluate → Repeat until satisfactory performance.\n\nInitial Results: Expect poor scores (example: 2.32) with basic prompts, especially when using less capable models. Scores improve as techniques are applied.\n</note>\n\n<note title=\"Being Clear and Direct\">\nBeing Clear and Direct = Use simple, direct language with action verbs in the first line of prompts to specify the exact task.\n\nFirst line importance = Most critical part of prompt that sets the foundation for AI response.\n\nStructure = Action verb + clear task description + output specifications.\n\nExamples:\n- \"Write three paragraphs about how solar panels work\"\n- \"Identify three countries that use geothermal energy and for each include generation stats\"\n- \"Generate a one day meal plan for an athlete that meets their dietary restrictions\"\n\nKey components = Action verb at start + direct task statement + expected output details.\n\nResult = Improved prompt performance (example showed score increase from 2.32 to 3.92).\n</note>\n\n<note title=\"Being Specific\">\nBeing Specific = adding guidelines or steps to direct model output in particular direction\n\nTwo types of guidelines:\nType A (Attributes) = list qualities/attributes desired in output (length, structure, format)\nType B (Steps) = provide specific steps for model to follow in reasoning process\n\nType A controls output characteristics. Type B controls how model arrives at answer.\n\nBoth techniques often combined in professional prompts.\n\nWhen to use:\n- Type A (attributes): recommended for almost all prompts\n- Type B (steps): use for complex problems where you want model to consider broader perspective or additional viewpoints it might not naturally consider\n\nExample improvement: meal planning prompt score jumped from 3.92 to 7.86 when guidelines added, demonstrating significant quality improvement through specificity.\n</note>\n\n<note title=\"Structure with XML Tags\">\nXML Tags for Prompt Structure = Using XML tags to organize and delineate different content sections within prompts to improve AI comprehension.\n\nPurpose = When interpolating large amounts of content into prompts, XML tags help AI models distinguish between different types of information and understand text grouping.\n\nImplementation = Wrap content sections in descriptive XML tags like <sales_records></sales_records> or <my_code></my_code> rather than dumping unstructured text.\n\nTag naming = Use descriptive, specific tag names (e.g., \"sales_records\" better than \"data\") to provide context about content nature.\n\nExample use case = Debugging prompt with mixed code and documentation becomes clearer when separated into <my_code> and <docs> tags.\n\nBenefits = Makes prompt structure obvious to AI, reduces confusion about content boundaries, improves output quality even for smaller content blocks.\n\nApplication = Can wrap any interpolated content like <athlete_information> even when content is short, to clarify it's external input requiring consideration.\n</note>\n\n<note title=\"Providing Examples\">\nOne-shot/Multi-shot prompting = providing examples in prompts to guide model behavior. One-shot = single example, multi-shot = multiple examples.\n\nImplementation: Structure examples with XML tags containing sample input and ideal output. Always wrap examples clearly to distinguish from actual prompt content.\n\nKey applications:\n- Corner case handling (sarcasm detection, edge scenarios)\n- Complex output formatting (JSON structures, specific formats)\n- Clarifying expected response quality/style\n\nBest practices:\n- Add context for corner cases (\"be especially careful with sarcasm\")\n- Include reasoning explaining why output is ideal\n- Use highest-scoring examples from prompt evaluations as templates\n- Place examples after main instructions/guidelines\n\nEffectiveness boost: Combine examples with explanations of what makes them ideal to reinforce desired output characteristics.\n</note>\n\n<note title=\"Introducing Tool Use\">\nTool use = method for Claude to access external information beyond training data.\n\nDefault limitation: Claude only knows information from training data, lacks current/real-time information.\n\nTool use flow:\n1. Send initial request to Claude + instructions for external data access\n2. Claude evaluates if external data needed, requests specific information\n3. Server runs code to fetch requested data from external sources\n4. Send follow-up request to Claude with retrieved data\n5. Claude generates final response using original prompt + external data\n\nWeather example: User asks current weather → Claude requests weather data → Server calls weather API → Claude receives weather data → Claude provides informed weather response.\n\nKey concept: Tools enable Claude to augment responses with live/current information by orchestrating external data retrieval between Claude's requests.\n</note>\n\n<note title=\"Project Overview\">\n**Project Overview**\n\nGoal = Teach Claude to set time-based reminders through tool implementation in Jupyter notebook\n\nTarget interaction = User: \"Set reminder for doctor's appointment, week from Thursday\" → Claude: \"I will remind you at that point in time\"\n\n**Three core problems requiring tools:**\n\n1. Time knowledge gap = Claude knows current date but not exact time\n2. Time calculation errors = Claude sometimes miscalculates time-based addition (e.g., 379 days from January 13th, 1973)\n3. No reminder mechanism = Claude understands reminder concept but lacks implementation capability\n\n**Three corresponding tools to build:**\n\n1. Current datetime tool = Gets current date + time\n2. Duration addition tool = Adds time duration to datetime (e.g., current date + 20 days)\n3. Reminder setting tool = Actually sets the reminder\n\nImplementation approach = One tool at a time, building toward multi-tool coordination\n</note>\n\n<note title=\"Tool Functions\">\nTool Functions = Python functions executed automatically when Claude needs extra information to help users.\n\nKey characteristics:\n- Plain Python functions called by Claude when it determines additional data is needed\n- Must use descriptive function names and argument names\n- Should validate inputs and raise errors with meaningful messages\n- Error messages are visible to Claude, allowing it to retry with corrected parameters\n\nBest practices:\n1. Well-named functions and arguments\n2. Input validation with immediate error raising for invalid inputs\n3. Meaningful error messages that guide correction\n\nExample implementation pattern:\n\\`\\`\\`\ndef get_current_datetime(date_format=\"%Y%m%d %H:%M:%S\"):\n    if not date_format:\n        raise ValueError(\"date format cannot be empty\")\n    return datetime.now().strftime(date_format)\n\\`\\`\\`\n\nTool function workflow: Claude identifies need for information → calls tool function → receives result or error → may retry with corrections if error occurred.\n\nPurpose: Extend Claude's capabilities beyond its training data by providing access to real-time information like current datetime, weather, etc.\n</note>\n\n<note title=\"Tool Schemas\">\nTool Schemas = JSON schema specifications that describe tool functions and their parameters for language models\n\nJSON Schema = data validation specification (not ML-specific) used to validate JSON data, adopted by ML community for tool calling\n\nTool Schema Structure:\n- name: tool identifier \n- description: 3-4 sentences explaining what tool does, when to use, what data it returns\n- input_schema: actual JSON schema describing function arguments with types and descriptions\n\nSchema Generation Trick:\n1. Take tool function to Claude.ai\n2. Prompt: \"write valid JSON schema spec for tool calling for this function, follow best practices in attached documentation\"\n3. Attach Anthropic API documentation tool use page\n4. Copy generated schema\n\nImplementation Pattern:\n- Name functions descriptively\n- Name schemas as [function_name]_schema\n- Import ToolParam from anthropic.types\n- Wrap schema dictionary with ToolParam() to prevent type errors\n\nPurpose = inform Claude about available tools, required arguments, and usage context through standardized JSON validation format\n</note>\n\n<note title=\"Handling Message Blocks\">\n**Tool-Enabled Claude Requests**\n\nStep 3: Making requests to Claude with tools = include tool schema in request alongside user message using \\`tools\\` keyword argument containing JSON schema specs.\n\n**Multi-Block Messages**\n\nContent structure change = messages now contain multiple blocks instead of just text blocks.\n\nTool response format = assistant message with:\n- Text block = user-facing explanation \n- Tool use block = contains function name + arguments for tool execution\n\n**Message History Management**\n\nCritical requirement = manually maintain conversation history since Claude stores nothing.\n\nMulti-block handling = append entire response.content (all blocks) to messages list, not just text.\n\nHelper function updates needed = add_user_message and add_assistant_message functions must support multiple blocks instead of single text blocks only.\n\nConversation flow = user message → assistant response with tool use block → execute tool → respond back to Claude with full history.\n</note>\n\n<note title=\"Sending Tool Results\">\nTool Results = Results from executed tool functions sent back to Claude in follow-up requests.\n\nProcess: Execute tool function requested by Claude → Create tool result block → Send follow-up request with full conversation history.\n\nTool Result Block Structure:\n- tool_use_id = Matches ID from original tool use block to pair requests with results\n- content = Tool function output converted to string (usually JSON)\n- is_error = Boolean flag for function execution errors (default false)\n\nTool Use ID Purpose = Links multiple tool requests to correct results when Claude makes simultaneous tool calls. Each tool use gets unique ID, tool results must reference matching IDs.\n\nFollow-up Request Requirements:\n- Include complete message history (original user message + assistant tool use message + new user message with tool result)\n- Must include original tool schemas even if not using tools again\n- Tool result block goes in user message, not assistant message\n\nConversation Flow: User request → Claude assistant response (text + tool use blocks) → Server executes tool → User message with tool result block → Claude final response with integrated results.\n</note>\n\n<note title=\"Multi-Turn Conversations with Tools\">\nMulti-Turn Tool Conversations = conversations where Claude uses multiple tools sequentially to answer a single user query.\n\nTool Chaining Process = user asks question → Claude requests first tool → tool executed → result returned → Claude requests second tool → tool executed → result returned → Claude provides final answer.\n\nExample Flow = user asks \"what day is 103 days from today\" → Claude calls get_current_datetime → Claude calls add_duration_to_datetime → Claude provides answer.\n\nImplementation Pattern = while loop that continues calling Claude until no more tool requests, checking each response for tool_use blocks.\n\nrun_conversation Function = takes initial messages, loops through Claude calls, executes requested tools, adds results to conversation, continues until final response.\n\nRequired Refactors:\n- add_user_message/add_assistant_message = updated to handle multiple message blocks instead of just plain text\n- chat function = accepts tools parameter, returns entire message instead of just first text block\n- text_from_message helper = extracts all text blocks from a message with multiple content blocks\n\nKey Insight = can't predict how many tools user queries will require, so system must handle arbitrary chains of tool calls automatically.\n</note>\n\n<note title=\"Implementing Multiple Turns\">\n**Multiple Turns Implementation = continuously calling Claude until it stops requesting tools**\n\n**Stop Reason Field = indicates why Claude stopped generating text**\n- stop_reason = \"tool_use\" means Claude wants to call a tool\n- Other values exist but tool_use is most commonly checked\n\n**run_conversation Function = main loop that:**\n1. Calls Claude with messages + available tools\n2. Adds assistant response to conversation history\n3. Checks stop_reason - if not \"tool_use\", breaks loop\n4. If tool_use, calls run_tools function\n5. Adds tool results as user message\n6. Repeats until no more tool requests\n\n**run_tools Function = processes multiple tool use blocks:**\n1. Filters message.content for blocks with type=\"tool_use\"\n2. Iterates through each tool request\n3. Runs appropriate tool function via run_tool helper\n4. Creates tool_result blocks with: type=\"tool_result\", tool_use_id=original_id, content=JSON_encoded_output, is_error=boolean\n5. Returns list of all tool result blocks\n\n**run_tool Function = dispatcher that:**\n- Takes tool_name and tool_input\n- Uses if statements to match tool names to functions\n- Executes appropriate tool function\n- Scalable for adding multiple tools\n\n**Error Handling = try/except blocks around tool execution:**\n- Success: is_error=false, content=tool_output\n- Failure: is_error=true, content=error_message\n\n**Key Architecture Points:**\n- Assistant messages can contain multiple blocks (text + multiple tool_use)\n- Each tool_use block gets separate tool_result response\n- Tool results sent back as user message containing all results\n- Process repeats until Claude provides final text-only response\n</note>\n\n<note title=\"Using Multiple Tools\">\nMultiple Tools Implementation = Adding additional tools to an existing tool system after initial framework setup.\n\nProcess = 3 steps: (1) Add tool schemas to RunConversation function's tools list, (2) Add conditional cases in RunTool function to handle new tool names, (3) Implement actual tool functions.\n\nKey Components:\n- RunConversation function = Contains tools list that makes Claude aware of available tools\n- RunTool function = Routes tool calls to appropriate functions based on tool name\n- Tool schemas = Define tool structure for the AI model\n- Tool functions = Actual implementation code\n\nExample Tools Added:\n- AddDurationToDateTime = Calculates date/time with duration offset\n- SetReminder = Creates reminder (mock implementation that prints confirmation)\n\nTool Chaining = AI can use multiple tools sequentially in single conversation (e.g., calculate date first, then set reminder with result).\n\nMessage Structure = Assistant responses can contain multiple blocks: text blocks + tool use blocks in same message.\n\nScalability = After initial framework setup, adding new tools becomes simple pattern of schema + routing + implementation.\n</note>\n\n<note title=\"The Batch Tool\">\nBatch Tool = tool that enables Claude to run multiple tools in parallel within a single Assistant message instead of making separate sequential requests.\n\nProblem: Claude can technically send multiple tool use blocks in one message but rarely does so in practice, leading to unnecessary sequential tool calls.\n\nSolution: Create batch tool schema that takes list of invocations (each containing tool name + arguments). Instead of calling tools directly, Claude calls batch tool with array of desired tool executions.\n\nImplementation:\n- Add batch tool to schema with invocations parameter\n- Create run_batch function that iterates through invocations list\n- Extract tool name and JSON-parsed arguments from each invocation\n- Call run_tool function for each requested tool\n- Return batch_output list containing results from all tool executions\n\nMechanism: Tricks Claude into parallel tool execution by providing higher-level abstraction that manually handles what multiple tool use blocks would accomplish automatically.\n\nResult: Single request-response cycle instead of multiple sequential rounds for parallel-executable tasks.\n</note>\n\n<note title=\"Tools for Structured Data\">\nTools for Structured Data = alternative method to extract structured JSON from data sources using Claude's tool system instead of message pre-fill and stop sequences.\n\nKey differences from prompt-based extraction:\n- More reliable output\n- More complex setup\n- Requires JSON schema specification\n\nCore Process:\n1. Define JSON schema for tool where inputs = desired data structure\n2. Send prompt + schema to Claude\n3. Claude calls tool with structured arguments matching schema\n4. Extract JSON from tool use block (no tool result needed)\n\nCritical requirement = Force tool calling using tool_choice parameter:\n- tool_choice = {\"type\": \"tool\", \"name\": \"your_tool_name\"}\n- Ensures Claude always calls specified tool\n\nImplementation steps:\n1. Create schema definition for extraction tool\n2. Update chat function to accept tool_choice parameter\n3. Pass tool_choice to client.messages.create()\n4. Access structured data from response.content[0].input\n\nUse cases = When reliability more important than simplicity. Prompt-based methods better for quick/simple extractions, tools better for complex/reliable extractions.\n</note>\n\n<transcript title=\"Fine Grained Tool Calling\">\nTool Streaming = streaming API responses while using tools with Claude\n\nKey Components:\n- Standard streaming returns content_block_delta events\n- Tool streaming adds input_json_delta events with partial_json (chunk) and snapshot (cumulative sum)\n- Implementation requires handling additional event type in streaming pipeline\n\nFine-Grained Tool Calling = feature that disables JSON validation for faster streaming\n\nDefault Behavior:\n- Claude generates JSON chunks for tool arguments\n- API buffers chunks until complete top-level key-value pair is generated\n- Validates JSON against schema before sending chunks to server\n- Results in delays followed by burst of chunks arriving simultaneously\n\nFine-Grained Mode (fine_grained: true):\n- Disables API-side JSON validation\n- Sends chunks immediately as generated\n- Provides traditional streaming experience\n- Requires client-side error handling for invalid JSON\n\nTrade-offs:\n- Default = slower but validated JSON\n- Fine-grained = faster streaming but potential invalid JSON (like \"undefined\" instead of null)\n- Invalid JSON in default mode gets wrapped as string rather than proper object structure\n\nUse Cases:\n- Fine-grained useful for immediate UI updates or early processing of tool arguments\n- Default sufficient when validation delays acceptable\n</transcript>\n\n\n<note title=\"The Text Edit Tool\">\nText Editor Tool = built-in Claude tool for file/text operations (read, write, create, replace, undo files/directories)\n\nKey characteristics:\n- Only JSON schema built into Claude, implementation must be custom-coded\n- Schema stub sent to Claude gets auto-expanded to full schema\n- Schema type string varies by Claude model version (3.5 vs 3.7 have different dates)\n- Enables Claude to act as software engineer out-of-the-box\n\nRequired implementation:\n- Custom class/functions to handle Claude's tool use requests\n- Functions for: view files, string replace, create files, etc.\n- Actual file system operations not provided by Claude\n\nWorkflow:\n1. Send minimal schema stub to Claude (name + type with version-specific date)\n2. Claude expands to full schema internally\n3. Claude sends tool use requests\n4. Custom implementation executes actual file operations\n5. Results sent back to Claude\n\nUse cases:\n- Replicate AI code editor functionality\n- File system operations where native editors unavailable\n- Automated code generation/refactoring\n- Multi-file project manipulation\n\nBenefits = approximates fancy code editor capabilities through API calls rather than GUI interaction.\n</note>\n\n<note title=\"The Web Search Tool\">\nWeb Search Tool = built-in Claude tool for searching web to find up-to-date/specialized information for user questions\n\nImplementation = no custom code needed, Claude handles search execution automatically\n\nSchema Requirements:\n- type: \"web_search_20250305\"  \n- name: \"web_search\"\n- max_uses: number (limits total searches, default 5)\n- allowed_domains: optional list to restrict search to specific domains\n\nResponse Structure:\n- Text blocks = Claude's explanatory text\n- Tool use blocks = search queries Claude executed  \n- Web search result blocks = found pages (title, URL)\n- Citation blocks = specific text supporting Claude's statements\n\nKey Features:\n- Multiple searches possible per request (up to max_uses limit)\n- Domain restriction available for quality control\n- Citation system links statements to source material\n\nUI Rendering Pattern:\n- Display text blocks as normal text\n- Show search results as reference list\n- Highlight citations with source attribution (domain, title, URL, quoted text)\n\nUse Case Example: Restricting to NIH.gov for medical/exercise advice ensures scientifically-backed information vs generic web content.\n</note>\n\n<note title=\"Introducing Retrieval Augmented Generation\">\nRAG = Retrieval Augmented Generation technique for querying large documents using language models.\n\nProblem: How to extract specific information from large documents (100-1000+ pages) using Claude without hitting context limits.\n\nOption 1 (Direct approach): Place entire document text directly into prompt.\n- Limitations: Hard token limits, decreased effectiveness with longer prompts, higher costs, slower processing\n\nOption 2 (RAG approach): Two-step process\n- Step 1: Break document into small chunks\n- Step 2: For user questions, find most relevant chunks and include only those in prompt\n\nRAG benefits: Model focuses on relevant content, scales to large/multiple documents, smaller prompts, lower costs, faster processing\n\nRAG downsides: More complexity, requires preprocessing, needs search mechanism to find relevant chunks, no guarantee chunks contain complete context, multiple chunking strategies possible (equal portions vs header-based)\n\nKey challenge: Defining relevance and optimal chunking strategy for specific use cases.\n\nRAG trades simplicity for scalability and efficiency but requires careful implementation and evaluation.\n</note>\n\n<note title=\"Text Chunking Strategies\">\nText Chunking Strategies = process of dividing documents into smaller pieces for RAG pipelines\n\nCore Problem: Chunking quality directly impacts RAG performance. Poor chunking leads to irrelevant context retrieval (e.g., medical \"bug\" text retrieved for software engineering query about bugs).\n\nThree Main Strategies:\n\n1. Size-Based Chunking = dividing text into equal-length strings\n- Pros: Easy to implement, most common in production\n- Cons: Cut-off words, lacks context\n- Solution: Overlap strategy = include characters from neighboring chunks to preserve context\n- Trade-off: Creates text duplication but improves chunk meaning\n\n2. Structure-Based Chunking = dividing based on document structure (headers, paragraphs, sections)\n- Best for structured documents (markdown, HTML)\n- Limitation: Requires guaranteed document formatting\n- Example: Split on markdown headers (##) to create section-based chunks\n\n3. Semantic-Based Chunking = using NLP to group related sentences/sections\n- Most advanced technique\n- Groups consecutive sentences based on semantic similarity\n- Complex implementation\n\nKey Implementation Notes:\n- Chunk by character = most reliable fallback, works with any document type\n- Chunk by sentence = good middle ground if sentence detection works reliably\n- Chunk by section = optimal results but requires structured input\n- Strategy choice depends on document type guarantees and use case requirements\n\nRule: No universal best chunking method - depends on document structure guarantees and specific use case.\n</note>\n\n<note title=\"Text Embeddings\">\nText Embeddings = numerical representation of text meaning generated by embedding models\n\nEmbedding Model = takes text input, outputs long list of numbers (range -1 to +1)\n\nEmbedding Numbers = scores representing unknown qualities/features of input text. Each number theoretically scores different aspects (happiness, topic relevance, etc.) but actual meaning is unknown to users.\n\nSemantic Search = uses text embeddings to find text chunks related to user questions in RAG pipelines. Solves the search problem of matching user queries to relevant document chunks.\n\nRAG Pipeline Process = extract text chunks → user submits query → find related chunks using semantic search → add relevant chunks as context to prompt\n\nImplementation = Anthropic recommends Voyage AI for embedding generation. Requires separate account/API key. Free to start, easy integration via SDK.\n\nKey Insight = Embeddings enable semantic similarity matching rather than keyword matching, allowing better understanding of text relationships for retrieval tasks.\n</note>\n\n<note title=\"The Full RAG Flow\">\nRAG Flow = 7-step process combining text chunking, embeddings, and vector search to retrieve relevant context for LLM queries.\n\nStep 1: Text Chunking = Split source documents into separate text pieces\nStep 2: Generate Embeddings = Convert text chunks into numerical vectors using embedding models\nStep 3: Normalization = Scale vector magnitudes to 1.0 (handled automatically by embedding APIs)\nStep 4: Vector Database Storage = Store embeddings in specialized database optimized for numerical vector operations\nStep 5: Query Processing = Convert user question into embedding using same model\nStep 6: Similarity Search = Find most similar stored embeddings using cosine similarity calculation\nStep 7: Prompt Assembly = Combine user question with retrieved relevant text chunks, send to LLM\n\nKey Math Concepts:\n- Cosine Similarity = cosine of angle between vectors, returns values -1 to 1, closer to 1 means more similar\n- Cosine Distance = 1 minus cosine similarity, values closer to 0 mean higher similarity\n- Vector Database = performs similarity calculations to find closest matching embeddings\n\nProcess Flow: Pre-processing (steps 1-4) → User Query → Real-time retrieval (steps 5-7) → LLM Response\n</note>\n\n<note title=\"Implementing the Rag Flow\">\nRAG Flow Implementation = practical walkthrough of 5-step retrieval-augmented generation process\n\nStep 1: Text Chunking = split document into sections using chunk_by_section function on report.MD file\n\nStep 2: Embedding Generation = create vector representations for each chunk using generate_embedding function (supports single string or list of strings input)\n\nStep 3: Vector Store Population = create vector index instance, loop through chunk-embedding pairs using zip(), store each pair with store.add_vector(embedding, {content: chunk}). Store original text with embeddings for meaningful retrieval results.\n\nStep 4: Query Processing = user asks question \"what did software engineering department do last year\", generate embedding for user query\n\nStep 5: Similarity Search = use store.search(user_embedding, 2) to find 2 most relevant chunks, returns results with cosine distances (0.71 for section two, 0.72 for methodology section)\n\nKey Components:\n- Vector Index Class = custom vector database implementation\n- Cosine Distance = similarity metric between query and stored embeddings\n- Metadata Storage = storing original text content alongside embeddings enables meaningful retrieval\n\nWorkflow complete but has limitations requiring further improvements.\n</note>\n\n<note title=\"BM25 Lexical Search\">\nBM25 = Best Match 25, a lexical search algorithm commonly used in RAG pipelines to complement semantic search.\n\nProblem with semantic search alone = Can miss exact term matches, returning irrelevant results even when specific terms appear frequently in certain documents.\n\nHybrid search approach = Combines semantic search (embeddings/vector database) with lexical search (BM25) in parallel, then merges results for better balance.\n\nBM25 algorithm steps:\n1. Tokenize user query into separate terms (remove punctuation, split on spaces)\n2. Count frequency of each term across all text chunks/documents\n3. Assign relative importance to terms based on usage frequency (rare terms = higher importance, common terms like \"a\" = lower importance)\n4. Rank text chunks by how often they contain higher-weighted terms\n\nKey insight = Frequently used terms across corpus are less important for search relevance than rare, specific terms.\n\nBM25 advantages = Better at finding exact term matches, prioritizes documents containing rare/specific search terms, complements semantic search weaknesses.\n\nImplementation = Both semantic and lexical search systems use similar APIs (add_document, search functions) making them easy to combine.\n\nNext step = Merge results from both search systems to get benefits of semantic understanding plus exact term matching.\n</note>\n\n<note title=\"A Multi-Index Rag Pipeline\">\nMulti-Index RAG Pipeline = system combining semantic search (vector index) and lexical search (BM25 index) for improved retrieval accuracy.\n\nKey Components:\n- Vector Index = semantic similarity search using embeddings\n- BM25 Index = lexical/keyword-based search \n- Retriever Class = wrapper that forwards queries to both indexes and merges results\n\nReciprocal Rank Fusion = technique for merging search results from different indexes. Formula: RRF_score = sum of (1/(rank + 1)) across all search methods for each document. Documents ranked by highest combined score.\n\nExample: Vector search returns [doc2, doc7, doc6], BM25 returns [doc6, doc2, doc7]. After RRF calculation, final ranking becomes [doc2, doc6, doc7] because doc2 ranked high in both methods.\n\nBenefits:\n- Improved search accuracy by combining different search paradigms\n- Modular design with standardized API (search() and add_document() methods)\n- Easy to extend with additional search indexes\n- Better handling of edge cases where single method fails\n\nImplementation pattern allows multiple search methodologies to work together while maintaining separate, isolated index classes.\n</note>\n\n<note title=\"Reranking Results\">\nReranking = post-processing step that uses LLM to reorder search results by relevance after initial retrieval.\n\nProcess: Run vector + BM25 search → merge results → pass to LLM with prompt asking to rank documents by relevance → get reordered results.\n\nImplementation details: Use document IDs instead of full text for efficiency. LLM receives user query + candidate documents + instruction to return most relevant docs in decreasing order. Assistant message pre-fill + stop sequence ensures structured JSON output.\n\nTradeoffs: Increases search accuracy by leveraging LLM's understanding of semantic relevance. Increases latency due to additional LLM call. Particularly effective when initial retrieval methods miss nuanced query intent (e.g., \"ENG team\" vs \"engineering team\").\n\nExample improvement: Query \"What did engineering team do with incident 2023?\" correctly prioritized software engineering section over cybersecurity section after reranking, despite hybrid search initially ranking it lower.\n</note>\n\n<note title=\"Contextual Retrieval\">\nContextual Retrieval = technique to improve RAG pipeline accuracy by adding context to document chunks before embedding.\n\nProblem: When documents are split into chunks, individual chunks lose context from the original document, reducing retrieval accuracy.\n\nSolution: Pre-processing step that adds contextual information to each chunk before inserting into retriever database.\n\nProcess:\n1. Take individual chunk + original source document\n2. Send to LLM (Claude) with prompt asking to generate situating context\n3. LLM generates brief context explaining chunk's relationship to larger document\n4. Join generated context with original chunk = \"contextualized chunk\"\n5. Use contextualized chunk as input to vector/BM25 indexes\n\nLarge Document Handling: If source document too large for single prompt, use selective context strategy:\n- Include starter chunks (1-3) from document beginning for summary/abstract\n- Include chunks immediately before target chunk for local context\n- Skip middle chunks that provide less relevant context\n\nImplementation: add_context function takes text chunk + source text, generates context via LLM, concatenates context with original chunk, returns contextualized version.\n\nBenefit: Chunks retain ties to larger document structure and cross-references, improving retrieval accuracy for complex documents with interconnected sections.\n</note>\n\n<note title=\"Extended Thinking\">\nExtended Thinking = Claude feature that allows reasoning time before generating final response\n\nKey mechanics:\n- Displays separate thinking process visible to users\n- Increases accuracy for complex tasks but adds cost (charged for thinking tokens) and latency\n- Thinking budget = minimum 1024 tokens allocated for thinking phase\n- Max tokens must exceed thinking budget (e.g., budget 1024 requires max_tokens ≥ 1025)\n\nWhen to use:\n- Enable after prompt optimization fails to achieve desired accuracy\n- Use prompt evals to determine necessity\n\nResponse structure:\n- Thinking block = contains reasoning text + cryptographic signature\n- Text block = final response\n- Signature = prevents tampering with thinking text (safety measure)\n\nSpecial cases:\n- Redacted thinking blocks = encrypted thinking text flagged by safety systems\n- Provided for conversation continuity without losing context\n- Can force redacted blocks using test string: \"entropic magic string triggered redacted thinking [special characters]\"\n\nImplementation:\n- Set thinking=true and thinking_budget parameter\n- Ensure max_tokens > thinking_budget for adequate response generation capacity\n</note>\n\n<note title=\"Image Support\">\nClaude Vision Capabilities = ability to process images within user messages for analysis, comparison, counting, and description tasks.\n\nImage Limitations:\n- Max 100 images per request\n- Size/dimension restrictions apply\n- Images consume tokens (charged based on pixel height/width calculation)\n\nImage Block Structure = special block type within user messages that holds either raw image data (base64) or URL reference to online image. Multiple image blocks allowed per message.\n\nCritical Success Factor = strong prompting techniques required for accurate results. Simple prompts often fail.\n\nPrompting Techniques for Images:\n- Step-by-step analysis instructions\n- One-shot/multi-shot examples (alternating image and text pairs)\n- Clear guidelines and verification steps\n- Structured analysis frameworks\n\nExample Use Case = automated fire risk assessment from satellite imagery analyzing tree density, property access, roof overhang, and assigning numerical risk scores.\n\nImplementation = base64 encode image data, create message with image block (type: image, source: base64, media_type, data) followed by text block containing detailed prompt instructions.\n\nKey Takeaway = image accuracy depends entirely on prompt sophistication, not just image quality.\n</note>\n\n<note title=\"PDF Support\">\nPDF Support in Claude:\n\nClaude can read PDF files directly using similar code to image processing. \n\nKey implementation changes:\n- File type = \"document\" instead of \"image\"\n- Media type = \"application/pdf\" instead of \"image/png\"\n- Variable naming = file_bytes instead of image_bytes\n\nClaude PDF capabilities = read text + images + charts + tables + mixed content extraction\n\nPDF processing = one-stop solution for comprehensive document analysis\n\nUsage pattern = same as image input but with document-specific parameters\n</note>\n\n<note title=\"Citations\">\nCitations = feature allowing Claude to reference source documents and show where information comes from\n\nCitation types:\n- citation_page_location = for PDF documents, shows document index/title/start page/end page/cited text\n- citation_char_location = for plain text, shows character position in text block\n\nImplementation:\n- Add \"citations\": {\"enabled\": true} to request\n- Add \"title\" field to identify source document\n- Works with both PDF files and plain text sources\n\nResponse structure = content becomes list of text blocks, some containing citations arrays with location data\n\nPurpose = transparency for users to verify Claude's information sources and check accuracy of interpretations\n\nUI benefit = enables citation popups/overlays showing source document, page numbers, and exact cited text when users hover over referenced content\n\nKey use case = ensuring users can investigate how Claude builds responses from source materials rather than appearing to speak from memory alone\n</note>\n\n<note title=\"Prompt Caching\">\nPrompt Caching = feature that speeds up Claude's responses and reduces text generation costs by reusing computational work from previous requests.\n\nNormal request flow: User sends message → Claude processes input (creates internal data structures, performs calculations) → Claude generates output → Claude discards all processing work → Ready for next request.\n\nProblem: When follow-up requests contain identical input messages, Claude must repeat all the same computational work it just threw away, creating inefficiency.\n\nSolution: Prompt caching stores the results of input message processing in temporary cache instead of discarding. When identical input appears in subsequent requests, Claude retrieves cached work rather than reprocessing, dramatically speeding response generation.\n\nKey benefit: Reuses previous computational work to avoid redundant processing of repeated content.\n</note>\n\n<note title=\"Rules of Prompt Caching\">\nPrompt Caching = system that saves processing work from initial request to reuse in follow-up requests with identical content\n\nCore mechanism: Initial request → Claude processes + saves work to cache → Follow-up requests with identical content → Claude retrieves cached work instead of reprocessing\n\nCache duration = 1 hour maximum\n\nCache activation requires manual cache breakpoint addition to message blocks\n\nText block formats:\n- Shorthand: content = \"text string\" (cannot add cache control)\n- Longhand: content = [{\"type\": \"text\", \"text\": \"content\", \"cache_control\": {...}}] (required for caching)\n\nCache scope = all content up to and including breakpoint gets cached\n\nCache invalidation = any change in content before breakpoint invalidates entire cache\n\nContent processing order = tools → system prompt → messages (joined together)\n\nCache breakpoint placement options:\n- Tool schemas\n- System prompts  \n- Message blocks (text, image, tool use, tool result)\n\nMaximum breakpoints = 4 per request\n\nMultiple breakpoints = create multiple cache layers, partial cache hits possible if only later content changes\n\nMinimum cache threshold = 1024 tokens required for content to be cached\n\nBest use cases = repeated identical content (system prompts, tool definitions, static message prefixes)\n</note>\n\n<note title=\"Prompt Caching in Action\">\nPrompt Caching Implementation = automatically caches tool schemas and system prompts to reduce token usage\n\nSetup = modify chat function to enable caching by default for tools and system prompts\n\nTool Schema Caching = add cache_control field with type \"ephemeral\" to last tool in list. Best practice: create copy of tools list, clone last tool schema, add cache control, then overwrite to avoid modifying original schemas\n\nSystem Prompt Caching = wrap system prompt in text block dictionary with cache_control type \"ephemeral\"\n\nMultiple Cache Breakpoints = can set cache points for both tools and system prompt in single request\n\nCache Order = tools → system prompt → messages\n\nToken Usage Patterns:\n- cache_creation_input_tokens = tokens written to cache on first use\n- cache_read_input_tokens = tokens retrieved from cache on subsequent identical requests\n- Partial cache reads possible when some content matches cached data\n\nCache Invalidation = any change to cached content (tools or system prompt) invalidates cache, forces new cache creation\n\nUse Cases = identical content across requests - same tool schemas, system prompts, or message sequences\n</note>\n\n<note title=\"Code Execution and the Files API\">\nFiles API = allows uploading files ahead of time and referencing them later via file ID instead of including raw file data in each request. Upload file → get file metadata object with ID → use ID in future requests.\n\nCode Execution = server-based tool where Claude executes Python code in isolated Docker containers. No implementation needed, just include predefined tool schema. Claude can run code multiple times, interpret results, generate final response.\n\nKey constraints: Docker containers have no network access. Data input/output relies on Files API integration.\n\nCombined workflow: Upload file via Files API → get file ID → include ID in container upload block → ask Claude to analyze → Claude writes/executes code with access to uploaded file → returns analysis and results.\n\nClaude can generate files (plots, reports) inside container that can be downloaded using file IDs returned in response.\n\nUse cases: Data analysis, file processing, automated code generation for complex tasks. Response contains code blocks, execution results, and final analysis.\n\nImplementation: Use container upload block with file ID, include analysis prompt, Claude handles code execution automatically.\n</note>\n\n<note title=\"Introducing MCP\">\nMCP = Model Context Protocol, communication layer providing Claude with context and tools without requiring developers to write tedious code.\n\nArchitecture: MCP client connects to MCP server. Server contains tools, resources, and prompts as internal components.\n\nProblem solved: Eliminates burden of authoring/maintaining numerous tool schemas and functions for service integrations. Example: GitHub chatbot would require implementing tools for repositories, pull requests, issues, projects - significant developer effort.\n\nSolution: MCP server handles tool definition and execution instead of your application server. MCP servers = interfaces to outside services, wrapping functionality into ready-to-use tools.\n\nKey benefits: Developers avoid writing tool schemas and function implementations themselves.\n\nCommon questions:\n- Who creates MCP servers? Anyone, often service providers make official implementations (AWS, etc.)\n- vs direct API calls? MCP eliminates need to author tool schemas/functions yourself\n- vs tool use? MCP and tool use are complementary - MCP handles WHO does the work (server vs developer), both still involve tools\n\nCore value: Shifts integration burden from application developers to MCP server maintainers.\n</note>\n\n<note title=\"MCP Clients\">\nMCP Client = communication interface between your server and MCP server, provides access to server's tools\n\nTransport agnostic = client/server can communicate via multiple protocols (stdio, HTTP, WebSockets)\n\nCommon setup = client and server on same machine using standard input/output\n\nCommunication = message exchange defined by MCP spec\n\nKey message types:\n- list tools request = client asks server for available tools\n- list tools result = server responds with tool list  \n- call tool request = client asks server to run tool with arguments\n- call tool result = server responds with tool execution result\n\nTypical flow:\n1. User queries server\n2. Server requests tool list from MCP client\n3. MCP client sends list tools request to MCP server\n4. MCP server responds with list tools result\n5. Server sends query + tools to Claude\n6. Claude requests tool execution\n7. Server asks MCP client to run tool\n8. MCP client sends call tool request to MCP server\n9. MCP server executes tool (e.g. GitHub API call)\n10. Results flow back through chain: MCP server → MCP client → server → Claude → user\n\nPurpose = enables servers to delegate tool execution to specialized MCP servers while maintaining Claude integration\n</note>\n\n<note title=\"Project Setup\">\nCLI-based chatbot project = teaches MCP client-server interaction through hands-on implementation\n\nProject components:\n- MCP client = connects to custom MCP server\n- MCP server = provides 2 tools (read document, update document)\n- Document collection = fake documents stored in memory only\n\nKey distinction: Normal projects implement either client OR server, not both. This project implements both for educational purposes.\n\nSetup process:\n1. Download CLI_project.zip starter code\n2. Extract and open in code editor\n3. Follow readme.md setup directions\n4. Add API key to .env file\n5. Install dependencies (with/without UV)\n6. Run project: \"uv run main.py\" or \"python main.py\"\n7. Test with chat prompt\n\nExpected outcome = working chat interface that responds to basic queries, ready for MCP feature additions.\n</note>\n\n<note title=\"Defining Tools with MCP\">\nMCP server implementation using Python SDK creates tools through decorators rather than manual JSON schemas.\n\nMCP Python SDK = Official package that auto-generates tool JSON schemas from Python function definitions using @mcp.tool decorator.\n\nTool definition syntax = @mcp.tool(name=\"tool_name\", description=\"description\") + function with typed parameters using Field() for argument descriptions.\n\nTwo tools implemented:\n1. read_doc_contents = Takes doc_id string, returns document content from in-memory docs dictionary\n2. edit_document = Takes doc_id, old_string, new_string parameters, performs find/replace on document content\n\nError handling = Check if doc_id exists in docs dictionary, raise ValueError if not found.\n\nKey advantage = SDK eliminates manual JSON schema writing, generates schemas automatically from Python function signatures and decorators.\n\nRequired imports = Field from pydantic for parameter descriptions, mcp package for server and tool decorators.\n\nImplementation pattern = Decorator defines tool metadata, function parameters define tool arguments with types and descriptions, function body contains tool logic.\n</note>\n\n<note title=\"The Server Inspector\">\nMCP Inspector = in-browser debugger for testing MCP servers without connecting to applications\n\nAccess: Run \\`mcp dev [server_file.py]\\` in terminal → opens server on port → navigate to provided URL in browser\n\nInterface: Left sidebar has connect button → top menu shows resources/prompts/tools sections → tools section lists available tools → click tool to open right panel for manual testing\n\nTesting workflow: Connect to server → navigate to tools → select specific tool → input required parameters → click run tool → verify output\n\nKey features: Live development testing, manual tool invocation, parameter input forms, success/failure feedback, no need for full application integration\n\nNote: UI actively changing during development, core functionality remains similar\n\nExample usage: Test document tools by inputting document IDs, verify read operations, test edit operations, chain operations to verify changes\n\nPrimary benefit: Debug MCP server implementations efficiently during development phase\n</note>\n\n<note title=\"Implementing a Client\">\nMCP Client Implementation:\n\nMCP Client = wrapper class around client session for resource cleanup and connection management to MCP server\n\nClient Session = actual connection to MCP server from MCP Python SDK, requires resource cleanup on close\n\nClient Purpose = exposes MCP server functionality to rest of codebase, enables reaching out to server for tool lists and tool execution\n\nKey Functions:\n- list_tools() = await self.session.list_tools(), return result.tools\n- call_tool() = await self.session.call_tool(tool_name, tool_input)\n\nUsage Flow = client gets tool definitions to send to Claude, then executes tools when Claude requests them\n\nCommon Pattern = wrap client session in larger class for resource management rather than use session directly\n\nTesting = can run client file directly with testing harness to verify server connection and tool retrieval\n\nIntegration = other code in project calls client functions to interact with MCP server, enabling Claude to inspect/edit documents through defined tools\n</note>\n\n<note title=\"Defining Resources\">\nMCP Resources = mechanism allowing MCP servers to expose data to clients for read operations\n\nResource Types = 2 types: direct (static URI like \"docs://documents\") and templated (parameterized URI like \"docs://documents/{doc_id}\")\n\nURI = address/identifier for accessing specific resource, defined when creating resource\n\nResource Flow = client sends read resource request with URI → server matches URI to function → server executes function → returns data in read resource result\n\nImplementation = use @mcp.resource decorator with URI and MIME type parameters\n\nMIME Types = hint to client about returned data format (application/json for structured data, text/plain for plain text)\n\nTemplated Resources = URI parameters automatically parsed by SDK and passed as keyword arguments to handler function\n\nResource vs Tools = resources provide data proactively (fetch document contents when @ mentioned), tools perform actions reactively (when Claude decides to call them)\n\nData Return = SDK automatically serializes returned data to strings, client responsible for deserialization\n\nTesting = MCP inspector can list direct resources separately from templated resources, allows testing individual resource calls\n</note>\n\n<note title=\"Accessing Resources\">\nMCP Resource Access Implementation:\n\nResource Reading Function = client-side function to request and parse resources from MCP server\n\nFunction Parameters = URI (resource identifier)\n\nImplementation Steps:\n- Import json module + AnyURL from pydantic\n- Call await self.session.read_resource(AnyURL(uri))\n- Extract first element from result.contents[0]\n- Check resource.mime_type for parsing strategy\n\nContent Parsing Logic:\n- If mime_type == \"application/json\" → return json.loads(resource.text)\n- Otherwise → return resource.text (plain text)\n\nServer Response Structure = result.contents list with first element containing type/mime_type metadata\n\nResource Integration = MCP client functions called by other application components to fetch document contents for prompts\n\nEnd Result = Document contents automatically included in Claude prompts without requiring tool calls\n\nKey Point = Resources expose server information directly to clients through structured request/response pattern\n</note>\n\n<note title=\"Defining Prompts\">\nMCP Prompts = Pre-defined, tested prompt templates that MCP servers expose to client applications for specialized tasks.\n\nPurpose = Instead of users writing ad-hoc prompts, server authors create high-quality, evaluated prompts tailored to their server's domain.\n\nImplementation = Use @mcpserver.prompt decorator with name/description, define function that returns list of messages (user/assistant messages that can be sent directly to Claude).\n\nExample Use Case = Document formatting prompt that takes document ID, instructs Claude to read document using tools, reformat to markdown, and save changes.\n\nKey Benefits = Server-specific expertise, pre-tested quality, reusable across client applications, better results than user-generated prompts.\n\nMessage Structure = Returns base.UserMessage objects containing the formatted prompt text with interpolated parameters.\n\nClient Integration = Prompts appear as autocomplete options (slash commands) in client applications, prompt user for required parameters, then execute the pre-built prompt workflow.\n</note>\n\n<note title=\"Prompts in the Client\">\nMCP Client Prompt Implementation:\n\nList prompts = await self.session.list_prompts(), return result.prompts\nGet prompt = await self.session.get_prompt(prompt_name, arguments), return result.messages\n\nPrompt workflow:\n1. Define prompt in MCP server with expected arguments (e.g., document_id)\n2. Client calls get_prompt with prompt name + arguments dictionary\n3. Arguments passed as keyword arguments to prompt function\n4. Function interpolates arguments into prompt text\n5. Returns messages array for direct feeding to LLM\n\nKey concept: Prompts are server-defined templates that clients can invoke with specific arguments to generate contextualized instructions for LLMs. Arguments flow from client call → prompt function → interpolated prompt text → LLM consumption.\n</note>\n\n<note title=\"Anthropic Apps\">\nAnthropic Apps = two deployed applications by Anthropic: Claude Code and Computer Use.\n\nClaude Code = terminal-based coding assistant that serves as example of agent architecture.\n\nComputer Use = toolset that expands Claude's capabilities beyond text generation.\n\nKey purpose = these apps demonstrate agent concepts and provide practical examples for understanding agent design and implementation.\n\nSetup process = involves terminal configuration for Claude Code usage on sample projects.\n\nAgent connection = both applications exemplify how agents work, serving as learning models for building effective agents.\n</note>\n\n<note title=\"Claude Code Setup\">\nClaude Code = terminal-based coding assistant program that helps with code-related tasks\n\nCore capabilities = search/read/edit files + advanced tools (web fetching, terminal access) + MCP client support for expanded functionality via MCP servers\n\nSetup process:\n1. Install Node.js (check with \"npm help\" command)\n2. Run npm install to install Claude Code\n3. Execute \"claude\" command in terminal to login to Anthropic account\n\nFull setup guide = docs.anthropic.com\n\nMCP client functionality = can consume tools from MCP servers to extend capabilities beyond basic file operations\n</note>\n\n<note title=\"Claude Code in Action\">\nClaude Code = AI coding assistant that functions as a collaborative engineer on projects, not just a code generator.\n\nKey capabilities: project setup, feature design, code writing, testing, deployment, error fixing in production.\n\nSetup workflow:\n- Download project, open in editor\n- Run \\`claude\\` command to launch\n- Ask Claude to read README and execute setup directions\n- Run \\`init\\` command = Claude scans codebase for architecture/coding style, creates claude.md file\n- claude.md = automatically included context for future requests\n\nMemory types: Project (shared), Local, User memory files.\n\nContext management:\n- Use # symbol to add specific notes to memory\n- Can manually edit claude.md or rerun init to update\n- Claude can handle Git operations (staging, committing)\n\nEffective prompting strategies:\n\nMethod 1 - Three-step workflow:\n1. Identify relevant files, ask Claude to analyze them\n2. Describe feature, ask Claude to plan solution (no code yet)\n3. Ask Claude to implement the plan\n\nMethod 2 - Test-driven development:\n1. Provide relevant context\n2. Ask Claude to suggest tests for the feature\n3. Select and implement chosen tests\n4. Ask Claude to write code until tests pass\n\nCore principle: Claude Code = effort multiplier. More detailed instructions = significantly better results. Treat as collaborative engineer, not just code generator.\n</note>\n\n<note title=\"Enhancements with MCP Servers\">\nClaude Code = AI assistant with embedded MCP (Model Context Protocol) client that can connect to MCP servers to expand functionality.\n\nMCP Server Integration = Connect external tools/services to Claude Code via command: \\`claude mcp add [server-name] [startup-command]\\`\n\nExample Implementation = Document processing server exposing \"Document Path to Markdown\" tool, allowing Claude Code to read PDF/Word documents by running \\`uv run main.py\\`\n\nDynamic Capability Expansion = MCP servers add new functions to Claude Code in real-time without core modifications.\n\nCommon Use Cases = Production monitoring (Sentry), project management (Jira), communication (Slack), custom development workflow tools.\n\nKey Benefit = Significant flexibility increase for development workflows through modular server connections.\n\nSetup Process = 1) Create MCP server with tools, 2) Add server to Claude Code with name and startup command, 3) Restart Claude Code to access new capabilities.\n</note>\n\n<note title=\"Parallelizing Claude Code\">\nParallelizing Claude Code = running multiple Claude instances simultaneously to complete different tasks in parallel\n\nCore Problem = multiple Claude instances modifying same files simultaneously creates conflicts and invalid code\n\nSolution = Git work trees providing isolated workspaces per Claude instance\n\nGit Work Trees = feature creating complete project copies in separate directories, each corresponding to different Git branches\n\nWorkflow = create work tree → assign task to Claude instance → work in isolation → commit changes → merge back to main branch\n\nCustom Commands = automating work tree creation/management through .claude/commands directory with markdown files containing prompts\n\nCommand Structure = .claude/commands/filename.md with $ARGUMENTS placeholder for dynamic values\n\nParallel Execution Benefits = single developer commanding virtual team of software engineers, major productivity scaling limited only by engineer's management capacity\n\nMerge Conflicts = Claude automatically resolves conflicts during branch merging process\n\nCleanup = Claude handles work tree removal after feature completion\n\nKey Advantage = scales to unlimited parallel instances based on developer's capacity to manage simultaneous tasks\n</note>\n\n<note title=\"Automated Debugging\">\nAutomated Debugging = using AI (Claude) to automatically detect, analyze, and fix production errors without manual intervention.\n\nCore Workflow:\n1. GitHub Action runs daily to check production environment\n2. Fetches CloudWatch logs from last 24 hours\n3. Claude identifies errors, deduplicates them\n4. Claude analyzes each error and generates fixes\n5. Creates pull request with proposed solutions\n\nKey Components:\n- GitHub Actions for scheduling/automation\n- AWS CLI for log retrieval\n- Claude Code for error analysis and code fixes\n- CloudWatch for production error monitoring\n\nBenefits:\n- Catches production-only errors (issues not present in development)\n- Reduces manual log hunting and debugging time\n- Provides context-aware fixes with explanations\n- Creates reviewable pull requests for changes\n\nCommon Use Case: Configuration errors between environments (invalid model IDs, API keys, etc. that work locally but fail in production)\n\nImplementation Requirements: Repository access, cloud logging service, AI coding assistant, CI/CD pipeline integration.\n</note>\n\n<note title=\"Computer Use\">\nComputer Use = Claude's ability to interact with computer interfaces through visual observation and control actions.\n\nKey capabilities:\n- Takes screenshots of applications/browsers\n- Clicks buttons, types text, navigates interfaces\n- Follows multi-step instructions autonomously\n- Performs QA testing and automation tasks\n\nHow it works:\n- Runs in isolated Docker container environment\n- User provides instructions via chat interface\n- Claude observes screen visually and executes actions\n- Generates reports on task completion/results\n\nPrimary use cases:\n- Automated QA testing of web applications\n- UI interaction testing across different scenarios\n- Time-saving for repetitive computer tasks\n- Bug identification through systematic testing\n\nSetup requirement = Reference implementation available for local testing\n\nExample workflow: User describes testing requirements → Claude navigates to application → Executes test cases → Reports pass/fail results with detailed findings\n</note>\n\n<note title=\"How Computer Use Works\">\nComputer use = tool system implementation allowing Claude to interact with computing environments\n\nTool use flow: User sends message + tool schema → Claude responds with tool use request (ID, name, input) → Server executes code → Result sent back to Claude as tool result\n\nComputer use follows identical flow:\n- Special tool schema sent to Claude (small schema expands to larger structure behind scenes)\n- Expanded schema includes action function with arguments: mouse move, left click, screenshot, etc.\n- Claude sends tool use request\n- Developers must fulfill request via computing environment (typically Docker container)\n- Container executes programmatic key presses/mouse movements\n- Response sent back to Claude\n\nKey points:\n- Claude doesn't directly manipulate computers\n- Computer use = tool system + developer-provided computing environment\n- Anthropic provides reference implementation (Docker container with pre-built mouse/keyboard execution code)\n- Setup requires Docker + simple command execution\n- Enables direct chat interface for testing Claude's computer use functionality\n\nComputer use = abstraction layer where tool system handles Claude communication while Docker container handles actual computer interactions.\n</note>\n\n<note title=\"Agents and Workflows\">\nWorkflows and agents = strategies for handling user tasks that can't be completed by Claude in a single request.\n\nDecision rule: Use workflows when you have precise task understanding and know exact steps sequence. Use agents when task details are unclear.\n\nWorkflow = series of calls to Claude for specific problems where steps are predetermined.\n\nExample workflow: Image to 3D model converter\n- Step 1: Claude describes uploaded image in detail\n- Step 2: Claude uses CADQuery Python library to model object from description\n- Step 3: Create rendering of model\n- Step 4: Claude compares rendering to original image\n- Step 5: If inaccurate, repeat from step 2 with feedback\n\nThis follows evaluator-optimizer pattern:\n- Producer = generates output (Claude + CADQuery modeling)\n- Evaluator = assesses output quality (comparison step)\n- Loop continues until evaluator accepts output\n\nKey point: Workflows are implementation patterns that other engineers have successfully used. Identifying workflow patterns doesn't automatically implement them - you still need to write the actual code.\n</note>\n\n<note title=\"Parallelization Workflows\">\nParallelization Workflows = breaking one complex task into multiple simultaneous subtasks, then aggregating results.\n\nExample: Material selection for parts\n- Instead of: One large prompt asking Claude to choose between metal/polymer/ceramic/composite with all criteria\n- Use: Separate parallel requests, each evaluating one material's suitability, then final aggregation step to compare results\n\nStructure: Input → Multiple parallel subtasks → Aggregator → Final output\n\nBenefits:\n- Focus = Each subtask handles one specific analysis instead of juggling multiple considerations\n- Modularity = Individual prompts can be improved/evaluated separately  \n- Scalability = Easy to add new subtasks without affecting existing ones\n- Quality = Reduces confusion from overly complex single prompts\n\nKey principle: Decompose complex decisions into specialized parallel analyses, then synthesize results.\n</note>\n\n<note title=\"Chaining Workflows\">\nChaining Workflows = breaking large tasks into series of distinct sequential steps rather than single complex prompt\n\nCore concept: Instead of one massive prompt with multiple requirements, split into separate calls where each focuses on one specific subtask.\n\nExample workflow: User enters topic → search trending topics → Claude selects most interesting → Claude researches topic → Claude writes script → generate video → post to social media\n\nKey benefit: Allows AI to focus on individual tasks rather than juggling multiple constraints simultaneously\n\nPrimary use case: When Claude consistently ignores constraints in complex prompts despite repetition. Common with long prompts containing many \"don't do X\" requirements.\n\nProblem scenario: Long prompt with constraints (don't mention AI, no emojis, professional tone) → Claude violates some constraints regardless of repetition\n\nSolution: Step 1 - Send initial prompt, accept imperfect output. Step 2 - Follow-up prompt asking Claude to rewrite based on specific violations found.\n\nCritical insight: Even simple-seeming workflow becomes essential when dealing with constraint-heavy prompts that AI struggles to follow completely in single pass.\n</note>\n\n<note title=\"Routing Workflows\">\nRouting Workflows = workflow pattern that categorizes user input to determine appropriate processing pipeline\n\nKey mechanism: Initial request to Claude categorizes user input into predefined genres/categories. Based on categorization response, system routes to specialized processing pipeline with customized prompts/tools.\n\nExample flow:\n1. User enters topic (e.g., \"Python functions\")\n2. Claude categorizes topic (e.g., \"educational\")\n3. System uses educational-specific prompt template\n4. Claude generates script with educational tone/structure\n\nBenefits: Ensures output matches topic nature. Programming topics get educational treatment with definitions/explanations. Entertainment topics get trendy language/engaging hooks.\n\nStructure: One routing step → Multiple specialized processing pipelines → Each pipeline has customized prompts/tools for specific category\n\nUse case: Social media video script generation where different topics require different tones and approaches.\n</note>\n\n<note title=\"Agents and Tools\">\nAgents = AI systems that create plans to complete tasks using provided tools, effective when exact steps are unknown. Workflows = better when precise steps are known.\n\nKey differences: Workflows require predetermined steps, agents dynamically plan using available tools.\n\nAgent advantages: Flexibility to solve variety of tasks with same toolset, can combine tools in unexpected ways.\n\nTool abstraction principle: Provide generic/abstract tools rather than hyper-specialized ones. Example - Claude code uses bash, web_fetch, file_write (abstract) rather than refactor_tool, install_dependencies (specialized).\n\nTool combination examples: get_current_datetime + add_duration + set_reminder can solve various time-related tasks through different combinations.\n\nAgent behavior: Can request additional information when needed, combines tools creatively to achieve goals, works best with small set of flexible tools.\n\nDesign approach: Give agent abstract tools that can be pieced together rather than single-purpose specialized tools. This enables dynamic problem-solving and unexpected use cases.\n</note>\n\n<note title=\"Environment Inspection\">\nEnvironment Inspection = agents evaluating their environment and action results to understand progress and handle errors.\n\nCore concept: After each action, agents need feedback mechanisms beyond basic tool returns to understand new environment state.\n\nComputer use example: Claude takes screenshot after every action (typing, clicking) to see how environment changed, since it cannot predict exact results of actions like button clicks.\n\nCode editing example: Before modifying files, agents must read current file contents to understand existing state.\n\nSocial media video agent applications:\n- Use Whisper CPP via bash to generate timestamped captions, verify dialogue placement\n- Use FFmpeg to extract video screenshots at intervals, inspect visual results\n- Validate video creation meets expectations before posting\n\nKey benefit: Environment inspection enables agents to gauge task progress, detect errors, and adapt to unexpected results rather than operating blindly.\n</note>\n\n<note title=\"Workflows vs Agents\">\nWorkflows = pre-defined series of calls to Claude with known exact steps. Agents = flexible approach using basic tools that Claude combines to complete unknown tasks.\n\nKey differences:\n\nTask division: Workflows break big tasks into smaller, specific subtasks enabling higher focus and accuracy. Agents handle varied challenges creatively without predetermined steps.\n\nTesting/evaluation: Workflows easier to test due to known execution sequence. Agents harder to test since execution path unpredictable.\n\nUser experience: Workflows require specific inputs. Agents create own inputs from user queries and can request additional input when needed.\n\nSuccess rates: Workflows = higher task completion rates due to structured approach. Agents = lower completion rates due to delegated complexity.\n\nRecommendation: Prioritize workflows for reliability. Use agents only when flexibility truly required. Users want 100% working products over fancy agents.\n\nCore principle: Solve problems reliably first, innovation second.\n</note>\n</notes>",
  "aif4ed": "<notes>\n<critical>\nBelow are notes from a video course about working with the Claude language model.\nUse these notes as a resource to answer the user's question.\nWrite your answer as a standalone response - do not refer directly to these notes unless specifically requested by the user.\n</critical>\n<note title=\"Lesson 2: Applying AI Fluency to Course Design and Learning Outcomes\">\n**Lesson 2: Applying AI Fluency to Course Design and Learning Outcomes**\n\n**Core Concept**\nAI Fluency transforms course design by building rich, shared context that turns AI from generic assistant into collaborative partner who understands specific teaching situation, constraints, and pedagogical vision.\n\n**Three Fundamental Course Design Tasks**\n1. Identifying content and concepts essential for students\n2. Mapping learning journey - how topics build on each other\n3. Articulating specific learning objectives\n\n**4Ds Applied to Course Design**\n\n**Delegation in Course Design**\n- Make smart delegation decisions based on subject matter expertise and goals\n- Determine what work needs to be done (content selection, sequencing, objectives)\n- Decide who does what (human, AI, or both) and how\n- Answer What and Why before determining Who and How\n\n**Description for Course Design**\n- Create thinking space rooted in actual pedagogical challenges\n- Example: \"I'm building stats course for journalism students who need data literacy but often come anxious about math or skeptical statistics matter for careers\"\n- Share specific challenges, student needs, prerequisites, outcomes\n- Explain vision for success and constraints\n- Use both product description (what you want AI to do) and process description (how)\n\n**Product Discernment in Course Design**\n- Evaluate AI suggestions against own expertise\n- Question: Does this concept actually help target students?\n- Don't just accept/reject - explain why something works/doesn't in context\n- Use expertise to shape better future suggestions\n- Recognize unexpected insights and new thinking approaches\n\n**Diligence in Course Design**\n- Don't outsource decisions - take responsibility for what reaches students\n- Document key decisions in AI collaboration\n- Share thinking with students to model responsible AI use\n- Quality assurance and transparency\n- Maintain accountability while demonstrating how AI enhances teaching expertise\n\n**AI Role-Playing Technique**\nAsk AI to \"act as one of my students\" to evaluate course flow:\n- How will students react at different points?\n- Where might confusion arise?\n- What background needed at each stage?\n- What support needed for topic transitions?\n\n**Learning Objectives Enhancement**\n- Use established context from previous collaboration\n- Upload course syllabus with richer context description\n- Craft objectives that are: accurate, meaningful, motivating, true to teaching philosophy, aligned with institutional requirements\n- Map course outcomes to resume skills for student motivation\n- Experiment with different approaches due to faster process\n\n**Benefits Beyond Time-Saving**\n- Surface hidden assumptions and blindspots\n- Discover creative connections between concepts\n- Maintain coherence while exploring new ideas\n- Build personal competencies that strengthen with practice\n\n**Key Principles**\n- 4Ds = framework for genuine cognitive partnership, not checklist\n- Pedagogical and subject expertise guides collaborative exploration\n- Context-building approach works for: new courses, course updates, curriculum sequences, learning pathways\n- Start with big picture, build/maintain context through details\n- Goal = enhancement not just efficiency - better teaching/learning, not just faster planning\n\n**Exercise Structure (50 minutes)**\nStage 1: Planning approach (clarify course, work needed, expertise vs AI help)\nStage 2: Initiating collaboration (share teaching context, explain challenges, set partnership)\nStage 3: Build conversation through Description & Discernment (content identification, journey mapping, learning objectives)\nStage 4: Documentation & reflection (verify accuracy, document decisions, note rejections, create AI role statement)\n\n**Outcome**\nCreates genuine cognitive partnership where human judgment, values, and student relationships remain central while AI enhances pedagogical practice.\n</note>\n\n<note title=\"Lesson 1: Introduction to AI Fluency for Educators\">\nAI Fluency for Educators = framework for effective, efficient, ethical, safe AI integration in teaching practice\n\nCourse Purpose = applying 4D Framework (Delegation, Description, Discernment, Diligence) specifically to education contexts\n\nWhy AI Fluency Matters in Education:\n- Students already using AI\n- Employers expect AI fluency from graduates  \n- Institutions need responsible AI integration strategies\n- Educators shape next generation's AI engagement\n\nCourse Outcomes:\n- Apply 4D Framework to teaching practice\n- Use AI for course design and lesson planning\n- Create materials maintaining teaching vision and academic integrity\n- Lead institutional AI conversations\n- Model responsible AI engagement for students\n\nPrerequisites = completion of AI Fluency: Framework & Foundations course\n\n4D FRAMEWORK SUMMARY:\n\nAI Fluency = responsible human in the loop, augmentation not automation\n\nDelegation = determining who does what work (human vs AI vs collaborative)\nThree types:\n- Problem Awareness = clarity on goals and success metrics before AI engagement\n- Platform Awareness = understanding different AI capabilities and limitations  \n- Task Delegation = dividing work to leverage human creativity/judgment + AI speed/processing\n\nDescription = how to communicate with AI (conversation not commands)\nThree dimensions:\n- Product Description = detailed specifications for final output (length, audience, style, format)\n- Process Description = guiding AI's approach (step-by-step thinking, multiple perspectives)\n- Performance Description = defining AI behavior during interaction (critical editor, supportive brainstormer)\n\nDiscernment = evaluating AI outputs and adjusting accordingly\nThree levels:\n- Product Discernment = quality assessment of AI creations (accuracy, helpfulness, new perspectives)\n- Process Discernment = examining AI reasoning and assumptions\n- Performance Discernment = evaluating AI behavior during interaction\n\nDescription + Discernment = continuous feedback loop transforming AI from tool to thinking partner\n\nDiligence = responsible AI use throughout collaboration\nThree aspects:\n- Creation Diligence = thoughtful AI system selection considering privacy, security, appropriateness\n- Transparency Diligence = honest disclosure of AI assistance\n- Deployment Diligence = taking ownership and responsibility for AI-assisted outputs\n\nKey Principles:\n- Augmentation > Automation (especially in learning contexts)\n- Pedagogical values remain central\n- 4Ds work together as interconnected competencies\n- Skills remain relevant as AI technology evolves\n\nTeaching Context Document = reusable artifact establishing shared understanding with AI for future collaborations\n\nEssential Context Areas:\n- Subject area, level, specific courses\n- Student backgrounds, challenges, goals\n- Institutional constraints\n- Teaching philosophy and methods\n- Common teaching pain points\n- AI integration goals\n- Unique teaching situation aspects\n\nContext Document Benefits:\n- Improves AI collaboration effectiveness\n- Saves time in future interactions\n- Maintains consistency across AI conversations\n- Helps AI understand educator's specific needs and constraints\n</note>\n\n<note title=\"Lesson 3: Applying AI Fluency to Learning Materials and Assignments\">\n**LESSON 3: APPLYING AI FLUENCY TO LEARNING MATERIALS AND ASSIGNMENTS**\n\n**CORE LEARNING OBJECTIVES**\n- Build teaching materials with AI using 4D framework\n- Leverage established context for coherent material development  \n- Apply systematic quality control through Discernment\n- Create integrated learning experiences\n\n**AI'S DISCIPLINARY IMPACT FRAMEWORK**\n\n**Three Critical Questions for Disciplines:**\n1. What gets automated = routine tasks likely automated by AI in future careers\n2. Partnership potential = where human-AI collaboration most impactful\n3. Who's in charge = how humans manage/remain accountable for AI work\n\n**Three Types of Existing Expertise:**\n- Disciplinary Expertise = field content, values, methods, thinking patterns\n- Pedagogical Expertise = student struggle points, breakthrough moments, mastery development\n- Assessment Expertise = recognizing genuine understanding, designing evaluations\n\n**AI DISRUPTION ACROSS THREE DOMAINS**\n\n**Curriculum Impact:**\n- Fundamentals remain but students question necessity\n- Need compelling discipline-specific answers for \"why learn this when AI can do it\"\n- Tasks: Identify what AI can automate, determine which concepts become more/less important, assess how AI changes best practices\n\n**Pedagogical Impact:**\n- AI enables personalized tutoring, interactive simulations, immediate feedback\n- Challenge: distinguishing AI that enhances vs shortcuts learning\n- Tasks: Define how AI enhances/inhibits learning, identify best teacher-student-AI collaborations, determine effective pedagogical approaches\n\n**Assessment Impact:**\n- Most urgent disruption = when students generate content in seconds, what are we assessing\n- Need authentic assessment of both AI-assisted and human-only work\n- Tasks: Identify authentic demonstrations of AI-assisted learning, create shortcut-proof assignments, value process over products, adapt to inevitable student AI use\n\n**4Ds APPLIED TO DISCIPLINARY EXPERTISE**\n\n**Discernment = Quality Evaluation**\n- Build Quality Criteria = detailed rubrics beyond vague terms, specific excellence markers\n- Collect Outstanding Work = systematic analysis with students, make expert thinking visible\n- Diagnose Failures = forensic examination of flawed examples, understand failure modes\n\n**Description = Communication Mastery**  \n- Map Discipline Products = document key outputs with precision, reveal underlying logic\n- Reveal Expert Thinking = make problem-solving processes visible, trace micro-decisions\n- Name Norms = surface field-specific behaviors, define \"thinking like X\" in concrete terms\n\n**Delegation = Work Decomposition**\n- Reveal Problem Anatomy = break challenges into components, map AI possibilities\n- Map AI Possibilities = what can be automated/augmented/delegated to AI agents\n- Design Decision Trees = frameworks for when/how to involve AI, case-study based\n\n**Diligence = Ethical Standards**\n- Codify Ethical Frameworks = field-specific \"do no harm,\" create decision matrices\n- Clarify Transparency Norms = disclosure expectations, citation templates for AI assistance  \n- Co-Create Accountability Policies = build standards with students, develop peer review protocols\n\n**MATERIAL CREATION WITH AI**\n\n**Context Utilization:**\n- Established context = each new workflow better than starting fresh\n- Reference previous conversations and course design work\n- Share video transcripts and course context with AI\n\n**4Ds in Material Creation:**\n- Delegation = understand what materials needed and why\n- Description = leverage established context + specific requirements\n- Description-Discernment Loop = refinement tool, explain why suggestions work/don't work\n- Diligence = protect sensitive data, verify accuracy, check bias, create transparency\n\n**Four Material Creation Options:**\n\n**Option 1: Lecture Slide Deck**\n- Setup: Share course context, presentation style, classroom dynamics, visual preferences\n- Process: Initial outline → evaluate flow/complexity → develop crucial slides → full deck\n- Final check: Verify accuracy, note image/citation needs, document AI contribution\n\n**Option 2: Study/Revision Guide**  \n- Setup: Reference lecture topic, describe student study habits, explain effective aids\n- Process: Identify key concepts → develop self-check questions → anticipate misconceptions → create connections\n- Final check: Verify answer accuracy, check alignment with assessments, ensure accessibility\n\n**Option 3: Interactive In-Class Exercise**\n- Setup: Describe classroom setup, time constraints, student interaction patterns, learning goals\n- Process: Collaborate on activity → think through logistics → anticipate responses → create facilitation notes\n- Final check: Test instruction clarity, consider accessibility, plan contingencies\n\n**Option 4: Knowledge Check Quiz**\n- Setup: Reference learning objectives, assessment level, academic integrity approach\n- Process: Develop 5-10 varied questions → mix question types → connect to objectives → create answer explanations  \n- Final check: Verify answer correctness, check for bias/confusion, ensure appropriate difficulty\n\n**KEY PRINCIPLES**\n- AI Fluency = amplifying human expertise, not replacing it\n- Disciplinary knowledge = foundation for unprecedented achievement  \n- Human capabilities = domain expertise, judgment, creativity, ethics, relationships\n- Students prepared to be irreplaceable, not replaced\n- Focus on coherence and quality, not just efficiency\n- Description-Discernment loop = powerful refinement through meaningful iteration\n</note>\n\n</notes>",
  "aif4students": "<notes>\n<critical>\nBelow are notes from a video course about working with the Claude language model.\nUse these notes as a resource to answer the user's question.\nWrite your answer as a standalone response - do not refer directly to these notes unless specifically requested by the user.\n</critical>\n<note title=\"Lesson 1: Introduction to AI Fluency & The 4D Framework\">\nAI Fluency = using AI effectively, efficiently, ethically, safely through enduring principles rather than temporary tips/tools\n\n4D Framework = four interconnected competencies (Delegation, Description, Discernment, Diligence) that create AI fluency\n\nAugmentation vs Automation:\n- Augmentation = AI helps you do your work better (preferred for learning)\n- Automation = AI does work for you without deep thought (problematic for learning)\n\nHuman in the Loop = making responsible decisions and using AI thoughtfully while maintaining ownership\n\nDELEGATION = determining what work should be done by humans, AI, or together\n\nThree types of delegation awareness:\n- Problem Awareness = clarifying actual goals/objectives before using AI\n- Platform Awareness = understanding different AI capabilities/limitations to choose right tool\n- Task Delegation = dividing work to leverage human strengths (creativity, judgment, context) and AI strengths (speed, consistency, information processing)\n\nDESCRIPTION = how you communicate with AI as collaborative conversation, not just prompting\n\nThree dimensions of description:\n- Product Description = specifying final result details (length, key messages, audience, style, format)\n- Process Description = guiding AI's approach (\"think step-by-step,\" \"consider multiple perspectives\")\n- Performance Description = defining AI's behavior during interaction (critical editor, supportive brainstormer)\n\nDISCERNMENT = recognizing good/bad results and adapting accordingly through critical evaluation\n\nThree levels of discernment:\n- Product Discernment = evaluating accuracy, helpfulness, new perspectives in AI outputs\n- Process Discernment = examining AI's reasoning, assumptions, logical steps\n- Performance Discernment = assessing whether AI behaved helpfully in requested role\n\nDescription-Discernment Loop = continuous feedback cycle where you describe needs, evaluate results, then refine descriptions iteratively\n\nDILIGENCE = taking responsibility for AI use through ethical, transparent, accountable practices\n\nThree types of diligence:\n- Creation Diligence = thoughtfully selecting AI systems considering privacy, security, appropriateness\n- Transparency Diligence = honestly reporting AI collaboration to professors, teammates, employers\n- Deployment Diligence = taking ownership of AI-assisted outputs, verifying accuracy, ensuring appropriateness\n\nLearning Context Document = personalized reference document establishing academic context, learning goals, AI boundaries, and collaboration preferences for future AI sessions\n\nKey Elements for Learning Context:\n- Academic context (major, courses, strengths/weaknesses, goals)\n- Learning style and challenges\n- AI experience and boundaries\n- Preferred support types (guidance vs task completion)\n- Academic integrity considerations\n\nEssential Principle = AI fluency skills remain relevant regardless of technological evolution because they focus on strategic thinking, communication, evaluation, and ethical responsibility rather than specific tools\n</note>\n\n<note title=\"Lesson 3: AI in Career Planning\">\nAI in Career Planning = using AI as thinking partner for career exploration, skill building, job searching while maintaining authenticity\n\nCore Principle = AI handles information gathering/coaching, human drives strategy/decisions/values\n\nCareer Exploration Process:\n- AI strengths = industry research, trend analysis, skill gap identification, salary insights, trajectory mapping, brainstorming roles\n- Human responsibility = self-knowledge, values, trade-offs, life goals, gut instincts\n- Two-part process = information gathering (AI) + personal reflection (human)\n\nAI Career Exploration Tasks:\n- Industry research = \"What are emerging roles in sustainable tech combining environmental science with data analysis?\"\n- Skill gap analysis = \"UX designer skills employers want vs bootcamp teaching\"\n- Market insights = \"Real salary progression for entry-level consultants by city\"\n- Career trajectories = \"Transition paths from teaching to corporate training\"\n- Role brainstorming = \"Careers using journalism skills with better stability\"\n- Values clarification = AI asks questions to surface career goals/values\n\nSkill Development with AI:\n- Specific learning needs = replace \"help me learn data analysis\" with \"marketing manager needing customer survey analysis skills starting with Excel pivot tables\"\n- AI as coach = create practice scenarios, provide feedback on samples, roleplay situations\n- Active learning check = regularly assess if actually improving or just going through motions\n\nJob Search Applications:\n- Rule of thumb = AI handles brainstorming/coaching/drafts, human presents authentic self\n- AI tasks = identify transferable skills, practice interview questions, find unique cover letter angles\n- Human responsibility = professional expertise, career goals, authentic identity, final materials\n\nGeneric AI Content Problem = recruiters immediately recognize AI-generated resumes/cover letters as ineffective\n\nCareer Development Workflow:\nPart 1 - Career Exploration (15 min):\n- Share background with AI\n- Research careers using similar skills, emerging roles, salary progressions, day-to-day realities\n- AI-guided self-reflection on energy sources, trade-offs, values, success definition\n- Create career map with 2-3 target roles and skill gaps\n\nPart 2 - CV/Resume Improvement (15 min):\n- Step 1: AI critiques current CV against specific job spec\n- Step 2: AI gathers information to address improvement areas\n- Step 3: Revise CV incorporating actual experiences with stronger language\n\nPart 3 - Interview Preparation (15 min):\n- AI analyzes role for 5 realistic questions\n- One question at time with follow-ups\n- Practice with specific examples\n- AI provides feedback on content/clarity/relevance\n\nKey Guidelines:\n- AI helps practice/prepare, human must be authentic in actual situations\n- Don't memorize scripts, understand experiences for flexible discussion\n- Employers want to hire you, not AI version of you\n- AI fluency itself = valuable career skill to highlight\n- Track AI interactions, be transparent about AI assistance when appropriate\n\nAuthentication Principle = AI can help tell your story better but story starts and ends with your expertise\n</note>\n\n<note title=\"Lesson 4: Being the Human in the Loop\">\n**LESSON 4: BEING THE HUMAN IN THE LOOP**\n\n**Core Concept**\nHuman in the Loop = Human maintains decision-making control in all AI interactions. You decide what to ask, evaluate AI outputs for accuracy/sense, determine if output is usable. Bring judgment, creativity, ethics to every interaction.\n\n**Key Principles**\n- AI fluency = collaborating with AI safely, efficiently, effectively, responsibly\n- Leaders of tomorrow = those who partner with AI systems thoughtfully\n- Your humanity = uniquely valuable (talents, experiences, knowledge, perspectives)\n- AI enhances human capabilities, cannot replace human insight/judgment/care\n\n**Essential Components of AI Fluency**\n- Delegate thoughtfully\n- Describe needs clearly  \n- Think critically about outputs\n- Follow through with care\n\n**Personal AI Collaboration Policy Structure**\n\n**Section 1: Usage Boundaries**\n- Define when AI is appropriate (practice, exploration, building understanding)\n- Define when to work independently (exams, demonstrating mastery, original thinking)\n- Identify professional contexts where AI adds value\n- Establish \"red lines\" = situations where you absolutely will not use AI\n\n**Section 2: Learning Applications**\n- AI as tutor/coach, not homework completer\n- Maintain ability to explain everything you submit\n- Use AI to enhance understanding, not bypass it\n- Document specific strategies that work\n\n**Section 3: Transparency Standards**\n- When to proactively disclose AI assistance\n- How to document AI contributions\n- Level of detail to provide\n- Handling ambiguous situations\n\n**Section 4: Skill Maintenance**\n- Core skills to practice without AI\n- Regular \"AI-free\" work intervals\n- Methods to verify continued learning\n- Warning signs of over-reliance (and under-utilization)\n\n**Section 5: Ethical Guidelines**\n- Accuracy and fact-checking responsibilities\n- Respect for others' work and ideas\n- Privacy and data considerations\n- Broader impacts of AI/computer use\n\n**Section 6: Continuous Growth**\n- Staying updated on AI capabilities\n- Regular reflection intervals\n- Policy adjustment process\n- Knowledge sharing with others\n\n**Critical Reflection Questions**\n- Is AI engagement helping me grow?\n- Am I developing skills needed for my future or just completing tasks?\n- Am I being honest about AI's role in my work?\n- Am I using AI in ways that align with my values?\n\n**Implementation Guidelines**\n- Keep policy realistic given context/constraints\n- Make it concise enough to actually reference\n- Include creation date and update plans\n- Save in multiple accessible places\n- Practice applying to real scenarios\n- Share and discuss with others for accountability\n\n**Course Completion Elements**\n- 4D Framework for thoughtful AI interactions\n- AI-augmented learning approach that enhances rather than replaces talent\n- AI-augmented career thinking maintaining authenticity\n- Personal AI policy reflecting individual values\n- Foundation for AI leadership role\n\n**Next Steps**\n- Implement policy immediately\n- Share learnings with peers\n- Add AI Fluency to resume with evidence\n- Stay curious about AI developments\n- Help shape positive AI use in community\n</note>\n\n<note title=\"Lesson 2: AI as a Learning Partner\">\n**AI Learning Partner Framework**\n\nCore Distinction = Using AI to do work vs using AI to learn\n- AI doing work = borrowed intelligence, weakens critical thinking\n- AI as learning partner = builds knowledge, enhances capability\n\n**Consequences of AI Dependency**\n- Exam failure without AI support\n- Job interview difficulties explaining thinking\n- Inability to evaluate AI advice quality\n- Real-world problem solving deficits\n\n**4Ds for Learning Context**\n\n**Delegation (Learning Focus)**\nProblem Awareness = Ask before using AI: What am I trying to learn? What's my learning goal?\nTask Allocation = Keep learning-critical tasks (writing arguments), delegate support tasks (debate partner role)\nPlatform Selection = Choose AI systems designed for learning vs quick answers\n\n**Description (Learning Configuration)**\nDefault AI Behavior = Designed to give direct answers (unhelpful for learning)\nLearning Prompts = Configure AI as tutor/coach, not answer-giver\nEffective Descriptions:\n- \"Ask me questions instead of explaining\"\n- \"Point me toward solution without solving\"\n- \"Help me dig deeper into my interpretation\"\n\n**Useful AI Learning Outputs:**\n- Pointers/clues for difficult topics\n- Comprehension testing questions\n- Critical feedback from different perspectives\n- Concept re-explanation with new examples\n- Progressive practice problems\n- Act as student for you to teach\n\n**Discernment (Learning Evaluation)**\nLearning Check Questions:\n- Can I explain this to someone else?\n- Can I solve similar problems without AI?\n- Do I understand why, not just that it works?\n\nGenuine Understanding vs Following Along = Trust internal sense of comprehension\nInformation Verification = Cross-check AI learning content with non-AI sources\n\n**Diligence (Academic Integrity)**\nCreation Diligence = Follow school AI policies, use approved systems\nTransparency Diligence = Document AI interactions, develop disclosure skills\nDeployment Diligence = Must explain every part of submitted work, apply concepts to new situations\n\n**Exercise Structure**\n\n**Part 1: AI Learning Partner Setup (15-30 min)**\nConfiguration Steps:\n- Share learning context from Lesson 1\n- Share video transcripts for AI understanding\n- Specify learning vs assignment completion goals\n- Define questioning vs direct answer approach\n\nStudy Protocols by Need:\n- Problem-solving = hints/questions only\n- Concept review = progressive difficulty testing\n- Exam prep = quiz with explanation of wrong answers\n- Writing = argument development through questioning\n- Reading = explain concepts in own words\n- Assignment planning = test understanding of requirements\n\n**Part 2: Living Learning Journal (15-30 min)**\nJournal Structure:\n- Weekly concept work summary\n- Understanding clarity assessment\n- AI learning usage documentation\n- Effective study strategy identification\n- Practice needs identification\n- Specific skill development notes\n\nTracking System:\n- Weekly frequency recommended\n- Monthly synthesis entries for patterns\n- AI vs independent work notation\n- \"Aha moment\" documentation\n- Accountability through reflection prompts\n\n**Key Outcomes**\nLong-term Benefits = Real understanding, problem-solving skills, interview confidence, AI advice evaluation ability, workplace value\nInvestment Mindset = Building thinking/learning/adaptation capabilities beyond school completion\nCompetitive Advantage = Human intelligence/creativity/judgment skills in AI-accessible world\n</note>\n\n</notes>",
  "bedrock": "<notes>\n<critical>\nBelow are notes from a video course about working with the Claude language model.\nUse these notes as a resource to answer the user's question.\nWrite your answer as a standalone response - do not refer directly to these notes unless specifically requested by the user.\n</critical>\n<note title=\"Accessing the API\">\nAWS Bedrock API Access:\n\nChat bot flow = User submits text → Server receives request → Bedrock client makes API call to AWS Bedrock → Request includes user message + model ID → Model generates text → Returns assistant message → Server sends response to browser\n\nKey API components = User message (input text), Model ID (specifies which model), Assistant message (generated output)\n\nCourse focus = Communication between bedrock client and bedrock service, API requests, text access, design patterns\n\nCritical distinction = AWS Bedrock API ≠ Anthropic API. Same Claude models (Sonnet, Haiku) but different services, different SDKs, different documentation. Must use AWS Bedrock-specific resources.\n\nClaude models = Hosted on both AWS Bedrock and official Anthropic API, but accessing through AWS Bedrock in this context.\n</note>\n\n<note title=\"Making a Request\">\nAWS Bedrock API Request Basics:\n\nThree requirements for API requests:\n1. Boto3 client = connects to Bedrock runtime service (specify region like US West 2)\n2. Model ID = identifier for specific model to run\n3. User message = specially formatted object containing input text\n\nModel ID complexity:\n- Not all models hosted in every region\n- Wrong region = cryptic error messages\n- Inference profiles = route requests to regions where model actually exists\n- Use inference profile ID instead of direct model ID for guaranteed routing\n- Found in AWS console under \"cross region inference\" not \"model catalog\"\n\nMessage structure:\n- User message = {\"role\": \"user\", \"content\": [{\"text\": \"your input\"}]}\n- Assistant message = response from model with same structure but role=\"assistant\"\n- Content is list because messages can contain multiple parts (text + images)\n\nMaking request:\n- Use client.converse(modelId=\"profile_id\", messages=[user_message])\n- Access response text via: response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n\nMessage types:\n- User message = input to model\n- Assistant message = model-generated response\n</note>\n\n<note title=\"Multi-Turn Conversations\">\nMulti-Turn Conversations = conversations with multiple back-and-forth exchanges that maintain context\n\nKey constraint: Bedrock/Claude APIs = stateless, store no messages or responses\n\nRequirements for multi-turn conversations:\n1. Manual message management = maintain complete list of all exchanged messages in your code\n2. Full context transmission = send entire message history with every follow-up request\n\nMessage structure rules:\n- Messages = list of alternating user/assistant roles\n- Never have consecutive messages with same role\n- Pattern must be: user → assistant → user → assistant\n\nImplementation approach:\n- Create helper functions to add user/assistant messages to list\n- Include previous messages + responses in each new API call\n- Build conversation history incrementally\n\nExample flow:\n1. Send \"what's 1+1?\" → get \"2\"\n2. Add both to message list\n3. Send full list + \"and three more\" → get \"5\" (with proper context)\n\nWithout message history: follow-up questions produce nonsensical responses because model lacks context.\n</note>\n\n<note title=\"System Prompts\">\nSystem Prompts = instructions that assign a role to language models to guide their responses\n\nImplementation = pass system prompt to converse function using system keyword parameter\n\nPurpose = control response behavior by making model \"pretend\" to have specific job/role instead of listing explicit do/don't requirements\n\nExample = \"You are an AWS cloud support specialist\" makes model respond like real support specialist, automatically avoiding competitor mentions and refusing non-cloud questions\n\nTechnical Requirements = system prompt must have at least 1 character, passed as list with dictionary containing text field\n\nBenefits = more natural constraint enforcement than explicit rule lists, role-based responses automatically handle edge cases\n\nBest Practice = make system prompt parameter optional in chat functions for reusability, defaults to none to avoid empty string errors\n</note>\n\n<note title=\"Temperature\">\n**Temperature = decimal parameter (0-1) that controls randomness in Claude's token selection**\n\n**Text Generation Process:**\n1. Tokenization - breaks input into chunks\n2. Prediction - assigns probabilities to possible next tokens  \n3. Sampling - selects token based on probabilities\n\n**Temperature Effects:**\n- Temperature 0 = deterministic output, always selects highest probability token\n- Higher temperature = increases chance of selecting lower probability tokens\n- Default temperature = 1.0 (maximum creativity)\n\n**Usage Guidelines:**\n- Low temperature (near 0) = data extraction, factual tasks, consistent output\n- High temperature (near 1) = creative writing, brainstorming, jokes, marketing\n\n**Implementation:**\n- Pass temperature parameter in inference_config when calling converse function\n- Lower temperature produces more similar/predictable responses\n- Higher temperature produces more varied/creative responses\n\n**Key Takeaway:** Temperature directly controls creativity vs consistency tradeoff in model outputs.\n</note>\n\n<note title=\"Streaming\">\n**Streaming in Chat Interfaces**\n\nProblem = User expectations for immediate feedback vs actual response time (3-30 seconds) when generating text responses.\n\nSolution = Converse Stream function - streams generated text as it's being produced by model instead of waiting for complete response.\n\n**How Streaming Works:**\n1. User submits question → server sends request to bedrock via converse stream\n2. Immediate initial response received (no text, just acknowledgment)\n3. Stream of events received containing pieces of generated response\n4. Each event contains small text chunks sent to browser/app for display\n5. User sees response building in real-time\n\n**Event Types:**\n- Message start (first event)\n- Content block delta (contains actual generated text chunks)\n- Content block stop\n- Message stop\n- Metadata (last event)\n\n**Key Implementation Points:**\n- Response contains 'stream' key = event stream object/generator\n- Iterate over stream to get individual events\n- Only content block delta events matter for text generation\n- Text chunks found at: event['content_block_delta']['delta']['text']\n- Collect chunks to reconstruct full message if needed\n\n**Code Pattern:**\n\\`\\`\\`\nfor event in response['stream']:\n    if 'content_block_delta' in event:\n        chunk = event['content_block_delta']['delta']['text']\n        # Send chunk to user interface\n        total_text += chunk\n\\`\\`\\`\n</note>\n\n<note title=\"Controlling Model Output\">\nModel output control methods:\n\nPre-filling assistant messages = manually adding assistant message at end of message list to steer response direction. Claude continues from where pre-filled text ends, not from beginning. Must concatenate pre-filled text with returned response for complete output.\n\nStop sequences = strings that force Claude to stop generation immediately when encountered. Provided as parameter to converse function. Can specify multiple sequences. Stopping sequence itself is excluded from final response.\n\nKey mechanics:\n- Pre-filling: Claude sees partial assistant response, assumes already authored, continues from exact endpoint\n- Stop sequences: Generation halts at first matching string, partial response returned\n- Both techniques strongly influence output beyond prompt modification alone\n\nImplementation: Add stop_sequences parameter to chat function, pass to inference_config. Pre-filling requires manual message list construction with assistant role.\n</note>\n\n<note title=\"Structured Data\">\nStructured Data Generation = combining stop sequences + assistant message prefilling to extract only desired content without extra commentary\n\nProblem: Claude naturally adds headers/footers/explanatory text when generating structured data (JSON, Python, lists), but applications often need only the raw content for copying/parsing.\n\nSolution Pattern:\n1. Pre-fill assistant message with opening delimiter (e.g., \"\\`\\`\\`json\")\n2. Set stop sequence to match closing delimiter (e.g., \"\\`\\`\\`\")\n3. Claude assumes it already wrote the opening, generates only the content, stops at closing delimiter\n\nResult: Returns only the structured data between delimiters, no additional commentary.\n\nExample Flow:\nUser: \"generate JSON rule\"\nAssistant prefill: \"\\`\\`\\`json\"\nStop sequence: \"\\`\\`\\`\"\nOutput: raw JSON only\n\nTechnique works for any structured data format, not just JSON. Essential for applications requiring clean, parseable output without manual text extraction.\n</note>\n\n<note title=\"Prompt Evaluation\">\nPrompt Engineering = techniques for writing/editing prompts to help Claude understand requests and desired responses.\n\nPrompt Evaluation = automated testing of prompts with objective metrics to measure effectiveness.\n\nThree paths after writing a prompt:\n1. Test once/twice, use in production (trap)\n2. Test with custom inputs, minor tweaks (trap)  \n3. Run through evaluation pipeline for objective scoring (recommended)\n\nOptions 1 and 2 are common traps - engineers don't test prompts enough for serious applications.\n\nRecommended process: Write prompt → evaluation pipeline → get objective score → iterate → optimize performance.\n\nPrompt evaluation comes before prompt engineering in the workflow to establish measurement baseline before optimization.\n</note>\n\n<note title=\"A Typical Eval Workflow\">\nTypical eval workflow = iterative process to improve prompts through systematic testing and scoring\n\nNo standard methodology exists - many different approaches and tools available (open source packages, paid options). Can start simple and scale up.\n\nCore workflow steps:\n1. Initial prompt draft = write baseline prompt to improve\n2. Evaluation dataset = collection of test inputs (can be hand-crafted or AI-generated, ranging from tens to thousands of records)\n3. Prompt execution = feed each dataset input into prompt, generate responses from LLM\n4. Grading = score each response using grader system (typically 1-10 scale where 10 = perfect, lower scores indicate improvement needed)\n5. Averaging = calculate overall performance score across all test cases\n6. Iteration = modify prompt based on results, repeat entire process\n\nComparison methodology = run multiple prompt versions through same pipeline, compare average scores to identify better performing version\n\nObjective = establish quantitative way to measure prompt performance improvements rather than relying on subjective assessment\n\nKey insight = lightweight custom implementation possible, don't need heavy enterprise solutions to get started with prompt evaluation\n</note>\n\n<note title=\"Generating Test Datasets\">\nCustom Prompt Evaluation Workflow = building system to test prompt performance for AWS code generation tasks\n\nGoal = prompt accepts user task, outputs one of three formats: Python code, JSON configuration, or regular expression (no extra explanation/formatting)\n\nEvaluation Dataset = collection of test inputs (JSON objects with task properties) fed into prompt to measure performance\n\nDataset Generation Methods = manual assembly or automatic generation using LLMs\n\nImplementation Steps:\n1. Draft prompt (V1: \"please provide solution to following task\")\n2. Generate test dataset using Haiku model (faster/cheaper for dataset generation)\n3. Create generate_dataset() function with lengthy prompt requesting AWS-related tasks\n4. Use JSON extraction technique: pre-filled assistant message + stop sequences\n5. Parse JSON response and return structured data\n6. Save dataset to JSON file for reuse\n\nKey Technical Details:\n- Uses inference profile for Haiku model (speed optimization)\n- Requests array of objects with task properties\n- Example generates 3 test cases (real-world needs many more)\n- JSON extraction pattern: backticks + stop sequence\n- Dataset structure will evolve (mentioned future changes)\n\nOutput = structured test dataset for evaluating AWS code generation prompt performance\n</note>\n\n<note title=\"Running the Eval\">\nRunning the Eval = process of executing test cases through LLM and collecting results for grading.\n\nTest case = individual record from generated dataset (JSON objects containing tasks).\n\nRun prompt function = takes test case, merges with prompt template, sends to Claude, returns output. Basic V1 prompt: \"please solve the following task [test_case_task]\". Currently no output formatting constraints.\n\nRun test case function = takes individual case, calls run prompt, grades result (placeholder hardcoded score of 10), returns dictionary with output/test_case/score.\n\nRun eval function = loads dataset, loops through all test cases calling run test case, assembles results into list.\n\nEval pipeline structure = vast majority is just these three functions plus grading component. Minimal code required.\n\nRuntime = ~31 seconds with Haiku model for full dataset. Speed optimization techniques exist.\n\nOutput format = array of objects, each containing Claude's output (verbose/unformatted), original test case definition, and score.\n\nCurrent limitations = no output formatting in prompt leads to verbose responses, hardcoded scoring placeholder needs actual grading implementation.\n\nNext step = implement actual graders to evaluate Claude's responses against expected outputs.\n</note>\n\n<note title=\"Model Based Grading\">\nModel Based Grading = Using additional AI models to evaluate original model outputs by providing objective scoring\n\nThree grader types:\n- Code graders = Programmatic checks (length, syntax, keywords, readability scores)\n- Model graders = Additional API calls to evaluate quality, instruction following, completeness  \n- Human graders = Manual evaluation (flexible but time-consuming)\n\nModel grader process:\n1. Take original model output\n2. Feed into evaluation model with detailed prompt\n3. Request reasoning + score (typically 1-10 scale)\n4. Return structured assessment\n\nKey implementation points:\n- Evaluation criteria must be defined upfront\n- Prompts should request reasoning/strengths/weaknesses to avoid default middling scores\n- JSON response format with pre-filled assistant messages for clean parsing\n- Average scores across test cases for objective metrics\n\nCommon evaluation criteria:\n- Format compliance (correct output type)\n- Syntax validation\n- Task completion accuracy\n- General response quality\n\nModel graders provide flexibility for subjective quality assessment that code graders cannot handle programmatically.\n</note>\n\n<note title=\"Code Based Grading\">\nCode Based Grading = automated validation system for model outputs containing code/structured data\n\nCore Components:\n- validate_json(): parses JSON using json.loads(), returns 10 if valid else 0\n- validate_python(): parses Python using AST, returns 10 if valid else 0  \n- validate_regex(): compiles regex pattern, returns 10 if valid else 0\n- grade_syntax(): router function that calls appropriate validator based on test case format\n\nImplementation Requirements:\n- Dataset must include \"format\" key specifying expected output type (JSON/Python/regex)\n- Prompt template updated to specify \"respond only with Python, JSON, or plain regex, no comments\"\n- Assistant message pre-filled with \\`\\`\\`code to force raw code output\n- Stop sequence using closing backticks\n- Final score = (model_score + syntax_score) / 2\n\nKey Mechanism = try/catch parsing - successful parse = full score, parsing error = zero score\n\nPurpose = ensures model outputs valid, executable code without explanatory text or syntax errors\n</note>\n\n<note title=\"Prompt Engineering\">\nPrompt Engineering = improving written prompts to get more reliable and higher quality outputs from language models.\n\nCore Process:\n1. Set goal for prompt\n2. Write initial version (usually poor quality)\n3. Evaluate prompt performance\n4. Apply prompt engineering techniques iteratively\n5. Re-evaluate after each improvement\n\nExample Goal: Generate one-day meal plan for athletes based on height, weight, goals, dietary restrictions.\n\nEvaluation Setup:\n- Uses flexible evaluation pipeline with prompt evaluator class\n- Supports concurrency for faster processing (adjust based on rate limits)\n- Generates test datasets with specified inputs and criteria\n- Creates output.html reports for detailed analysis\n\nKey Components:\n- prompt_inputs = dictionary containing variables to interpolate into prompt\n- run_prompt function = executes once per test case\n- extra_criteria = additional validation requirements for model grading\n- Dataset generation requires: prompt purpose description, input specifications, number of test cases\n\nInitial Performance: Typically poor scores (baseline for improvement measurement).\n\nIterative Improvement: Each prompt engineering technique application followed by re-evaluation to measure performance gains.\n</note>\n\n<note title=\"Being Clear and Direct\">\nBeing Clear and Direct = using simple, direct language with action verbs in the first line of prompts to specify exact tasks.\n\nFirst line = most important part of prompt. Should contain action verb + clear task description.\n\nAction verbs = write, generate, create, identify, analyze, etc.\n\nGood examples:\n- \"Write three paragraphs about how solar panels work\"\n- \"Identify three countries that use geothermal energy and include generation stats for each\"\n- \"Generate a one day meal plan for an athlete that meets their dietary restrictions\"\n\nStructure = Action verb + specific task + output expectations\n\nResult = clearer task understanding, better output quality. In example, score improved from 2.32 to 3.92 by applying this technique.\n</note>\n\n<note title=\"Being Specific\">\nBeing Specific = adding guidelines or steps to direct model output in particular direction\n\nTwo types of guidelines:\n- Type A: List qualities/attributes for output (length, structure, format requirements)\n- Type B: Provide step-by-step process for model to follow (forces consideration of specific elements, improves reasoning quality)\n\nBoth can be combined in professional prompts.\n\nWhen to use:\n- Type A (attributes): Almost always recommended for any prompt\n- Type B (steps): Use for complex problems requiring broader perspective or when model needs to consider additional viewpoints beyond natural inclination\n\nExample improvement: Adding guidelines improved evaluation score from 3.92 to 7.86, demonstrating significant impact on output quality.\n</note>\n\n<note title=\"Structure with XML Tags\">\nXML Tags for Prompt Structure = Method to organize and clarify different content sections within prompts using custom XML-style tags.\n\nPurpose = Helps language models distinguish between different types of content when large amounts of text are interpolated into prompts.\n\nImplementation = Wrap content sections in descriptive tags like <sales_records></sales_records> or <my_code></my_code> and <docs></docs>.\n\nTag naming = Use specific, descriptive names rather than generic ones. \"sales_records\" better than \"data\".\n\nUse cases = Essential when prompt contains multiple large content blocks that could be confused (code vs documentation, data vs instructions). Also useful for smaller content to explicitly mark it as external input.\n\nBenefits = Reduces ambiguity about content boundaries and context, leading to improved model performance and output quality.\n\nExample application = Wrapping athlete information (height, weight, goals, restrictions) in <athlete_information></athlete_information> tags to clarify it as external input for meal plan generation.\n</note>\n\n<note title=\"Providing Examples\">\nExample-based prompting = providing sample inputs and ideal outputs in prompts to guide model behavior\n\nOne-shot prompting = providing single example\nMulti-shot prompting = providing multiple examples\n\nImplementation = wrap examples in XML tags for structure, include sample input and ideal output\n\nUse cases = handling corner cases (like sarcasm detection), complex output formats (JSON structures), improving consistency\n\nCorner case handling = explicitly warn model about specific scenarios then provide example demonstrating proper handling\n\nOutput format guidance = show complex structures through examples rather than just describing them\n\nPrompt evaluation optimization = find highest-scoring outputs from testing, use as examples in prompts, optionally include reasoning why output is ideal\n\nEffectiveness boost = explaining why example output is ideal helps reinforce desired behavior patterns\n\nCommon application = anytime you need consistent handling of edge cases or specific output formatting requirements\n</note>\n\n<note title=\"Introducing Tool Use\">\nTool use = Claude accessing external information beyond training data\n\nProblem: Claude lacks real-time data (e.g., current weather, recent events)\n\nTool use flow:\n1. Send request to Claude + tool instructions\n2. Claude requests specific external data\n3. Server runs code to fetch data from external APIs\n4. Send fetched data back to Claude\n5. Claude generates final response with external data\n\nWeather example:\nUser asks weather → Claude requests weather data → Server calls weather API → Claude receives weather info → Claude responds with current weather\n\nKey challenge: Code implementation order differs from logical flow. Developers typically write: tool function → JSON schema → tool result handling → include schema in request. This jumping around makes tool use conceptually difficult.\n\nSolution: Reference the logical flow diagram frequently when implementing to maintain clarity on which piece is being built.\n</note>\n\n<note title=\"Tool Functions\">\nTool Functions = Python functions automatically called by Claude to solve tasks it cannot handle natively.\n\nClaude limitations requiring tools:\n- Doesn't know exact current time\n- Imperfect time-based calculations \n- Cannot actually set reminders/perform external actions\n\nTool implementation approach = Create dedicated function for each limitation. Example project uses 3 tools: get_current_datetime, add_duration_to_date, set_reminder.\n\nTool function best practices:\n- Use descriptive argument names (critical for Claude understanding)\n- Validate inputs when possible, raise errors for invalid data\n- Write as plain Python functions that return useful data\n\nTool creation process = Multi-step: 1) Write tool function 2) Create JSON schema spec 3) Register with Claude 4) Test integration.\n\nExample tool function:\n\\`\\`\\`\ndef get_current_datetime(date_format=\"%Y%m%d%H%M%S\"):\n    return datetime.now().strftime(date_format)\n\\`\\`\\`\n\nTool functions bridge gap between Claude's knowledge and real-world system capabilities.\n</note>\n\n<note title=\"JSON Schema for Tools\">\nJSON Schema for Tools = specification format describing tool function arguments that helps Claude understand how to use functions.\n\nJSON Schema = general-purpose data validation technique, not specific to LLMs or tool use.\n\nSchema structure = two main parts: name/description at top, JSON object defining function parameters.\n\nSchema creation process:\n1. Create dictionary with function arguments as keys and sample data as values\n2. Convert to JSON format (Python True → JSON true)\n3. Use online \"JSON to JSON schema converter\" tools\n4. Remove $schema statement from output\n5. Add description field to each property\n\nDescription best practices:\n- Overall tool description = 3-4 sentences explaining what tool does, when to use it, what data it returns\n- Property descriptions = detailed explanations of each argument's purpose\n- Can use Claude to generate descriptions by pasting function code\n\nSchema purpose = sent to Claude in requests to enable proper function calling with correct argument types and structure.\n</note>\n\n<note title=\"Handling Tool Use Responses\">\nTool Use Response Structure:\n\nResponse = dictionary with stop_reason key. stop_reason values indicate why model stopped generating output.\n\nstop_reason = \"tool_use\" signals model wants to call a tool.\n\nAssistant Message Structure:\n\ncontent = list of parts (not single text object)\n- text part = helpful message to user\n- tool_use part = instruction for developers to run specified tool\n\nTool Use Part Components:\n\ntool_use_id = unique identifier for tracking\nname = exact tool name from JSON schema\ninput = dictionary of arguments to pass to tool function\n\nConversation History Requirements:\n\nMust include entire conversation history in follow-up requests including:\n- Original user message\n- Complete assistant message (all parts)\n- Tool result parts (for subsequent requests)\n\nImplementation Changes:\n\nchat() function returns both text and parts (not just text)\nadd_assistant_message() and add_user_message() accept either string or list of parts\nContent parameter replaces text parameter for flexibility\n\nTool Choice Configuration:\n\nauto = Claude decides whether to use tools (default)\nany = forces Claude to use any available tool\n{tool: {name: \"tool_name\"}} = forces specific tool (useful for testing)\n\nKey Takeaway: Tool use responses contain multiple message parts requiring conversation history management to maintain context across tool calls.\n</note>\n\n<note title=\"Running Tool Functions\">\nTool Function Execution = Process of running actual tool functions based on tool use parts received from Claude\n\nKey Components:\n\nTool Use Parts Extraction = Filter message parts to find dictionaries containing \"tool_use\" key\nMultiple Tool Handling = Claude can send multiple tool use requests in parallel within single message\nRequired Extraction = tool_use_id, name, input from each tool use part\n\nTool Execution Function:\n- Match tool name to actual function\n- Use **tool_input to splat dictionary arguments into function\n- Handle unknown tool names with exceptions\n- Return function output\n\nTool Result Parts = Response format sent back to Claude after tool execution\nStructure: {tool_result: {tool_use_id, content: [text output], status: success/error}}\n\nTool Use ID System = Unique identifiers linking tool requests to results when multiple tools run in parallel\n\nError Handling = Wrap tool execution in try/except, return error status to Claude instead of failing\n- Claude can analyze errors and potentially retry with corrected arguments\n- Error tool result includes error message in content\n\nOutput Processing = JSON serialize tool outputs to strings for consistent formatting\nFinal Step = Return list of tool result parts to send back to Claude\n</note>\n\n<note title=\"Sending Tool Results\">\nTool results completion process:\n\nTool result parts = list of outputs from executed tools that must be added to conversation history\n\nMessage history management = add assistant message with tool use requests, then add user message with tool result parts to maintain conversation flow\n\nFinal API call requirements = send complete message history + original tool schemas to Claude (schemas必须included or Claude gets confused about tool definitions)\n\nConversation structure = User message → Assistant message (tool requests) → User message (tool results) → Assistant response\n\nVerification method = successful tool execution evidenced by Claude accessing real-time data it normally wouldn't have (like current time vs just date)\n\nCritical requirement = tool schemas must be included in follow-up calls for Claude to understand tool definitions and process results correctly.\n</note>\n\n<note title=\"Multi-Turn Conversations with Tools\">\nMulti-turn conversations with tools = conversations where AI can iteratively use tools and respond based on results until task completion.\n\nKey implementation pattern:\n1. Check response.stop_reason to determine if tool use needed\n2. If stop_reason != \"tool_use\", conversation complete\n3. If stop_reason == \"tool_use\", process tool requests and continue loop\n\nEssential components:\n- Chat function returns: parts list, stop_reason, text content\n- Run_conversation function: handles while loop until non-tool-use stop reason\n- Message flow: user message → assistant with tool_use → tool results → assistant final response\n\nCritical logic: Only process tool requests when stop_reason indicates tool use needed. Prevents adding empty tool result messages when no tools requested.\n\nFunction structure:\n\\`\\`\\`\nrun_conversation(messages):\n  while True:\n    result = chat(messages, tools)\n    add assistant_message(result.parts) to messages\n    if result.stop_reason != \"tool_use\": break\n    tool_results = run_tools(result.parts)\n    add user_message(tool_results) to messages\n  return messages\n\\`\\`\\`\n\nBenefits: Handles both tool-requiring and non-tool conversations automatically. Maintains proper message ordering throughout multi-turn interactions.\n</note>\n\n<note title=\"Adding Multiple Tools\">\nMultiple Tools Implementation = Adding tools beyond initial single tool requires minimal code changes\n\nTool Addition Process:\n1. Add tool schemas to conversation function (JSON specs define tool parameters/structure)\n2. Add corresponding cases to run_tools function (maps tool names to actual function calls)\n3. Rerun cells to activate changes\n\nExample Flow: \"Set reminder for doctor appointment in 100 days\"\n- Claude plans: get current date → add 100 days duration → set reminder\n- Executes three tools sequentially to complete task\n\nKey Insight: Initial tool use setup is complex, but adding additional tools afterward is straightforward - just schema registration + function mapping.\n\nTool Schema = JSON specification defining tool parameters and structure\nRun Tools Function = Dispatcher that maps tool names to actual function implementations\n</note>\n\n<note title=\"Batch Tool Use\">\nBatch Tool Use = technique to parallelize tool calls by implementing a special \"batch tool\" that can execute multiple tool operations simultaneously.\n\nProblem: Claude can send multiple tool use parts in one message but often executes them sequentially rather than in parallel, even when operations are independent.\n\nSolution: Create batch tool with spec containing:\n- Tool name: \"batch_tool\" \n- Input: list called \"invocations\"\n- Each invocation object has: tool name + arguments (JSON encoded string)\n\nImplementation:\n- Add batch tool case to run_tool function\n- Create run_batch function that loops through invocations\n- Parse JSON arguments with json.loads()\n- Delegate to existing run_tool function for each invocation\n- Collect all outputs in batch_output list\n- Return combined results as single tool response\n\nResult: Claude treats multiple tool calls as single batch operation, achieving parallelization.\n\nKey insight: By providing batch capability as a tool option, Claude is \"tricked\" into grouping parallel-eligible operations into single request rather than making sequential calls.\n</note>\n\n<note title=\"Structured Data with Tools\">\nStructured Data with Tools = method for extracting JSON from Claude using tool schemas instead of prompt-based techniques with message pre-fill and stop sequences.\n\nCore Concept = Define a JSON schema as a fake tool where inputs match the desired data structure. Claude calls this tool with extracted data as arguments.\n\nProcess Flow:\n1. Create tool schema with desired output structure as input parameters\n2. Send prompt + data + schema to Claude with tool_choice parameter\n3. Claude responds with assistant message containing tool use call\n4. Extract JSON arguments from tool call response\n5. End conversation (don't continue after getting data)\n\nAdvantages = More reliable than prompt-based extraction\nDisadvantages = More complex setup, requires tool schema definition\n\nTool Choice Parameter = Forces Claude to use specific tool instead of auto-deciding. Options: \"auto\", \"any\", or specific tool name.\n\nExample Structure = For financial data extraction: tool inputs would be balance (integer) and key_insights (string array), matching desired JSON output format.\n\nImplementation = Use tool_choice parameter set to specific tool name to ensure Claude calls the extraction tool rather than responding normally.\n\nKey Difference from Prompt Method = Uses Claude's tool-calling mechanism instead of JSON formatting in text responses, providing more structured and reliable output parsing.\n</note>\n\n<note title=\"Flexible Tool Extraction\">\nFlexible Tool Extraction = method to avoid writing large JSON schemas for structured data extraction\n\nCore approach: Create single schema named \"toJSON\" with flexible object input allowing any properties Claude wants to add. Specify desired structure in prompt text instead of rigid schema.\n\nImplementation steps:\n1. Define toJSON tool with flexible object parameter\n2. Write prompt listing exact properties and types needed\n3. Tell Claude to call toJSON with specified structure\n4. Use escaped curly braces in F-strings for property definitions\n\nBenefits: Easy structure modifications by editing prompt text only, no schema rewrites needed\n\nTrade-off: Slightly lower quality results compared to dedicated schemas, but still produces high-quality JSON\n\nUse case recommendation: Flexible approach for general extraction, dedicated schemas for critical data extraction tasks requiring maximum accuracy\n\nKey advantage: Rapid iteration and structure changes without managing complex JSON schema definitions\n</note>\n\n<note title=\"The Text Editor Tool\">\nText Editor Tool = built-in tool for Claude providing file/directory manipulation capabilities\n\nJSON Schema = provided by Claude automatically, defines tool interface and parameters  \nTool Implementation = must be written by developer to handle actual file operations\n\nRequired Tool Names:\n- Claude 3-7: \"str_replace_editor\"  \n- Claude 3-5: \"str_replace_based_edit_tool\"\n- Must use exact string IDs for AWS Bedrock\n\nFive Commands Claude Can Request:\n1. view = read file/directory contents\n2. str_replace = replace specific text in file\n3. create = make new file/directory\n4. insert = add text at specific line\n5. undo_edit = reverse previous edit\n\nTool Flow:\n1. User sends prompt requiring file access\n2. Claude responds with assistant message containing tool_use part\n3. Developer code processes command using text editor class\n4. Tool result sent back to Claude as user message\n5. Claude provides final response\n\nKey Point = Developer must implement all five command handlers despite Claude having built-in schema. Enables Claude to act as software engineer with file system access.\n\nUse Case = Replicates AI code editor functionality - can refactor files, create new features, generate test files automatically.\n</note>\n\n<note title=\"Introducing Retrieval Augmented Generation\">\nRAG = Retrieval Augmented Generation, technique for querying large documents with LLMs.\n\nProblem: Need to extract specific information from large documents (100-1000 pages) using Claude, but face context limits and performance issues.\n\nOption 1 - Direct approach: Put entire document text into prompt with question. Issues: hits token limits, reduced effectiveness with long prompts, higher costs, slower processing.\n\nOption 2 - RAG approach: Two-step process. Step 1: Break document into small chunks. Step 2: For user question, find most relevant chunks and include only those in prompt with question.\n\nRAG advantages: Claude focuses on relevant content only, scales to very large/multiple documents, smaller prompts = faster/cheaper processing.\n\nRAG disadvantages: Higher complexity, requires preprocessing step, need search mechanism to find relevant chunks, must define \"relevance\", no guarantee chunks contain complete context, multiple chunking strategies possible (equal portions vs headers vs other methods).\n\nKey tradeoff: RAG adds technical complexity but enables handling of large documents that would otherwise exceed token limits or perform poorly.\n\nImplementation requires: document chunking strategy, relevance search mechanism, evaluation of chunking approaches for specific use case.\n</note>\n\n<note title=\"Text Chunking Strategies\">\nText Chunking Strategies = process of dividing documents into smaller text segments for RAG pipelines\n\n**Core Problem**: Poor chunking creates context errors. Example: medical text containing \"bug\" gets retrieved for software engineering questions due to word overlap without proper context.\n\n**Three Main Strategies**:\n\n1. **Size-based chunking** = divide document into equal-length strings\n   - Pros: easiest to implement, most common in production\n   - Cons: cuts off words mid-sentence, lacks context\n   - Solution: overlap strategy = include characters from neighboring chunks to add context\n   - Trade-off: creates text duplication but improves meaning\n\n2. **Structure-based chunking** = divide based on document structure (headers, paragraphs, sections)\n   - Example: split on markdown headers (##)\n   - Pros: creates well-formed, contextual sections\n   - Cons: requires structured documents, doesn't work with plain PDFs/unformatted text\n\n3. **Semantic-based chunking** = use NLP to group related sentences/sections\n   - Most advanced technique\n   - Groups consecutive sentences by semantic similarity\n   - Infinite possible chunking variations exist\n\n**Strategy Selection**:\n- Structured documents with format guarantees = structure-based chunking\n- Unstructured documents = sentence-based chunking\n- Code or complex formats = character-based chunking (most reliable fallback)\n\n**Key Parameters**:\n- Chunk size\n- Overlap amount\n- Splitting criteria (characters, sentences, sections)\n\nCharacter-based chunking = most reliable default despite suboptimal results because it works across document types.\n</note>\n\n<note title=\"Text Embeddings\">\nText Embeddings = numerical representation of meaning in text generated by embedding models\n\nProcess: Text input → Embedding model → List of numbers (typically 1024 elements, values -1 to +1)\n\nKey concepts:\n- Each number represents score of some text quality/feature\n- Actual meaning of individual numbers is unknown/opaque\n- Useful mental model: imagine numbers as scores for different qualities (happiness, topic relevance, etc.)\n- Multiple embedding models available (e.g., Titan embed text V2)\n\nPurpose in RAG: Enable semantic search to find text chunks related to user queries by comparing numerical representations rather than exact text matching\n\nImplementation: Simple API call to embedding service, returns long array of float values representing text meaning in high-dimensional space\n</note>\n\n<note title=\"The Full RAG Flow\">\nRAG Flow = complete pipeline merging text chunking, embeddings, and retrieval for document-based AI responses.\n\nStep 1: Document chunking = split source documents into separate text pieces.\n\nStep 2: Generate embeddings = convert text chunks into numerical vectors using embedding models. Embeddings = multi-dimensional arrays representing semantic meaning of text.\n\nStep 3: Normalization = scale vector magnitudes to 1.0 (handled automatically by embedding APIs).\n\nStep 4: Vector database storage = specialized database for storing, comparing, and retrieving numerical vectors.\n\nStep 5: User query processing = embed user question using same embedding model.\n\nStep 6: Similarity search = vector database finds most similar stored embeddings to user query.\n\nCosine similarity = measure of similarity between vectors, calculated as cosine of angle between them. Results range from -1 to 1, where values closer to 1 indicate higher similarity.\n\nCosine distance = 1 minus cosine similarity. Values closer to 0 indicate higher similarity.\n\nStep 7: Prompt construction = combine user question with most relevant retrieved text chunks.\n\nStep 8: LLM generation = send combined prompt to language model for response.\n\nVector databases use cosine similarity calculations to identify semantically related content without exact keyword matching.\n</note>\n\n<note title=\"Implementing the Rag Flow\">\nRAG Flow Implementation = 5-step process for retrieval-augmented generation using vector database\n\nStep 1: Text Chunking = Split document into sections using chunk_by_section function on report.MD file\n\nStep 2: Embedding Generation = Loop through chunks, generate embeddings via API calls using generate_embedding function for each text chunk\n\nStep 3: Vector Store Creation = Create vector_index instance, store embedding-chunk pairs using zip operation. Store both embedding vector + original text content in dictionary format for later text retrieval\n\nStep 4: Query Processing = Generate embedding for user question (\"what did software entering depth do last year\")\n\nStep 5: Similarity Search = Use store.search with user embedding, retrieve top 2 most relevant chunks based on cosine distance\n\nResults = Section 2 (distance 0.71) and Methodology section (distance 0.72) identified as most relevant chunks\n\nKey Design Choice = Store original text with embeddings because raw embeddings lack meaning for developers - need text or chunk ID for practical use\n\nVector Database Class = Custom implementation called vector_index for storing and searching embeddings\n\nSearch Output = Returns document content + cosine distance score for relevance ranking\n\nCurrent Limitations = Workflow functional but has scenarios requiring improvements\n</note>\n\n<note title=\"BM25 Lexical Search\">\nBM25 = Best Match 25, a lexical search algorithm used in RAG pipelines to improve search results through keyword-based text matching.\n\nProblem with pure semantic search = Can miss documents with exact keyword matches, returning irrelevant results despite semantic similarity.\n\nHybrid search strategy = Run semantic search and lexical search in parallel, then merge results to get both semantic understanding and exact keyword matching.\n\nBM25 algorithm steps:\n1. Tokenize user query into individual terms (remove punctuation, split on spaces)\n2. Count frequency of each term across all document chunks\n3. Assign importance weights - rare terms get higher weights, common terms get lower weights\n4. Return chunks that contain higher-weighted terms more frequently\n\nTerm weighting logic = Frequently used terms (like \"a\", \"the\") are less important, rare specific terms (like \"incident 2023\") are more important for search relevance.\n\nBM25 advantages = Better handles exact keyword matches that semantic search might miss, especially for specific identifiers, codes, or technical terms.\n\nImplementation approach = Create unified API where both semantic and lexical search systems have add_document() and search() functions for easy integration.\n\nNext step = Merge semantic and lexical search results to combine benefits of both approaches.\n</note>\n\n<note title=\"A Multi-Search RAG Pipeline\">\nMulti-Search RAG Pipeline = RAG system combining semantic search (vector index) and lexical search (BM25 index) for improved retrieval accuracy.\n\nCore Components:\n- Vector Index = semantic search using embeddings\n- BM25 Index = lexical search using keyword matching\n- Retriever Class = wrapper that coordinates both indexes with unified API (add_document, search methods)\n\nReciprocal Rank Fusion (RRF) = technique for merging results from multiple search methods. Formula: score = sum of (1/(1+rank)) for each search method's ranking of a document. Higher scores indicate better relevance.\n\nRRF Process:\n1. Execute same query on both indexes\n2. Collect all unique results with their ranks from each method\n3. Apply RRF formula to calculate combined score\n4. Sort by score (highest to lowest)\n\nExample: Document ranked 1st in vector search and 2nd in BM25 gets score = 1/(1+1) + 1/(1+2) = 0.5 + 0.33 = 0.83\n\nBenefits:\n- Improved search accuracy by combining semantic and lexical strengths\n- Modular design allows easy addition of new search methods\n- Unified API enables seamless integration\n- Better handling of edge cases where single method fails\n\nImplementation uses consistent API across all search components, enabling straightforward composition and extension.\n</note>\n\n<note title=\"Reranking Results\">\nReranking = post-processing step that uses LLM to reorder search results by relevance after initial retrieval\n\nProcess: Vector + BM25 search → merge results → pass to LLM (Claude) with prompt asking to rank documents by relevance to user query\n\nImplementation details:\n- Assigns temporary IDs to documents for efficiency (LLM returns ordered IDs rather than full text)\n- Uses XML formatting for document presentation to LLM\n- Requests specific number of most relevant results in decreasing order\n- Uses assistant message pre-fill + stop sequence for structured JSON output\n\nBenefits: Improves retrieval accuracy by leveraging LLM's understanding of semantic relevance between query and documents\n\nTradeoffs: Increases latency due to additional LLM call but significantly improves search result quality\n\nExample improvement: Query \"What did engineering team do with incident 2023?\" correctly surfaces software engineering section to top position after reranking, whereas hybrid retrieval alone ranked it second.\n</note>\n\n<note title=\"Contextual Retrieval\">\nContextual Retrieval = technique to improve RAG pipeline accuracy by adding context to document chunks before vector storage.\n\nProblem: When documents are split into chunks, each chunk loses context from the original document.\n\nSolution: Pre-processing step that uses LLM to generate contextual information for each chunk before inserting into retriever database.\n\nProcess:\n1. Take individual chunk + original source document\n2. Send to LLM with prompt asking to situate the chunk within larger document context\n3. LLM generates brief contextual description\n4. Combine generated context + original chunk = contextualized chunk\n5. Insert contextualized chunk into vector/BM25 indexes\n\nLarge Document Handling: If source document too large for single prompt, use subset strategy:\n- Include chunks from document start (summary/abstract)\n- Include chunks immediately before target chunk\n- Skip middle sections that provide less relevant context\n\nOutput Example: LLM adds context like \"This is section X from larger report covering Y domains, following methodology Z, before financial analysis section\"\n\nImplementation: add_context() function takes text chunk + source text, prompts LLM for context, returns contextualized chunk for indexing.\n\nBenefit: Chunks retain connection to overall document structure and cross-references, improving retrieval accuracy for complex documents with interconnected sections.\n</note>\n\n<note title=\"Extended Thinking\">\nExtended Thinking = Claude's feature that allows reasoning before generating final response\n\nKey mechanics:\n- Displays separate thinking process in chat UIs\n- Generates reasoning content part + text part in response\n- Reasoning content part contains thinking text + cryptographic signature\n- Signature prevents tampering with reasoning text (security measure)\n- Redacted content = encrypted thinking text flagged by safety systems\n\nTrade-offs:\n- Increased accuracy for complex tasks\n- Higher cost (charged for thinking tokens)\n- Increased latency\n\nWhen to use:\n- Run prompt evals first\n- Enable only if accuracy insufficient after prompt optimization efforts\n- No universal rule - depends on specific use case evals\n\nTechnical implementation:\n- thinking parameter (boolean)\n- thinking_budget parameter (minimum 1024 tokens)\n- Budget = max tokens Claude can spend on thinking phase\n- Optimal budget determined by evals, not rules of thumb\n\nTesting:\n- Magic string available to force redacted content responses\n- Used only for testing application's handling of redacted content blocks\n\nCore principle: Decision to use extended thinking must be eval-driven, not assumption-based.\n</note>\n\n<note title=\"Image Support\">\nClaude Vision Capabilities = ability to process images in messages alongside text\n\nImage Requirements:\n- Max 20 images per request across all messages\n- Size/dimension limitations apply\n- Images consume tokens based on pixel dimensions (height × width)\n\nImage Implementation:\n- Images = message parts in user messages\n- One image part per image\n- Multiple images = multiple image parts in same message\n\nCritical Success Factor = Strong prompting techniques required for accurate results\n\nCommon Mistake = Simple prompts with images produce poor results\nExample: \"How many marbles?\" → incorrect count (13 instead of 12)\n\nEffective Techniques:\n1. Step-by-step analysis instructions\n   - Break down image analysis into sequential steps\n   - Provide verification/recount mechanisms\n   - Compare results for accuracy\n\n2. One-shot/multi-shot prompting\n   - Alternate image parts with text parts\n   - Provide example image with correct analysis\n   - Format: Image → Text explanation → Target Image → Query\n\nReal-world Application Example = Fire risk assessment for insurance\n- Satellite imagery analysis\n- Multi-step evaluation: residence identification, tree density, fire service access, roof overhang assessment\n- Structured scoring system with detailed criteria\n- Final risk rating with summary\n\nKey Takeaway = Image accuracy depends heavily on prompt engineering, not just image quality. Apply same advanced prompting techniques used for text-only interactions.\n</note>\n\n<note title=\"Prompt Caching\">\nPrompt Caching = feature that speeds up Claude's responses and reduces text generation costs by reusing computational work from previous requests.\n\nNormal Request Flow:\n1. User sends message to Claude\n2. Claude creates internal data structures and performs calculations on input text\n3. Claude generates output using that work\n4. Claude sends response back\n5. Claude discards all calculations and internal work\n\nProblem = Claude repeatedly processes identical input text, wasting computational resources and time.\n\nSolution = Prompt Caching:\n- Instead of discarding calculations, Claude stores them in temporary cache\n- When follow-up requests contain identical input messages, Claude retrieves cached work\n- Reuses previous calculations rather than reprocessing same text\n- Results in faster response times and lower costs\n\nKey Benefit = Dramatic speed improvement for conversations or requests with repeated content by eliminating redundant processing.\n</note>\n\n<note title=\"Rules of Prompt Caching\">\nPrompt Caching = temporary storage of Claude's processing work for 5 minutes to avoid reprocessing identical content\n\nCache Point = special message part that marks where caching should occur - caches all content before the cache point\n\nKey mechanics:\n- Initial request processes content and saves work to cache\n- Follow-up requests reuse cached work if content before cache point is identical\n- Content after cache point is never cached\n- Works across multiple messages/parts including assistant messages\n\nRequirements:\n- Minimum 1024 tokens of content before cache point required for caching\n- Content before cache point must be exactly identical between requests\n- Any difference in pre-cache-point content invalidates cache usage\n\nCommon applications:\n- Tool definition schemas (JSON specs often reused)\n- System prompts (rarely change between requests)\n- Repeated content processing scenarios\n\nCache points can be applied to:\n- Text parts within messages\n- Tool definition lists\n- System prompts\n- Multi-message conversations\n\nMost useful for scenarios with repeated identical content sent to Claude multiple times.\n</note>\n\n<note title=\"Prompt Caching in Action\">\nPrompt caching = storing processed prompt parts to reuse in subsequent requests, reducing cost and latency\n\nImplementation:\n- Add cache_point with type=\"default\" to system prompt parts\n- Concatenate cache_point to tools list \n- Include usage field in response to monitor cache activity\n\nCache behavior:\n- First request = cache_write (stores processed tokens)\n- Subsequent requests = cache_read (retrieves cached tokens)\n- Cache expires after 5 minutes of inactivity\n- Changing cached content triggers new cache_write\n\nUsage monitoring:\n- cache_write_input_tokens = tokens stored to cache\n- cache_read_input_tokens = tokens retrieved from cache\n- Monitor usage field to verify caching effectiveness\n\nBest practices:\n- Cache system prompts (rarely change between requests)\n- Cache tool schemas (static definitions, often >1024 tokens)\n- Minimum 1024 tokens required for caching\n- Cache survives text changes below cache_point\n- Cache invalidated by content changes above cache_point\n\nBenefits = reduced generation cost + faster response times for repeated prompt components\n</note>\n\n<note title=\"Introducing MCP\">\nMCP = Model Context Protocol, communication layer providing Claude with context and tools without requiring developers to write tedious code.\n\nMCP Architecture: Client-server model. Server contains tools, resources, and prompts as internal components.\n\nCore Problem Solved: Eliminates developer burden of authoring, testing, and maintaining numerous tool schemas and functions for external service integrations like GitHub APIs.\n\nMCP Server = Interface to outside service that wraps functionality into pre-built tools. Shifts tool definition and execution from your server to dedicated MCP server.\n\nKey Benefits: Developers no longer author tool schemas and function implementations themselves. MCP server handles GitHub/AWS/other service integrations with ready-made tools.\n\nCommon Questions:\n- Who authors MCP servers? Anyone, but often service providers create official implementations\n- How different from direct API calls? Saves time by eliminating need to author schemas and function implementations yourself\n- MCP vs tool use? They're complementary, not the same. MCP handles who does the work, tool use is the mechanism\n\nEssential Point: MCP servers provide pre-built tools for external services, removing integration development overhead from application developers.\n</note>\n\n<note title=\"MCP Clients\">\nMCP Client = communication layer between your server and MCP server, provides access to server's tools\n\nTransport agnostic = client/server can communicate via multiple protocols (stdio, HTTP, WebSockets, etc.)\n\nCommon setup = MCP client + server on same machine communicating via standard input/output\n\nCommunication = message exchange defined by MCP spec\n\nKey message types:\n- list tools request = client asks server for available tools\n- list tools result = server responds with tool list  \n- call tool request = client asks server to run specific tool with arguments\n- call tool result = server returns tool execution results\n\nTypical flow:\n1. User queries server\n2. Server requests tool list via MCP client\n3. MCP client sends list tools request to MCP server\n4. Server gets tools, sends query + tools to Claude\n5. Claude responds with tool use request\n6. Server asks MCP client to execute tool\n7. MCP client sends call tool request to MCP server\n8. MCP server executes tool (e.g., GitHub API call)\n9. Results flow back: MCP server → MCP client → server → Claude\n10. Claude formulates final response with tool results\n</note>\n\n<note title=\"Project Setup\">\nProject Setup = Building CLI-based chatbot to understand MCP client-server interaction\n\nGoal = Create MCP client + custom MCP server in single project for learning (normally would build only client OR server)\n\nFeatures = Work with fake documents stored in memory, 2 tools (read document contents, update document contents)\n\nSetup steps = Download CLI project.zip starter code, configure environment variables (bedrock region + model ID from previous notebooks), install Python dependencies via uv or pip\n\nRunning = \"uv run main.py\" or \"python main.py\" launches chat prompt\n\nKey note = Educational project combines both client and server development, real projects typically focus on one side only\n</note>\n\n<note title=\"Defining Tools with MCP\">\nMCP tools = functions that language models can call to perform specific actions\n\nMCP Python SDK = official package that simplifies MCP server creation and tool definition without manual JSON schema writing\n\nTool definition syntax = @mcp.tool decorator with name, description, and typed parameters using pydantic Field\n\nTool implementation process:\n1. Use @mcp.tool decorator\n2. Define function with typed parameters  \n3. Add Field descriptions for each parameter\n4. Implement function logic\n5. SDK auto-generates JSON schema\n\nExample tools implemented:\n- read_doc_contents = takes doc_id string, returns document content from in-memory dictionary\n- edit_document = takes doc_id, old_string, new_string, performs find/replace operation\n\nError handling = check if doc_id exists in docs dictionary, raise ValueError if not found\n\nKey advantage = MCP SDK eliminates manual JSON schema creation, significantly simplifying tool development compared to raw schema definitions\n</note>\n\n<note title=\"The Server Inspector\">\nMCP Inspector = in-browser debugger for testing MCP servers without connecting to applications\n\nAccess method: Run \\`mcp dev [server-filename.py]\\` in terminal with activated Python environment → opens server on port → navigate to provided localhost address\n\nKey features:\n- Connect button = starts MCP server\n- Top menu bar = shows resources, prompts, tools sections\n- Tools section = lists available tools from server\n- Right panel = manual tool invocation interface for testing\n\nTesting workflow:\n1. Click Connect to start server\n2. Navigate to Tools → List Tools\n3. Select tool to test\n4. Input required parameters in right panel\n5. Click Run Tool to execute and verify results\n\nPurpose = live development testing of MCP servers without full application integration\n\nNote: UI subject to change during active development, but core functionality remains similar\n</note>\n\n<note title=\"Implementing a Client\">\nMCP Client Implementation:\n\nMCP Client = wrapper class around client session for resource management and cleanup\n\nClient Session = actual connection to MCP server (part of MCP Python SDK)\n\nClient Purpose = expose MCP server functionality to rest of codebase, handle resource cleanup through async enter/exit functions\n\nKey Functions:\n- list_tools() = await self.session.list_tools(), return result.tools\n- call_tool() = await self.session.call_tool(tool_name, tool_input)\n\nTool Schema Conversion = MCP tool definitions differ from Bedrock expectations, requires conversion via to_bedrock_tools() function\n\nTesting = run MCP client directly to verify server connection and tool retrieval\n\nWorkflow = client gets tool list for Claude, executes tools when Claude requests them, manages server connection lifecycle\n\nImplementation Note = typical projects use either client OR server, not both - this project shows both sides for demonstration\n</note>\n\n<note title=\"Defining Resources\">\nMCP Resources = mechanism for MCP servers to expose data to clients for read operations\n\nResource Types:\n- Direct/Static Resources = fixed URI (e.g., \"docs://documents\")\n- Templated Resources = parameterized URI with wildcards (e.g., \"docs://documents/{doc_id}\")\n\nResource Flow:\n1. Client sends read resource request with URI to MCP server\n2. Server matches URI to resource function\n3. Server executes function and returns result\n4. Client receives data in read resource result message\n\nImplementation:\n- Use @mcp.resource decorator with URI and MIME type\n- MIME types: application/json for structured data, text/plain for raw text\n- Templated resource parameters become function keyword arguments\n- Python MCP SDK auto-serializes return values to strings\n\nExample Use Case:\n- List documents resource returns available document names for autocomplete\n- Fetch document resource returns specific document content by ID\n- Enables @ mentions in chat without requiring tool calls\n\nKey Points:\n- One resource per distinct read operation\n- Resources expose data, tools perform actions\n- Client handles deserialization based on MIME type\n- Error handling with exceptions (e.g., ValueError for missing documents)\n</note>\n\n<note title=\"Accessing Resources\">\nMCP Resource Access = reading resources from MCP server by URI\n\nFunction Implementation:\n- read_resource(uri) = main client function to fetch and parse resources\n- Uses session.read_resource(AnyURL(uri)) to make server request\n- Takes result.contents[0] = first resource from response list\n\nContent Parsing Logic:\n- Check resource.mime_type property from server response\n- If mime_type == \"application/json\": return json.loads(resource.text)\n- Otherwise: return resource.text as plain text\n\nIntegration Flow:\n- MCP client function called by other application components\n- Client requests resource list from server\n- User selects resource via CLI interface\n- Resource content inserted into prompt and sent to Claude\n- No tool calls needed since content already retrieved\n\nKey Point: Resources expose server information to clients through standardized URI-based access with automatic content-type parsing.\n</note>\n\n<note title=\"Defining Prompts\">\nMCP Prompts = pre-defined, tested prompts exposed by servers for specialized tasks\n\nPurpose = Allow server developers to create high-quality, domain-specific prompts that clients can use instead of users writing ad-hoc prompts\n\nImplementation = Use @prompt decorator with name/description, return list of messages (user/assistant format)\n\nKey Benefits = Better results than user-written prompts, reusable across clients, specialized for server's domain\n\nExample Use Case = Document formatting prompt that instructs Claude to read document via tools, reformat to markdown, and save changes\n\nStructure = Prompt function receives parameters (like document ID), returns base.UserMessage objects with interpolated prompt text\n\nTesting = Use MCP development inspector to list prompts, select prompt, enter parameters, get formatted message list ready for LLM\n\nCore Concept = Server authors create optimized prompts for their domain (document management, etc.) that any client can leverage without prompt engineering\n</note>\n\n<note title=\"Prompts in the Client\">\n**Prompts in MCP Client Implementation**\n\n**Core Functions:**\n- list_prompts() = await self.session.list_prompts(), return result.props\n- get_prompt() = await self.session.get_prompt(prompt_name, arguments), return result.messages\n\n**Prompt Flow:**\n1. Define prompts in MCP server with argument placeholders (e.g., document_id)\n2. Client requests prompt by name + provides arguments dictionary\n3. Server interpolates arguments into prompt template\n4. Returns formatted messages for LLM consumption\n\n**Key Concepts:**\n- Prompts = templates with variable placeholders for dynamic content\n- Arguments = key-value pairs passed to prompt functions as keyword arguments\n- result.messages = conversation format fed directly to LLM (Claude)\n- CLI integration shows prompts as commands with argument selection\n\n**Example Usage:**\nformat_document prompt expects document_id argument → Client provides document_id → Server returns formatted prompt with document reference → LLM uses prompt + tools to complete task\n\n**Purpose:** Allows clients to retrieve server-defined prompt templates with runtime variable substitution for consistent LLM interactions.\n</note>\n\n<note title=\"MCP Review\">\nMCP Server Primitives = 3 types with distinct control patterns and purposes\n\nTools = Model-controlled. Claude decides when to execute. Purpose: Add capabilities to Claude. Example: JavaScript execution for calculations.\n\nResources = App-controlled. Application code decides when to fetch/use data. Purpose: Provide data to UI or augment prompts. Example: Auto-complete options, document content injection.\n\nPrompts = User-controlled. User triggers via UI buttons, slash commands, etc. Purpose: Predefined workflows. Example: Chat starter buttons in Claude interface.\n\nControl Pattern Summary:\n- Tools serve the model\n- Resources serve the app  \n- Prompts serve users\n\nUsage Guidelines:\n- Need Claude capabilities → implement tools\n- Need app data/UI content → use resources\n- Need predefined workflows → create prompts\n\nReal Examples in Claude.ai:\n- Buttons below chat input = prompts\n- Google Drive document listing = resources\n- Automatic code execution = tools\n</note>\n\n<note title=\"Claude Code in Action\">\nClaude Code = AI coding assistant that functions as another engineer on your team, not just a code generator\n\nKey workflow:\n1. Download project zip, extract, open in editor\n2. Run \\`claude\\` command in terminal\n3. Ask Claude to read README and execute setup directions\n4. Run \\`init\\` command - Claude scans codebase, creates claude.md file with project architecture/coding style notes\n5. claude.md automatically included as context in future requests\n\nThree memory types: project, local, user\n- Use # + note to append specific directions to claude.md\n- Can manually edit claude.md or rerun init to update\n\nEffort multiplier principle = small effort in directing Claude yields significantly better results\n\nTwo effective workflows for complex features:\n\nWorkflow 1 - Three-step approach:\n1. Identify relevant files, ask Claude to read/analyze them\n2. Describe feature, ask Claude to plan solution (no code yet)\n3. Ask Claude to implement the plan\n\nWorkflow 2 - Test-driven development:\n1. Ask Claude to read relevant context\n2. Ask Claude to suggest tests for feature (no implementation)\n3. Select most relevant tests, ask Claude to implement them\n4. Ask Claude to write code until tests pass\n\nExample feature: document-to-markdown conversion tool\n- Reads PDF/Word documents from file path\n- Converts to markdown format\n- Includes error handling for unsupported files\n\nCommands:\n- \\`claude\\` = launch Claude Code\n- \\`init\\` = scan codebase, generate claude.md\n- \\`/clear\\` = reset conversation history\n- \\`# [note]\\` = append note to claude.md\n</note>\n\n<note title=\"Enhancements with MCP Servers\">\nMCP Servers in Claude Code = embedded MCP client allows connecting external MCP servers to expand functionality dynamically\n\nSetup Process:\n- Command: \\`claude mcp add [server-name] [startup-command]\\`\n- Example: \\`claude mcp add documents \"uv run main.py\"\\`\n- Restart Claude Code with \\`claude\\` command\n\nDemonstration = Document Path to Markdown tool allows Claude Code to read PDF/Word documents, convert to Markdown format\n\nUse Cases:\n- Sentry MCP server = fetch production error details\n- Jira MCP server = view ticket contents  \n- Slack MCP server = send completion notifications\n- Custom development workflow integrations\n\nKey Benefit = Dramatically expands Claude Code capabilities through modular MCP server connections, enabling specialized functionality for specific development needs\n</note>\n\n<note title=\"Parallelizing Claude Code\">\nParallelizing Claude Code = running multiple Claude instances simultaneously to complete different tasks in parallel, allowing one developer to manage multiple virtual software engineers.\n\nCore Challenge = file conflicts when multiple Claude instances modify same files simultaneously.\n\nSolution = Git work trees. Each Claude instance gets isolated workspace with complete project copy on separate branch.\n\nGit Work Trees = Git feature creating total project copies in separate directories, each corresponding to different branch. Enables complete isolation between parallel tasks.\n\nWorkflow = create work tree → assign task to Claude instance → work in isolation → commit changes → merge back to main branch.\n\nAutomation via Custom Commands = create .claude/commands directory with markdown files containing prompts. Use $ARGUMENTS placeholder for dynamic inputs. Access via /project:command-name syntax.\n\nImplementation Steps:\n1. Create multiple work trees for different features\n2. Launch Claude in each work tree with specific task\n3. Let instances work in parallel\n4. Commit completed work in each branch  \n5. Merge all branches back to main (Claude can handle merge conflicts)\n6. Clean up work trees\n\nBenefits = massive productivity increase, scales to as many parallel instances as developer can manage. Claude handles Git operations including conflict resolution.\n\nKey Insight = delegate entire workflow management to Claude itself, including work tree creation, merging, and cleanup.\n</note>\n\n<note title=\"Automated Debugging\">\nAutomated Debugging = using AI agents to automatically detect, diagnose, and fix production errors without manual intervention.\n\nCore workflow: GitHub Action runs daily → pulls CloudWatch logs → filters/deduplicates errors → Claude analyzes each error → generates fixes → commits changes → opens pull request for review.\n\nKey components:\n- Error detection: Automated log monitoring from production systems\n- Error analysis: AI examines stack traces and error messages \n- Fix generation: AI writes code corrections based on error context\n- Integration: Commits fixes and creates PRs automatically\n\nExample case: Production app failing silently (worked locally) → AI found \"invalid model identifier\" error in logs → identified typo in model ID → corrected configuration → submitted fix via PR.\n\nBenefits: Catches production-only issues, reduces manual log hunting, provides explained fixes with context, maintains code review process through PRs.\n\nImplementation: Requires CI/CD pipeline, log access (CloudWatch/similar), AI coding assistant integration, version control automation.\n\nLimitation: Still requires human review of proposed fixes before merging.\n</note>\n\n<note title=\"Computer Use\">\nComputer Use = Claude's ability to directly interact with and control computer interfaces through visual input and actions.\n\nKey capabilities:\n- Takes screenshots to see current screen state\n- Performs mouse clicks, keyboard input, scrolling\n- Navigates web browsers and applications\n- Executes multi-step workflows autonomously\n\nPrimary use cases:\n- QA testing automation - Claude can run test cases, identify UI bugs, generate reports\n- Web application testing - navigates sites, fills forms, validates functionality\n- General computer task automation\n\nImplementation:\n- Runs in isolated Docker container for security\n- Uses chat interface for instruction input\n- Processes visual feedback to determine next actions\n- Provides detailed reports on task completion\n\nBenefits:\n- Saves developer time on repetitive testing\n- Identifies edge cases and failures automatically\n- Handles complex multi-step processes\n- Works with existing applications without modification\n\nThe feature essentially turns Claude into an automated user that can perform any task a human could do through a computer interface.\n</note>\n\n<note title=\"How Computer Use Works\">\nTool use flow = User sends request with tool schema to Claude → Claude responds with tool use part (ID, name, input) → Server executes code and returns result → Claude receives tool result.\n\nComputer use = Special case of tool use system. Uses minimal schema that expands into large structure describing action function with arguments (mouse move, left click, screenshot, etc.).\n\nImplementation = Docker container simulates computing environment, executes programmatic key presses/mouse movements based on Claude's tool requests. Claude doesn't directly manipulate computer - relies on external execution environment.\n\nKey point = Claude decides to use tools but developers must provide actual tool execution (weather API, Docker environment, etc.).\n\nSetup = Docker + local AWS profile + simple Docker command. Anthropic provides reference implementation with pre-built container for testing computer use functionality.\n</note>\n\n<note title=\"Qualities of Agents\">\nAgent = language model with tool access executed repeatedly until goal achieved\n\nKey Agent Qualities:\n- Tool-centric operation = agents use tools in loops until goal/error\n- Information gathering focus = most tool calls gather environment data vs modify it\n- Context dependency = agents rely on tools for environment inspection, not RAG/detailed prompts\n- Focused toolset = small set of tools with clear purposes\n- High-value, low-risk tasks = valuable work where mistakes don't cause major economic/safety damage\n\nTool Usage Patterns:\n- Claude Code = reads files (info gathering) + writes files (modification) + runs tests\n- Computer Use = screenshots provide automatic visual feedback of environment state\n- Primary info source = tool calls rather than pre-written knowledge\n\nAgent Design Principles:\n- Context is king = LM has no inherent world knowledge, depends entirely on tool-provided environment data\n- Evaluation critical = EVALS necessary to ensure agent effectiveness\n- Risk assessment = suitable for high-value tasks with manageable failure costs\n\nCore Loop: Tool execution → Environment inspection → Goal progress → Repeat until success/failure\n</note>\n</notes>",
  "claudecode": "<notes>\n<critical>\nBelow are notes from a video course about working with the Claude language model.\nUse these notes as a resource to answer the user's question.\nWrite your answer as a standalone response - do not refer directly to these notes unless specifically requested by the user.\n</critical>\n<note title=\"What is a Coding Assistant?\">\nCoding Assistant = tool that uses language models to write code and complete development tasks\n\nCore Process:\n1. Receives task (e.g., fix bug from error message)\n2. Language model gathers context (reads files, understands codebase)\n3. Formulates plan to solve issue\n4. Takes action (updates files, runs tests)\n\nKey Limitation: Language models only process text input/output - cannot directly read files, run commands, or interact with external systems.\n\nTool Use System = method enabling language models to perform actions:\n- Assistant appends instructions to user request\n- Instructions specify formatted responses for actions (e.g., \"read file: filename\")\n- Language model responds with formatted action request\n- Assistant executes actual action (reads file, runs command)\n- Results sent back to language model for final response\n\nClaude Models Advantage:\n- Superior tool use capabilities vs other language models\n- Better at understanding tool functions and combining them for complex tasks\n- Claude Code is extensible - easy to add new tools\n- Better security through direct code search vs indexing that sends codebase to external servers\n\nEssential Points:\n- All language models require tool use for non-text generation tasks\n- Tool use quality directly impacts coding assistant effectiveness\n- Claude's strength in tool use makes it adaptable to development changes\n</note>\n\n<note title=\"Claude Code in Action\">\nClaude Code = AI assistant with tool-based capabilities for code tasks\n\nDefault tools = file reading/writing, command execution, basic development operations\n\nPerformance optimization demo: Claude analyzed Chalk JavaScript library (5th most downloaded JS package, 429M weekly downloads). Used benchmarks, profiling tools, created todo lists, identified bottlenecks, implemented fixes. Result = 3.9x throughput improvement.\n\nData analysis demo: Claude performed churn analysis on video streaming platform CSV data using Jupyter notebooks. Executed code cells iteratively, viewed results, customized successive analyses based on findings.\n\nTool extensibility: Claude Code accepts new tool sets. Example used Playwright MCP server for browser automation. Claude opened browser, took screenshots, updated UI styling, iterated on design improvements.\n\nGitHub integration: Claude Code runs in GitHub Actions, triggered by pull requests/issues. Gets GitHub-specific tools (comments, commits, PR creation). \n\nInfrastructure review example: Terraform-defined AWS infrastructure with DynamoDB table and S3 bucket shared with external partner. Developer added user email to Lambda function output. Claude Code automatically detected PII exposure risk in pull request review by analyzing infrastructure flow and identifying external data sharing.\n\nKey principle: Claude Code = flexible assistant that grows with team needs through tool expansion rather than fixed functionality.\n</note>\n\n<note title=\"Adding Context\">\nContext management = critical for Claude Code effectiveness. Too much irrelevant info decreases performance.\n\n/init command = analyzes entire codebase on first run, creates Claude.md file with project summary/architecture/key files. File contents included in every request.\n\nThree Claude.md file types:\n- Project level = shared with team, committed to source control\n- Local level = personal instructions, not committed  \n- Machine level = global instructions for all projects\n\nMemory mode (# symbol) = edit Claude.md files intelligently with natural language requests\n\n@ symbol = mention specific files to include in requests, provides targeted context instead of letting Claude search\n\nBest practice = reference critical files (like database schemas) in Claude.md so they're always available as context\n\nGoal = provide just enough relevant information for Claude to complete tasks effectively\n</note>\n\n<note title=\"Making Changes\">\nClaude Code Change Management:\n\nScreenshot integration = Control-V (not Command-V on macOS) pastes screenshots to help Claude understand specific UI elements to modify\n\nPerformance boosting modes:\n- Plan Mode = Shift + Tab twice, makes Claude research more files and create detailed implementation plans before executing\n- Thinking Mode = triggered by phrases like \"Ultra think\", gives Claude extended reasoning budget for complex logic\n\nPlanning vs Thinking usage:\n- Planning = handles breadth, useful for multi-step tasks requiring wide codebase understanding\n- Thinking = handles depth, useful for tricky logic or debugging specific issues\n- Can be combined for complex tasks\n- Both consume additional tokens (cost consideration)\n\nGit integration = Claude Code can stage/commit changes and write descriptive commit messages\n\nKey workflow: Screenshot problematic area → paste with Control-V → describe desired change → optionally enable Plan/Thinking modes for complex tasks → review and accept implementation\n</note>\n\n<note title=\"Controlling Context\">\nContext Control Techniques:\n\nEscape = Stops Claude mid-response to redirect conversation flow. Press once to interrupt current output.\n\nEscape + Memory = Powerful error prevention. Stop Claude, add memory about repeated mistakes using # shortcut to prevent future occurrences.\n\nDouble Escape = Conversation rewind. Shows all previous messages, allows jumping back to earlier point while maintaining relevant context and skipping irrelevant debugging/back-and-forth.\n\nCompact Command = Summarizes entire conversation history while preserving Claude's learned knowledge about current task. Use when Claude has gained expertise but conversation has accumulated clutter.\n\nClear Command = Deletes entire conversation history for fresh start. Use when switching to completely unrelated tasks.\n\nKey Benefits: Maintains focus, reduces distracting context, preserves relevant knowledge, prevents repeated errors. Most effective for long conversations and task transitions.\n</note>\n\n<note title=\"Custom Commands\">\nCustom Commands = user-defined automation commands in Claude Code accessed via forward slash\n\nLocation = .Claude/commands/ folder in project directory\nFile naming = filename becomes command name (audit.md creates /audit command)\nActivation = restart Claude Code after creating command files\n\nCommand structure = markdown file containing instructions for Claude to execute\nArguments = use $arguments placeholder in command text to accept runtime parameters\nArgument types = any string (file paths, descriptive text, etc.)\n\nUse cases = automating repetitive tasks like dependency auditing, test generation, vulnerability fixes\nExecution = /commandname in Claude Code interface, optionally followed by argument string\n</note>\n\n<note title=\"Extending Claude Code with MCP Servers\">\nMCP servers = external tools that extend Claude Code capabilities, run locally or remotely.\n\nPlaywright MCP server = popular server enabling Claude to control browsers for web automation.\n\nInstallation: Terminal command \\`claude mcp add [name] [start-command]\\` adds MCP server to Claude Code.\n\nPermission management: Initial tool usage requires approval. Auto-approve by adding \"MCP__[servername]\" to settings.local.json allow array.\n\nPractical example: Claude used Playwright to navigate localhost:3000, generate UI component, analyze styling quality, then automatically update generation prompts based on visual feedback.\n\nResults: Automated prompt refinement produced significantly better component styling, demonstrating MCP servers unlock sophisticated development workflows.\n\nKey benefit: MCP servers enable Claude to perform complex multi-step tasks involving external systems, expanding beyond code editing to full development automation.\n</note>\n\n<note title=\"Github Integration\">\nClaude Code GitHub Integration = official integration allowing Claude to run inside GitHub actions\n\nSetup Process:\n- Run \"/install GitHub app\" command\n- Install Claude Code app on GitHub\n- Add API key\n- Auto-generated pull request adds two GitHub actions\n\nDefault Actions:\n1. Mention support = @Claude in issues/PRs to assign tasks\n2. PR review = automatic code review on new pull requests\n\nCustomization:\n- Actions are customizable via config files in .github/workflows directory\n- Custom instructions = direct context/directions passed to Claude\n- MCP server integration = allows Claude to access external tools (like Playwright for browser automation)\n\nPermission Requirements:\n- Must explicitly list all permissions for Claude Code in actions\n- MCP server tools require individual permission listing (no shortcuts)\n\nExample Use Case:\n- Integrated Playwright MCP server for browser testing\n- Development server setup before Claude runs\n- Claude can visit app in browser, test functionality, create checklists\n- Provides automated testing and issue verification\n\nKey Features = mention-based task assignment, automated PR reviews, customizable workflows, MCP server integration for extended functionality\n</note>\n\n<note title=\"Introducing Hooks\">\nHooks = commands that run before/after Claude executes tools\n\nPre-tool use hooks = run before tool execution, can inspect and block tool operations, send error messages to Claude\nPost-tool use hooks = run after tool execution, perform follow-up operations, provide feedback to Claude\n\nConfiguration = added to Claude settings file (global/project/personal) via manual editing or /hooks command\n\nHook structure = two sections (pre-tool use, post-tool use), each with matcher (specifies which tools to target) and commands to execute\n\nExample uses = auto-format files after creation, run tests after edits, block file access, code quality checks, type checking\n\nHook commands = receive tool call details, can modify Claude's workflow through blocking or feedback mechanisms\n</note>\n\n<note title=\"Defining Hooks\">\n**Hooks Overview**\nHooks = mechanisms to intercept and control tool calls before/after execution\n\n**Hook Types**\nPre-tool use hook = executes before tool call, can block execution\nPost-tool use hook = executes after tool call, cannot block execution\n\n**Hook Implementation Process**\n1. Choose hook type (pre vs post)\n2. Identify target tool names to monitor\n3. Write command to receive tool call data via stdin as JSON\n4. Parse JSON containing tool_name and input parameters\n5. Exit with appropriate code to signal intent\n\n**Exit Codes**\nExit 0 = allow tool call to proceed\nExit 2 = block tool call (pre-tool use only)\nStandard error output = feedback message sent to Claude when blocking\n\n**Tool Call Data Structure**\nJSON object containing:\n- tool_name (e.g., \"read\", \"grep\")\n- input parameters (e.g., file_path)\n\n**Common Use Case**\nBlocking file access by monitoring \"read\" and \"grep\" tools that can access file contents\n\n**Tool Discovery**\nAsk Claude directly for list of available tool names rather than memorizing them\n</note>\n\n<note title=\"Defining Hooks\">\nHooks = mechanisms to control Claude's tool usage by running custom commands before/after tool calls\n\nPre-tool use hook = executes before tool call, can block with exit code 2\nPost-tool use hook = executes after tool call, cannot block\n\nHook process:\n1. Claude sends tool call data as JSON via stdin to your command\n2. Command parses JSON containing tool_name and input arguments  \n3. Command exits with code 0 (allow) or 2 (block for pre-hooks only)\n4. Exit code 2 sends stderr output as feedback to Claude\n\nTool call data format = JSON object with tool name and input parameters\n\nCommon tools that read files = \"read\" tool and \"grep\" tool\n\nHook use case example = blocking Claude from reading sensitive .env file by watching for read/grep tools targeting that file path\n\nSetup = define command in project, Claude automatically executes it when relevant tool calls occur\n</note>\n\n<note title=\"Implementing a Hook\">\n**Custom Hook Implementation**\n\nHook purpose = prevent Claude from reading .env file contents\n\n**Configuration Setup**\n- Location = .clod/settings.local.json\n- Hook type = pre-tool use hook (blocks before execution)\n- Matcher = \"read|grep\" (pipe symbol separates tool names)\n- Command = \"node ./hooks/read_hook.js\"\n\n**Implementation Details**\n- Hook receives JSON object via stdin containing: session ID, tool name, tool input, file path\n- Logic: if file path includes \".env\" → exit with code 2 + log error to stderr\n- Error output goes to stderr for Claude feedback\n- Exit code 2 = blocked operation\n\n**Key Requirements**\n- Must restart Claude after hook changes\n- Console.error() sends feedback to Claude via stderr\n- Hook works for both read and grep tools\n- File path checking: tool_input.path with fallback handling\n\n**Testing Results**\n- Successfully blocks .env file access\n- Claude recognizes prevention by read hook\n- Works for both read and grep operations\n</note>\n\n<note title=\"Implementing a Hook\">\n**Hook Implementation Process:**\n\nHook = custom script that intercepts and controls tool usage in Clod\n\n**Configuration (settings.local.json):**\n- Pre-tool use hooks = run before tool execution\n- Post-tool use hooks = run after tool execution\n- Matcher = specifies which tools to intercept (e.g., \"read|grep\")\n- Command = script to execute when matched tools are called\n\n**Implementation Steps:**\n1. Add hook config to settings.local.json with matcher and command\n2. Create hook script (e.g., read_hook.js) that receives JSON input via stdin\n3. JSON input contains: session ID, tool name, tool input, file path\n4. Script logic: check if file path includes \".env\"\n5. If blocked file detected: console.error() message + process.exit(2)\n6. Exit code 2 = blocks tool execution\n\n**Key Technical Details:**\n- Hook script receives tool data as JSON from stdin\n- Use console.error() to send feedback to Clod (logs to stderr)\n- Must restart Clod after hook changes\n- Hook applies to all specified tools (read, grep, etc.)\n- Fallback path checking via tool_input.path for compatibility\n\n**Result:** Successfully prevents Clod from reading .env files while providing user feedback about blocked operations.\n</note>\n\n<note title=\"Useful Hooks!\">\n**Useful Hooks for Claude Code Projects**\n\n**Problem**: Claude Code often misses type errors and creates duplicate code, especially in larger projects.\n\n**Hook 1: TypeScript Type Checker Hook**\n- **Purpose**: Catch type errors immediately after file edits\n- **Implementation**: Run \\`tsc --no-emit\\` after TypeScript file changes via post-tool-use hook\n- **Process**: Detects type errors → feeds errors back to Claude → Claude fixes call sites automatically\n- **Benefits**: Prevents broken function calls when signatures change\n- **Adaptable**: Works for any typed language with type checker, or use tests for untyped languages\n\n**Hook 2: Duplicate Code Prevention Hook**\n- **Problem**: Claude creates new queries/functions instead of reusing existing ones, especially in complex tasks\n- **Solution**: Launch separate Claude instance to review changes in specific directories (e.g., queries folder)\n- **Process**: \n  1. Detect edits to watched directory\n  2. Launch new Claude instance via TypeScript SDK\n  3. Compare new code against existing code\n  4. If duplicate found, exit with code 2 + feedback\n  5. Original Claude receives feedback and reuses existing code\n- **Trade-offs**: Extra time/cost vs cleaner codebase\n- **Recommendation**: Only watch critical directories to minimize overhead\n\n**Key Takeaway**: Hooks = automated feedback loops that catch common Claude Code weaknesses (type errors, code duplication) by running additional checks and feeding results back to Claude for self-correction.\n</note>\n\n<note title=\"Useful Hooks!\">\nTypeScript Type Checker Hook:\n- Problem: Claude edits function signatures but doesn't update call sites, causing type errors\n- Solution: Post-tool-use hook that runs \\`tsc --no-emit\\` after TypeScript file edits\n- Process: Detects type errors → feeds errors back to Claude → Claude fixes call sites automatically\n- Works for any typed language with type checker, or untyped languages using tests instead\n\nQuery Deduplication Hook:\n- Problem: Claude creates duplicate SQL queries/functions instead of reusing existing ones, especially in complex tasks\n- Cause: Focused tasks work well, but wrapped/complex tasks make Claude lose focus\n- Solution: Hook monitors query directory changes → launches separate Claude instance → reviews for duplicates → provides feedback to original Claude\n- Process: File edit in queries/ → secondary Claude analyzes existing queries → reports duplicates → primary Claude removes duplicate and reuses existing code\n- Trade-off: Additional time/cost vs cleaner codebase with less duplication\n- Recommendation: Only apply to critical directories to minimize overhead\n\nBoth hooks use post-tool-use pattern to provide immediate feedback and course correction to Claude's edits.\n</note>\n\n<note title=\"The Claude Code SDK\">\nClaude Code SDK = programmatic interface for Claude Code with CLI, TypeScript, and Python libraries. Contains same tools as terminal version.\n\nPrimary use case = integration into larger pipelines/workflows to add intelligence to existing processes.\n\nDefault permissions = read-only (files, directories, grep operations). Write permissions require manual configuration via options.allowTools array or .Claude directory settings.\n\nSDK execution shows raw conversation between local Claude Code and language model, with final response as last message.\n\nKey implementation pattern = add write permissions by specifying tools like \"edit\" in options.allowTools when making query calls.\n\nBest suited for = helper commands, scripts, and hooks within existing projects rather than standalone usage.\n</note>\n\n<note title=\"The Claude Code SDK\">\nClaude Code SDK = programmatic interface to use Claude Code via CLI, TypeScript, or Python libraries. Same tools as terminal version.\n\nPrimary use case = integration into larger pipelines/workflows to add intelligence to processes.\n\nKey characteristics:\n- Default permissions = read-only (files, directories, grep operations)\n- Write permissions = must be manually enabled via query options or settings file\n- Raw conversation output = shows message-by-message exchange between local Claude Code and language model\n\nBest applications = helper commands, scripts, hooks within existing projects rather than standalone use.\n\nOutput format = conversational messages with final response from Claude as last message.\n</note>\n</notes>",
  "mcp_advanced": "<notes>\n<critical>\nBelow are notes from a video course about working with the Claude language model.\nUse these notes as a resource to answer the user's question.\nWrite your answer as a standalone response - do not refer directly to these notes unless specifically requested by the user.\n</critical>\n<note title=\"Sampling\">\nSampling = technique allowing MCP servers to request language model text generation from clients instead of directly accessing LLMs themselves.\n\nPurpose = shifts LLM access responsibility from server to client, avoiding need for servers to handle API keys, authentication, or token costs.\n\nArchitecture = Server creates message request → Client receives via sampling callback → Client calls LLM → Client returns generated text to server.\n\nBenefits = eliminates server complexity for LLM integration, removes API key requirements from servers, prevents unauthorized token usage on public servers.\n\nImplementation = Server uses create_message() function with message list, Client implements sampling callback to handle LLM requests and return create_message_result.\n\nPrimary use case = publicly accessible MCP servers that need LLM capabilities without direct LLM access or associated costs/security concerns.\n</note>\n\n<note title=\"Log and Progress Notitications\">\nLog and Progress Notifications = MCP server feature that provides real-time feedback during tool execution to improve user experience.\n\nImplementation on server side:\n- Tool functions automatically receive context argument as last parameter\n- Context object provides methods: info() for logging, report_progress() for progress updates\n- Calling these methods automatically sends messages back to client\n\nImplementation on client side:\n- Create callback function for logging statements\n- Create separate callback for progress updates\n- Pass logging callback to client session\n- Pass progress callback to call_tool function\n- Callbacks handle how to display information to user (terminal output, web UI, etc.)\n\nKey benefits:\n- Prevents user confusion about stalled/failed tool calls\n- Provides visibility into long-running operations\n- Real-time feedback during tool execution\n\nOptional feature = can be omitted if not needed, purely for UX enhancement.\n</note>\n\n<note title=\"Roots\">\nMCP Roots = codified way for users to grant server access to specific files/folders\n\nProblem without roots: User says \"convert bikin.mp4\" but Claude can't locate file in complex filesystem without full path. Requiring full paths inconvenient for users.\n\nSolution with roots: Add 3 tools to MCP server:\n- ConvertVideo (original tool)\n- ReadDirectory (lists files/folders in directory)  \n- ListRoots (returns available roots)\n\nRoot = file/folder user grants permission to access beforehand (via command line args when starting server)\n\nImplementation requirement: Tools must check that accessed files/folders are contained within granted roots using function like is_path_allowed()\n\nTwo main benefits:\n1. Permission control - limits server access to authorized areas only\n2. Autonomous discovery - Claude can search through available roots to find files without user providing full paths\n\nKey limitation: MCP SDK doesn't automatically enforce root restrictions. Server developer must implement access checks manually.\n\nListRoots tool optional - can alternatively include root list in prompt directly. Tool pattern allows Claude to dynamically check available roots when needed.\n</note>\n\n<note title=\"JSON Message Types\">\nJSON Message Types in MCP:\n\nMCP communication = JSON messages between clients and servers. Each message type has distinct purpose.\n\nMessage categories:\n- Request/Result pairs = Always come together (call_tool_request + call_tool_result, initialize_request + initialize_result)\n- Notifications = Events that don't need responses (progress_notification, logging_message_notification, tool_change_notification)\n\nMessage direction classification:\n- Client messages = Sent by MCP client to server\n- Server messages = Sent by MCP server to client\n\nKey insight: Servers can send messages TO clients (server requests, server notifications). This directional capability becomes critical limitation in streamable HTTP transport.\n\nSchema definition = TypeScript file in MCP spec repository (schema.ts). Not executable code, just type descriptions for convenience.\n\nMessage structure = JSON-RPC format with method, params, ID fields.\n</note>\n\n<note title=\"The Stdio Transport\">\nMCP Transport = mechanism for moving JSON messages between client and server\n\nStdio Transport = transport where client launches server as separate process, communicates via standard input/output streams\n\nCommunication mechanism: Client writes to server's stdin, reads from server's stdout. Server writes to stdout, reads from stdin.\n\nAdvantages: Bidirectional communication - either client or server can initiate requests at any time\n\nLimitations: Only works when client and server run on same physical machine\n\nMessage exchange patterns:\n- Client-to-server request: Write to stdin, read response from stdout\n- Server-to-client request: Server writes to stdout, client responds via stdin\n\nRequired initialization sequence:\n1. Initialize request (client to server)\n2. Initialize result (server to client) \n3. Initialize notification (client to server, no response required)\n\nMessage types:\n- Requests = expect responses\n- Notifications = no response required\n- Results = responses to requests\n\nKey characteristic: Full bidirectional communication support - both parties can initiate requests\n\nContrast with HTTP transport: HTTP transport has limitations on server-initiated requests that stdio transport doesn't have\n</note>\n\n<note title=\"The StreamableHTTP Transport\">\nStreamableHTTP Transport = MCP transport enabling client-server communication over HTTP connections, allows remote server hosting unlike standard I/O transport which requires same-machine operation.\n\nKey advantage = Remote hosting capability - servers can be publicly accessible at URLs like mcpserver.com, expanding MCP server possibilities.\n\nCritical limitation = Restricted server-to-client messaging functionality due to HTTP's unidirectional nature - clients easily request from servers, but servers cannot easily initiate requests to clients.\n\nTwo key settings impact functionality:\n- stateless HTTP (default: false)\n- JSON response (default: false)\n\nSetting these to true = Reduced functionality, breaks progress bars, logging notifications, progress notifications, and sampling requests.\n\nHTTP communication constraint = Server doesn't know client address and client may not be publicly accessible, making server-initiated requests challenging.\n\nAffected message types when using HTTP = Sampling requests, listing routes, progress notifications, logging notifications - all require server-to-client communication.\n\nCommon deployment issue = Application works fine locally with standard I/O transport but fails when deployed with HTTP transport due to these messaging restrictions.\n\nSolution exists = StreamableHTTP transport has workarounds for server-to-client communication challenges, but with caveats.\n</note>\n\n<note title=\"StreamableHTTP in Depth\">\nStreamableHTTP Transport = HTTP-based MCP transport using server-sent events (SSE) to enable server-to-client communication\n\nCore Problem: MCP requires server-to-client requests (sampling, notifications, logging) but HTTP naturally supports only client-to-server requests\n\nWorkaround Solution: Uses SSE connections to allow server streaming messages to client\n\nSession ID = Random identifier assigned during initialization, included in all subsequent requests as HTTP header\n\nInitialization Flow:\n1. Client sends initialize request\n2. Server responds with result + MCP session ID header\n3. Client sends initialized notification with session ID\n4. Client optionally makes GET request with session ID to establish SSE connection\n\nTwo SSE Connections:\n1. Long-lived SSE connection = For server-initiated requests (sampling, notifications)\n2. Short-lived SSE connection = For specific tool call responses, automatically closed after result\n\nMessage Routing:\n- Progress notifications → Long-lived SSE connection\n- Logging messages + tool results → Short-lived SSE connection tied to specific request\n\nKey Limitation: Setting certain flags to true breaks the workaround, making StreamableHTTP complex to understand and use properly\n\nCritical Point: SSE responses enable bidirectional communication over HTTP by keeping connections open and streaming individual messages from server to client\n</note>\n\n<note title=\"Stateless HTTP\">\n**Stateless HTTP Flag**\n\nStateless HTTP = flag set to true when MCP server needs horizontal scaling across multiple instances with load balancer\n\n**Why needed**: Single server instance can't handle high traffic. Horizontal scaling uses multiple server copies + load balancer routing requests randomly.\n\n**Problem without stateless**: Client needs 2 connections (GET SSE for server-to-client requests, POST for client-to-server). Load balancer may route these to different server instances. If tool on Server A needs sampling request, it must go through GET SSE connection on Server B - requires complex coordination.\n\n**Effect of stateless=true**:\n- No session IDs assigned to clients\n- Server cannot track individual clients\n- GET SSE response pathway disabled (server cannot send requests to client)\n- Eliminates sampling, progress logging, resource subscriptions\n- No client initialization required (skips initialize request + notification)\n- Reduces server traffic\n\n**JSON Response Flag**\n\nJSON response = flag disabling streaming responses on POST requests\n\n**Effect of JSON response=true**:\n- POST responses return final result as plain JSON only\n- No intermediate streaming messages\n- No progress/log statements during execution\n- Client waits for complete tool execution before receiving response\n\n**Key Takeaway**: Both flags significantly change server behavior. Use same transport in development as planned for production to avoid deployment issues.\n</note>\n</notes>",
  "mcp_intro": "<notes>\n<critical>\nBelow are notes from a video course about working with the Claude language model.\nUse these notes as a resource to answer the user's question.\nWrite your answer as a standalone response - do not refer directly to these notes unless specifically requested by the user.\n</critical>\n<note title=\"Introducing MCP\">\nMCP = Model Context Protocol, communication layer providing Claude with context and tools without requiring developers to write tedious code.\n\nCore Architecture: MCP client connects to MCP server. MCP server contains tools, resources, and prompts as internal components.\n\nProblem Solved: Traditional approach requires developers to manually author tool schemas and functions for each service integration (like GitHub API tools). This creates maintenance burden for complex services with many features.\n\nMCP Solution: Shifts tool definition and execution from developer's server to dedicated MCP server. MCP server = interface to outside service, wrapping functionality into pre-built tools.\n\nKey Benefits: Eliminates need for developers to write/maintain tool schemas and function implementations. Someone else authors the tools, packages them in MCP server.\n\nCommon Questions:\n- Who authors MCP servers? Anyone, but often service providers create official implementations\n- Difference from direct API calls? Saves developer time by providing pre-built tool schemas/functions instead of manual authoring\n- Relationship to tool use? MCP and tool use are complementary, not identical. MCP focuses on who does the work of creating tools\n\nCore Value: Reduces developer burden by outsourcing tool creation to MCP server implementations rather than requiring custom tool development for each service integration.\n</note>\n\n<note title=\"MCP Clients\">\nMCP Client = communication interface between your server and MCP server, provides access to server's tools.\n\nTransport agnostic = client/server can communicate via multiple protocols (stdin/stdout, HTTP, WebSockets, etc). Common setup: both on same machine using stdin/stdout.\n\nCommunication = message exchange defined by MCP spec. Key message types:\n- list tools request/result = client asks server for available tools, server responds with tool list\n- call tool request/result = client asks server to run tool with arguments, server returns execution result\n\nTypical flow: User query → Server asks MCP client for tools → MCP client sends list tools request to MCP server → Server gets tool list → Server sends query + tools to Claude → Claude requests tool execution → Server asks MCP client to run tool → MCP client sends call tool request to MCP server → MCP server executes tool (e.g., GitHub API call) → Results flow back through chain → Claude formulates final response → User gets answer.\n\nMCP client acts as intermediary - doesn't execute tools itself, just facilitates communication between your server and MCP server that actually runs the tools.\n</note>\n\n<note title=\"Project Setup\">\nMCP Learning Project = CLI-based chatbot implementing both client and server components for educational purposes.\n\nProject Structure = Custom MCP client connects to custom MCP server, both built in same project.\n\nDocument System = Fake documents stored in memory only, no persistence.\n\nServer Tools = Two tools implemented: read document contents, update document contents.\n\nReal-world Context = Normally projects implement either client OR server, not both. This project does both for learning.\n\nSetup Requirements = Download CLI_project.zip, extract, configure .env with API key, install dependencies.\n\nRunning Project = \"uv run main.py\" (with UV) or \"python main.py\" (without UV).\n\nVerification = Chat prompt appears, responds to basic queries like \"what's one plus one\".\n</note>\n\n<note title=\"Defining Tools with MCP\">\nMCP server implementation = Python SDK simplifies tool creation vs manual JSON schemas\n\nTool definition syntax = @mcp.tool decorator + function with typed parameters + Field descriptions\n\nDocument storage = in-memory dictionary with doc_id keys and content values\n\nTool 1 - read_doc_contents = takes doc_id string parameter, returns document content from docs dictionary, raises ValueError if doc not found\n\nTool 2 - edit_document = takes doc_id, old_string, new_string parameters, performs find/replace operation on document content, includes existence validation\n\nMCP Python SDK benefits = auto-generates JSON schemas from decorated functions, single line server creation, eliminates manual schema writing\n\nParameter definition = use Field() with description for tool arguments, import from pydantic\n\nError handling = validate document existence before operations, raise ValueError for missing documents\n\nImplementation pattern = decorator → function definition → parameter typing → validation → core logic\n</note>\n\n<note title=\"The Server Inspector\">\nMCP Inspector = in-browser debugger for testing MCP servers without connecting to actual applications\n\nAccess: Run \\`mcp dev [server_file.py]\\` in terminal with activated Python environment → opens server on port → visit provided localhost address\n\nInterface: Left sidebar with Connect button → top navigation bar shows Resources/Prompts/Tools sections → Tools section lists available tools → click tool to open right panel for manual testing\n\nTesting process: Select tool → input required parameters (like document ID) → click Run Tool → verify output/success message\n\nKey features: Live development testing, tool invocation simulation, parameter input fields, success/failure feedback\n\nStatus: Inspector in active development - UI may change but core functionality remains similar\n\nUsage pattern: Essential for MCP server development and debugging before production deployment\n</note>\n\n<note title=\"Implementing a Client\">\nMCP Client Implementation:\n\nMCP Client = wrapper class around client session for connecting to MCP server with resource cleanup management\n\nClient Session = actual connection to MCP server from MCP Python SDK, requires cleanup when closing\n\nResource Cleanup = necessary process when shutting down, handled by connect/cleanup/async enter/async exit functions\n\nClient Purpose = exposes MCP server functionality to rest of codebase, provides interface between application code and server\n\nKey Functions:\n- list_tools() = await self.session.list_tools(), return result.tools\n- call_tool() = await self.session.call_tool(tool_name, tool_input)\n\nImplementation Flow:\n1. Application requests tool list for Claude\n2. Client calls list_tools() to get server's available tools\n3. Claude selects tool and provides parameters\n4. Client calls call_tool() to execute on server\n5. Results returned to Claude\n\nTesting = run MCP client.py directly with testing harness to verify connection and tool listing\n\nIntegration = once implemented, can run CLI to have Claude use tools (e.g., \"what is contents of report.pdf document\")\n\nCommon Practice = wrap client session in larger class rather than using directly for better resource management\n</note>\n\n<note title=\"Defining Resources\">\nResources = MCP server feature that exposes data to clients for read operations\n\nResource types:\n- Direct/Static = static URI (e.g., docs://documents)\n- Templated = parameterized URI with wildcards (e.g., documents/{doc_id})\n\nResource flow:\n1. Client sends read resource request with URI\n2. MCP server matches URI to resource function\n3. Server executes function, returns result\n4. Client receives data via read resource result message\n\nImplementation:\n- Use @mcp.resource decorator\n- Define URI (route-like address)\n- Set MIME type (application/json, text/plain, etc.)\n- Templated resources: URI parameters become function keyword arguments\n- Python MCP SDK auto-serializes return values to strings\n\nCommon pattern = One resource per distinct read operation (list items vs fetch single item)\n\nMIME types = hints to client about returned data format for proper deserialization\n</note>\n\n<note title=\"Accessing Resources\">\nMCP Resource Access = method for clients to retrieve data from server resources\n\nClient Implementation:\n- read_resource function = takes URI parameter, requests resource from MCP server\n- Uses await self.session.read_resource(AnyUrl(uri)) for server communication\n- Accesses result.contents[0] = first resource from returned contents list\n\nResponse Parsing:\n- Checks resource.mime_type property to determine data format\n- If mime_type == \"application/json\": returns json.loads(resource.text)\n- Otherwise: returns resource.text as plain text\n\nResource Integration:\n- MCP client functions called by other application components\n- Enables document selection via CLI interface with arrow keys + space\n- Selected resource contents automatically included in LLM prompts\n- Eliminates need for tools to read document contents during chat\n\nKey Dependencies: json module, pydantic.AnyUrl for type handling\n</note>\n\n<note title=\"Defining Prompts\">\nPrompts = pre-written, tested instructions that MCP servers expose to clients for specialized tasks\n\nMCP Prompts Feature:\n- Servers define high-quality prompts tailored to their domain\n- Clients can access these prompts via slash commands (e.g., /format)\n- Alternative to users writing their own prompts manually\n\nImplementation Pattern:\n- Use @prompt decorator with name and description\n- Function receives arguments (e.g., document ID)\n- Returns list of messages (user/assistant format)\n- Messages sent directly to Claude\n\nKey Benefit: Server authors create optimized, tested prompts rather than leaving prompt quality to end users\n\nExample Structure:\n\\`\\`\\`\n@prompt(name=\"format\", description=\"rewrites document in markdown\")\ndef format_document(doc_id: str) -> list[messages]:\n    return [base.user_message(prompt_text)]\n\\`\\`\\`\n\nWorkflow: User types /format → selects document → server returns specialized prompt → client sends to Claude → Claude uses tools to read/reformat/save document\n\nPurpose = encapsulate domain expertise in prompt engineering within specialized MCP servers\n</note>\n\n<note title=\"Prompts in the Client\">\nMCP Client Prompt Implementation:\n\nList prompts function = await self.session.list_prompts(), return result.props\n\nGet prompt function = await self.session.get_prompt(prompt_name, arguments), return result.messages\n\nPrompt workflow = Client requests prompt by name → passes arguments as keyword parameters → MCP server interpolates arguments into prompt template → returns formatted messages for AI model\n\nArguments flow = Client arguments → prompt function keyword arguments → interpolated into prompt text (e.g., document_id parameter gets inserted into prompt template)\n\nReturn format = Messages array that forms conversation input for AI model\n\nCLI usage = /format command → select document → prompt with document ID sent to Claude → Claude uses tools to fetch document → returns formatted result\n\nKey concept = Prompts are server-defined templates that clients can invoke with parameters, enabling reusable AI instructions with dynamic content insertion.\n</note>\n\n<note title=\"MCP Review\">\nMCP Server Primitives = 3 types: tools, resources, prompts\n\nTools = model-controlled primitives where Claude decides when to execute them. Used to add capabilities to Claude (e.g., JavaScript execution for calculations). Serve the model.\n\nResources = app-controlled primitives where application code decides when to fetch data. Used to get data into apps for UI display or prompt augmentation (e.g., autocomplete options, document listings from Google Drive). Serve the app.\n\nPrompts = user-controlled primitives triggered by user actions like button clicks or slash commands. Used for predefined workflows (e.g., chat starter buttons in Claude interface). Serve users.\n\nControl patterns determine purpose: Need Claude capabilities → implement tools. Need app data → use resources. Need user workflows → create prompts.\n\nReal examples: Claude's chat starter buttons use prompts, Google Drive document selection uses resources, code execution uses tools.\n</note>\n</notes>",
  "taif": "<notes>\n<critical>\nBelow are notes from a video course about working with the Claude language model.\nUse these notes as a resource to answer the user's question.\nWrite your answer as a standalone response - do not refer directly to these notes unless specifically requested by the user.\n</critical>\n<note title=\"Introduction and Approaches to Teaching AI Fluency\">\nAI FLUENCY FRAMEWORK = dual-purpose tool that describes what happens when people work with AI AND guides them toward better practices\n\nCOURSE STRUCTURE = 3 lessons addressing: 1) How to introduce framework to students (pedagogy), 2) How to assess if students understand (assessment), 3) How to integrate with existing curriculum\n\nTEACHING APPROACHES = 4 distinct entry points for introducing AI Fluency Framework to students\n\nAPPROACH 1: STEP-BY-STEP (LINEAR)\n- Framework as sequential process: Delegation → Description → Discernment → Diligence\n- Best for: Beginning AI users who need structure and manageable progression\n- Implementation: Four-part assignments building sequentially (e.g., part 1 = Delegation/project planning, part 2 = Description/hands-on workshops, part 3 = Discernment/evaluation, part 4 = Diligence/ethics)\n- Benefits: Natural, intuitive, creates structured learning journey\n\nAPPROACH 2: START ANYWHERE (NON-LINEAR) \n- Framework as interconnected system where any competency can be starting point\n- Best for: Experienced AI users who can handle real-world complexity\n- Key insight: Competencies inform each other (Discernment problems → new Delegation decisions; Diligence requirements → different Description strategies; Description challenges → Delegation gaps)\n- Implementation: Group projects, case-based learning with real AI collaboration scenarios\n- Benefits: Reflects messy realities of actual AI collaboration, builds adaptive thinking\n\nAPPROACH 3: JUST ONE D (FOCUSED)\n- Deep exploration of single competency at a time\n- Best for: Specialized purposes, skill-building sessions, workshops\n- Implementation: Entire workshop on one competency (e.g., three Discernment sessions covering Product/Process/Performance evaluation)\n- Benefits: Masters specific area without cognitive overload, allows nuanced exploration of techniques and edge cases\n\nAPPROACH 4: TWO LOOPS\n- Most conceptually rich approach treating AI fluency as nested processes\n- Best for: Students who grasp basics and need deeper understanding\n- Structure: Delegation-Diligence loop (strategic/ethical decisions) + Description-Discernment loop (tactical/iterative work)\n- Benefits: Reveals how competencies constantly inform each other in practice\n\nDELEGATION-DILIGENCE LOOP = strategic and ethical decision-making framework for responsible AI collaboration\n\nLOOP COMPONENTS:\nDelegation = Problem Awareness + Platform Awareness + Task Delegation\nDiligence = Creation Diligence + Transparency Diligence + Deployment Diligence\n\nBIDIRECTIONAL FLOW:\nForward (Delegation → Diligence): Strategic decisions raise ethical questions\nReverse (Diligence → Delegation): Ethical constraints clarify and improve strategic choices\n\nTEACHING STRATEGIES:\n- Applied scenarios making connections visible\n- Step-by-step guides for both directions\n- Recognition that constraints enhance rather than limit creativity\n- Focus on developing clear rationales for choices\n\nDESCRIPTION-DISCERNMENT LOOP = moment-to-moment craft of building cognitive environments for human-AI collaboration\n\nCORE CONCEPT: Conversations not commands - building shared context and understanding rather than single prompt exchanges\n\nCOGNITIVE ENVIRONMENT = collaborative context including:\n- Shared vocabulary and references evolving over time\n- Well-defined goals, values, processes, methods\n- Established interaction patterns enabling optimal performance\n- Mechanisms for building on previous interactions\n\nTHREE LENSES:\nPRODUCT = what we're creating together (evolving understanding of goals and quality)\nPROCESS = shared approaches to thinking and problem-solving\nPERFORMANCE = relationship dynamics between human and AI\n\nTEACHING STRATEGIES:\n- Multi-interaction assignments requiring context accumulation\n- Share instructor's own AI collaboration processes\n- Document evolution of collaboration, not just outputs\n- Move beyond automation to augmentation\n\nSUCCESS INDICATORS:\n- Shared Language: Shorthand references to complex ideas from earlier conversations\n- Exploration Mindset: Evolution from rigid commands to flexible, interactive approaches\n\nNESTED SYSTEMS = how the two loops work together:\nDelegation-Diligence = strategic container setting direction and boundaries\nDescription-Discernment = tactical content filling container with rich interaction\nResult = Responsible, sophisticated AI collaboration transcending what either partner achieves alone\n\nLESSON DURATION = 2 hours (30 minutes video, 90 minutes exercises)\n\nEXERCISE STRUCTURE = 3 progressive exercises building teaching capability:\n1) Establish teaching context and explore approaches (30 min)\n2) Design Delegation-Diligence loop lesson (30 min) \n3) Design Description-Discernment loop lesson (30 min)\n</note>\n\n<note title=\"Assessing and Designing AI Fluency\">\n# Assessing and Designing AI Fluency Notes\n\n## Core Assessment Framework\n\nAI Fluency Assessment = Outcome-based + Process-based + Reflection-based approaches combined\n\n**Outcome-based assessment** = Focus on what students produce through AI collaboration. Assess whether students achieved stated goals through human-AI partnership.\n\n**Process-based assessment** = Examine how students work with AI over time. Captures iteration patterns, recovery from failures, methodological sophistication.\n\n**Reflection-based assessment** = Focus on metacognitive awareness. Students analyze why strategies worked/didn't work, what they learned, future applications.\n\n## Assessing the 4Ds\n\n### Delegation Assessment\n- **Outcome**: Did delegation plan make sense? Were goals realistic? Right tool selection?\n- **Process**: Review annotated chat logs showing exploration of options and decision-making\n- **Reflection**: Students explain choices, alternatives considered, how delegation shaped outcomes\n\n### Description Assessment  \n- **Outcome**: Quality of prompts and AI responses. Clear instructions? Good context? Evolution from initial to final versions\n- **Process**: Conversation logs showing iterative refinement, failed approaches, building shared context\n- **Reflection**: Analysis of which description techniques worked best, task-specific communication approaches, description quality vs output quality relationship\n\n### Discernment Assessment\n- **Outcome**: Students annotate AI outputs marking strengths/weaknesses. Decision logs explaining what was kept/modified/discarded\n- **Process**: In-line evaluation comments, catching/correcting errors, pattern recognition across interactions\n- **Reflection**: Analysis of evaluation criteria evolution, initially missed issues, comparison across task types\n\n### Diligence Assessment\n- **Outcome**: Quality of diligence statements, attribution/transparency documentation, fact-checking evidence\n- **Process**: Data handling in chat logs, ethical decision-making, sensitive information handling, constraint-checking\n- **Reflection**: Discussion of ethical dilemmas, responsibility understanding, surprising challenges, future improvements\n\n## Assignment Design Principles\n\n**Authenticity** = Create assignments mirroring real-world AI collaboration, genuine problems where AI partnership adds value\n\n**Iteration** = Build in refinement opportunities showcasing growth over time\n\n**Pedagogical Transparency** = Clear assessment of collaboration process and reflections, not just outputs\n\n## Assignment Types\n\n### Outcome-Based Assignments\n- **Improving AI outputs**: Transform mediocre AI output into excellent work\n- **Product comparison**: Use multiple AI systems for same task, analyze differences\n- **Constraint-based challenges**: Specific requirements (format, length, style, audience)\n- **Peer product review**: Students set goals, create with AI, swap and critique\n\n### Process-Based Assignments\n- **Annotated chat logs**: Students highlight turning points, breakthroughs, failure recovery\n- **Recorded narrations**: Real-time decision-making explanation during AI work\n- **Process playbooks**: Personal AI strategy guides for different task types\n- **AI-assisted/peer debrief**: Discussion of chat logs with AI or peer partners\n\n### Reflection-Based Assignments\n- **Guided inquiry**: Specific questions about particular assignments\n- **Learning journal**: Self-assess 4D development with evidence across course\n- **Scenarios/case studies**: Apply learnings to hypothetical or real-world situations\n- **Personal policy statements**: Develop own values/strategies for AI collaboration\n\n## Managing Assignment Volume\n\n**Detailed deliverable-based rubrics** = Clear, granular deliverables for quick verification\n\n**Self and peer review emphasis** = Students best positioned to observe development with proper guidance\n\n**Lightning round conferences** = Brief conferences replacing written feedback (5-minute discussions)\n\n**Selective sampling** = Don't read every word; students flag key moments for attention\n\n## Key Assessment Artifacts\n\n**Chat logs** = Primary process documentation\n**Diligence statements** = Transparency and attribution documentation\n**Learning journals** = Reflection and metacognitive development\n**Decision logs** = Record of what AI output was kept/modified/discarded\n**Process playbooks** = Personal strategy documentation\n\n## Assessment Focus Areas\n\nFocus on **observable actions and concrete artifacts** rather than assumptions about understanding\n\nDifferent competencies benefit from different assessment approaches\n\nAssessment should be **learning opportunity, not just measurement**\n\nStudents need to understand you care about **how they learn to work with AI**, not just what they produce\n\nEssential principle: Make invisible decision-making visible through documentation and reflection\n</note>\n\n<note title=\"AI Fluency Assignment Component Guide\">\nAI FLUENCY ASSIGNMENT COMPONENTS\n\nOUTCOME-BASED COMPONENTS\n\nImproving AI Outputs = Students transform mediocre AI output into excellent work through critique and iteration\n- Assessment focus: Discernment, Description\n- Deliverables: Original with annotations + improved version\n- Effective because: Forces explicit quality standards, requires domain knowledge application, develops AI guidance skills\n- Variations: Different flaw types, peer improvement exchanges\n\nProduct Comparison = Students use multiple AI systems for same task, analyze differences\n- Assessment focus: Delegation, Discernment  \n- Deliverables: Multiple outputs + comparison matrix + recommendation report\n- Effective because: Builds platform awareness, creates evidence-based selection skills, reveals trade-offs\n- Process: Use standard AI assistant + reasoning-capable system + specialized system\n\nConstraint-Based Challenges = Achieve specific requirements through AI collaboration within limits\n- Assessment focus: Description, Delegation\n- Deliverables: Final product meeting all constraints\n- Effective because: Develops precise AI communication, reveals specification importance, builds problem-solving under limitations\n- Variations: Ethical constraints, conflicting constraints, time pressure\n\nPeer Product Review = Evaluate AI-assisted work by peers against goals and ethics\n- Assessment focus: Discernment, Diligence\n- Deliverables: Review feedback form\n- Criteria: Goal achievement, appropriate AI use, human oversight quality, AI attribution adequacy\n- Variations: Blind review, group sessions\n\nPROCESS-BASED COMPONENTS\n\nAnnotated Chat Logs = Submit complete chat logs with detailed annotations\n- Assessment focus: Description, Discernment\n- Annotations mark: Turning points, insights/breakthroughs, failures/recovery, communication evolution, shared context development\n- Effective because: Makes thinking visible, captures authentic problem-solving, reveals communication evolution\n- Variations: Competency-focused annotations, pattern comparisons\n\nRecorded Narrations = Screen recording with real-time audio narration of AI work\n- Assessment focus: All 4Ds\n- Narration covers: Choice rationale, response evaluation, alternatives considered, ethical considerations\n- Effective because: Captures authentic decision-making, reveals tacit knowledge, shows actual vs reported work\n- Variations: Paired commentary, before/after comparisons\n\nProcess Playbooks = Personal reference guides for AI collaboration strategies\n- Assessment focus: All 4Ds\n- Includes: Conversation guides, decision trees, quality checklists, pitfall recovery, ethical guidelines\n- Effective because: Encourages systematic documentation, creates useful resource, builds metacognitive awareness\n- Variations: Collaborative playbooks, discipline-specific requirements\n\nAI-Assisted/Peer-Assisted Debrief = Structured analysis of AI collaboration with external perspective\n- Assessment focus: All 4Ds\n- Analysis covers: Positive patterns, missed opportunities, strengths, improvement areas\n- Effective because: Provides external perspective, reveals blind spots, encourages peer learning\n\nREFLECTION-BASED COMPONENTS\n\nGuided Inquiry = Respond to specific questions promoting deep reflection\n- Assessment focus: Variable by questions\n- Question types: Surprise moments, failed strategies, communication evolution, ethical decisions\n- Effective because: Targets specific AI fluency aspects, requires evidence-based reflection\n- Variations: Discipline-tailored, progressive questions\n\nLearning Journal = Ongoing documentation of AI fluency development\n- Assessment focus: All 4Ds over time\n- Weekly entries: Attempts, successes, challenges, skill improvements, future goals\n- Monthly synthesis: Pattern identification, goal setting\n- Variations: Multimedia journals, collaborative journals, AI conversation journals\n\nScenarios and Case Studies = Apply AI fluency learning to realistic scenarios\n- Assessment focus: Diligence, Delegation\n- Analysis includes: Delegation decisions, ethical considerations, action plans, risk mitigation\n- Effective because: Develops strategic thinking, builds ethical reasoning, connects theory to practice\n- Variations: Real-world cases, discipline-specific scenarios, stakeholder role-play\n\nPersonal Policy Statements = Synthesize learning into personal AI collaboration framework\n- Assessment focus: Diligence, All 4Ds holistically\n- Components: Ethical principles, decision criteria, quality standards, transparency commitments, boundaries, learning strategies\n- Effective because: Requires deep synthesis, builds professional identity, creates accountability\n- Variations: Collaborative policies for group work\n\nASSESSMENT MATRICES\n\nDelegation Assessment Matrix:\n- Strong performance = Strategic thinking about when/why/how to collaborate with AI\n- Outcome focus: Task division quality, tool selection appropriateness, goal achievement\n- Process focus: Decision documentation, platform exploration, adaptation evidence\n- Reflection focus: Choice analysis, alternative consideration, impact evaluation\n\nDescription Assessment Matrix:\n- Strong performance = Effective AI communication regarding outputs/processes/behaviors\n- Outcome focus: Prompt evolution, context building, communication clarity\n- Process focus: Iterative refinement, recovery patterns, shared vocabulary development\n- Reflection focus: Strategy analysis, technique recognition, context insights\n\nDiscernment Assessment Matrix:\n- Strong performance = Critical evaluation using domain-specific, nuanced criteria\n- Outcome focus: Annotation quality, evaluation depth, evidence-based judgment\n- Process focus: Error catching, consistency, quality improvement over time\n- Reflection focus: Criteria evolution, missed issue analysis, meta-evaluation skills\n\nDiligence Assessment Matrix:\n- Strong performance = Integrated ethical reasoning throughout collaboration\n- Outcome focus: Attribution quality, transparency documentation, verification evidence\n- Process focus: Data handling, permission documentation, constraint checking\n- Reflection focus: Ethical dilemma discussion, responsibility understanding, values articulation\n\nMatrix Usage:\n- Rubric development: Select relevant Ds, choose assessment types, adapt indicators, define performance levels\n- Feedback provision: Use indicators to guide review, reference strong performance markers\n- Student assessment: Share criteria beforehand, encourage self-identification of competency evidence\n</note>\n\n<note title=\"AIs Impact on Your Discipline\">\n**LESSON OVERVIEW**\n- Duration = 50+ minutes (20 min video + 30 min exercises + offline discussion)\n- Focus = AI impacts on specific disciplines across curriculum/pedagogy/assessment\n- Goal = Create discipline-specific AI fluency applications using 4Ds framework\n\n**CORE PRINCIPLE**\nAI Fluency = Amplifying human expertise, not replacing it\nDisciplinary knowledge = Foundation for unprecedented achievement through AI partnership\n\n**THREE KEY QUESTIONS FOR GRADUATES**\n1. What gets automated? = Which routine career tasks will AI handle\n2. Partnership potential = Where human-AI collaboration most impactful  \n3. Who's in charge? = How to manage/remain accountable for independent AI work\n\n**AI DISRUPTION PATTERNS**\nDisruption ≠ uniform across disciplines\n- Curriculum may stay stable while assessments transform\n- Pedagogy might evolve while core content constant\n- Some disruptions = opportunities to leverage\n- Others = problems to solve\n- Your expertise determines which is which\n\n**THREE EXPERTISE TYPES FOR AI RESPONSE**\n1. Disciplinary Expertise = Field content, values, methods, thinking patterns\n2. Pedagogical Expertise = Student struggle/breakthrough patterns, learning emotional journey\n3. Assessment Expertise = Recognizing genuine understanding, designing deep learning evaluations\n\n**CURRICULUM IMPACT ANALYSIS**\nTasks for discipline assessment:\n- Identify AI automation capabilities in field\n- Determine foundational concepts importance shifts with AI\n- Map AI augmentation improvements to best practices\n- Develop responses to \"Why learn X when AI does it?\" questions\n- Define how to direct AI agents effectively\n\n**PEDAGOGY TRANSFORMATION**\nEnhancement opportunities:\n- Personalized tutoring at scale\n- Interactive simulations  \n- Immediate feedback systems\n- AI as educational assistant/interactive learning partner\n\nRisk assessment:\n- Distinguish AI that enhances vs diminishes learning\n- Identify teacher-student AI collaborations that leverage potential\n- Determine pedagogical approaches to embrace vs avoid\n- Address reality that students will use AI regardless\n\n**ASSESSMENT REVOLUTION** \nCore challenges:\n- When students generate essays in seconds, what are we assessing?\n- Create evaluations honoring individual growth/creativity/problem-solving\n- Assess both AI-assisted and human-only performances\n\nEssential tasks:\n- Identify authentic learning demonstration in AI-assisted work\n- Create assignments students can't shortcut with AI\n- Value process/growth, not just products\n- Determine where AI collaboration enhances vs undermines objectives\n- Adapt to student AI use reality\n\n**IRREPLACEABLE HUMAN CAPABILITIES**\n- Domain expertise\n- Real-world context understanding\n- Judgment in ambiguous situations\n- Creative problem-solving\n- Ethical reasoning\n- Relationship building\n\n**APPLYING 4DS TO DISCIPLINES**\n\n**DISCERNMENT = Quality evaluation ability**\nBuild Quality Criteria:\n- Articulate excellence beyond vague terms\n- Create detailed rubrics capturing deep quality markers\n- Document criteria students can internalize/apply\n\nCollect Outstanding Work:\n- Analyze exemplars systematically with students\n- Make expert thinking visible through annotation guides\n\nDiagnose Failures:\n- Study flawed examples forensically\n- Understand failure modes for working alone or with AI\n\n**DESCRIPTION = Field communication mastery**\nMap Discipline Products:\n- Document key field outputs with precision\n- Create templates revealing underlying logic\n- Have students reverse-engineer professional outputs\n\nReveal Expert Thinking:\n- Make expert problem-approach processes visible\n- Document micro-decisions and iterations\n- Create flowcharts of expert thinking\n\nName Field Norms:\n- Surface behaviors defining your field\n- Build behavioral performance modes with students\n\n**DELEGATION = Work decomposition understanding**\nReveal Problem Anatomy:\n- Break challenges into component parts\n- Create problem anatomy diagrams\n- Map AI automation/augmentation/agent possibilities\n\nDesign Decision Trees:\n- Create frameworks for when/how to involve AI\n- Build through case studies making delegation decisions explicit\n\n**DILIGENCE = Field values embodiment**\nCodify Ethical Frameworks:\n- Define field-specific \"do no harm\"\n- Build case studies exploring ethical edges\n- Create ethical decision matrices\n\nClarify Transparency Norms:\n- Document disclosure expectations by field\n- Create AI assistance disclosure templates\n\nCo-Create Accountability Policies:\n- Build standards with students, not impose them\n- Draft policies together\n- Develop peer review protocols\n\n**FEEDBACK LOOP BENEFITS**\nStudents who articulate quality → better evaluate any output\nStudents who understand methods → guide any process effectively  \nStudents who internalize ethics → navigate any collaboration responsibly\n\n**EXERCISES**\n\n**Exercise 1: AI Impact Position Paper (40 min)**\nStep 1: Curriculum exploration with AI (10 min)\nStep 2: Pedagogical transformation analysis (10 min)  \nStep 3: Assessment strategy reimagining (10 min)\nStep 4: Position synthesis (10 min)\n\n**Exercise 2: Colleague 4D Discussions**\nDiscernment discussion = \"What does quality look like?\"\nDescription discussion = \"How do we communicate?\"\nDelegation discussion = \"What work happens?\"\nDiligence discussion = \"What are our values/standards?\"\n\n**FINAL MESSAGE**\nGoal ≠ preparing students to be replaced by AI\nGoal = preparing students to be irreplaceable\nFuture needs humans who: think critically, communicate clearly, collaborate wisely, act responsibly\n\nHuman expertise + AI fluency = Foundation for thriving in AI-enhanced future\n</note>\n</notes>",
  "vertex": "<notes>\n<critical>\nBelow are notes from a video course about working with the Claude language model.\nUse these notes as a resource to answer the user's question.\nWrite your answer as a standalone response - do not refer directly to these notes unless specifically requested by the user.\n</critical>\n<note title=\"Overview of Claude Models\">\nClaude models = 3 families optimized for different priorities\n\n**Opus** = highest intelligence model. For complex multi-step tasks requiring deep reasoning and planning. Can work independently for hours. Supports reasoning capability. Trade-off = higher cost + moderate latency.\n\n**Sonnet** = balanced model. Good intelligence + speed + cost efficiency. Strong coding abilities, precise code edits without breaking functionality. Fast text generation. Sweet spot for most use cases.\n\n**Haiku** = fastest model. Optimized for speed + cost efficiency. No reasoning capabilities. Best for real-time user interactions + high-volume processing.\n\n**Selection framework**: Intelligence priority → Opus. Speed priority → Haiku. Balanced needs → Sonnet.\n\n**Key insight**: Teams often use multiple models in same application. Haiku for user interactions, Sonnet for business logic, Opus for complex reasoning tasks.\n\nAll models share core capabilities = text generation, coding, image analysis.\n</note>\n\n<note title=\"Accessing the API\">\nAPI Access Flow = 5-step process from user input to response display\n\nStep 1: Client sends user text to developer's server (never access Vertex directly from client - credentials must stay secret)\n\nStep 2: Server makes request to Vertex AI using SDK (Python/TypeScript/Go/Ruby). Required parameters: model name, messages list, max_tokens limit\n\nStep 3: Claude text generation process = 4 stages:\n- Tokenization = input broken into tokens (words/parts/symbols)\n- Embedding = tokens converted to number lists representing all possible meanings\n- Contextualization = embeddings adjusted based on neighboring tokens to determine precise meaning\n- Generation = output layer produces word probabilities, model selects using probability + randomness, adds selected word, repeats process\n\nStep 4: Generation stops when max_tokens reached OR model generates end-of-sequence token\n\nStep 5: API returns response with generated text, usage counts, stop reason. Server forwards to client for display\n\nKey Components:\n- Token = text chunk (word/part/symbol)\n- Embedding = numerical representation of token meanings\n- Contextualization = meaning refinement using surrounding context\n- Max tokens = generation length limit\n- Stop reason = why generation ended\n- Usage = input/output token counts\n\nSecurity: Always route API calls through server, never expose credentials in client applications.\n</note>\n\n<note title=\"Making a Request\">\n**Making a Request to Vertex AI Claude**\n\n**Setup Steps:**\n1. Install anthropic SDK: \\`%pip install \"anthropic[vertex]\"\\`\n2. Import and create client: \\`from anthropic import vertex\\` then instantiate with region=\"global\" and project_id\n3. Define model version variable for reuse\n\n**Required Arguments for client.messages.create():**\n- model = specific Claude model version string\n- max_tokens = safety limit on response length (not a target, just maximum)\n- messages = list of message dictionaries\n\n**Message Types:**\n- User message = {role: \"user\", content: \"text written by humans\"}\n- Assistant message = {role: \"assistant\", content: \"text generated by Claude\"}\n\n**Response Access:**\nRaw response contains metadata. Extract generated text with: \\`message.content[0].text\\`\n\n**Key Concepts:**\nMessages = conversation exchanges between user and Claude\nMax tokens = safety mechanism preventing excessive generation\nProject ID = unique identifier from Google Cloud console\n</note>\n\n<note title=\"Multi-Turn Conversations\">\nMulti-Turn Conversations = maintaining context across multiple exchanges with Claude\n\nKey constraint: Anthropic API stores no messages. Each request is stateless - no memory of previous exchanges.\n\nProblem: Sending follow-up requests without context produces irrelevant responses since Claude has no memory of prior conversation.\n\nSolution requirements:\n1. Manually maintain message list in your code\n2. Send entire conversation history with every request\n\nImplementation pattern:\n- Create empty messages list\n- Add user message to list\n- Send to Claude, get response\n- Add assistant response to list\n- Add next user message to list\n- Send entire updated list to Claude\n- Repeat\n\nEssential helper functions:\n- add_user_message(messages, text) = appends user message to conversation history\n- add_assistant_message(messages, text) = appends assistant response to conversation history  \n- chat(messages) = sends message list to Claude API and returns response\n\nMessage structure = list of dictionaries with \"role\" (user/assistant) and \"content\" fields\n\nContext preservation = including full conversation history in each API call enables Claude to understand references and maintain coherent dialogue flow.\n</note>\n\n<note title=\"System Prompts\">\nSystem Prompts = instructions that customize Claude's tone, style, and behavior for specific use cases.\n\nPurpose = control how Claude responds rather than just what it responds with. Example: making Claude act as patient math tutor who gives hints instead of direct answers.\n\nImplementation = pass system prompt as plain string to create function using system keyword argument.\n\nStructure = first line typically assigns Claude a role (\"You are a patient math tutor\"), followed by specific behavioral instructions.\n\nKey principle = system prompts steer Claude's approach - tutor gives guidance and asks leading questions vs directly solving problems.\n\nTechnical consideration = system parameter cannot be None, must conditionally include in API call only when prompt exists.\n\nBest practice = make system prompts configurable rather than hard-coded for reusability across different use cases.\n\nResult = transforms direct question-answer interactions into guided, interactive experiences tailored to specific roles and contexts.\n</note>\n\n<note title=\"Temperature\">\nTemperature = parameter controlling randomness in Claude's text generation, decimal value 0-1\n\nText generation process = tokenization → probability assignment → token selection based on probabilities\n\nTemperature effects:\n- Temperature 0 = deterministic output, always selects highest probability token\n- Higher temperature = increases chance of selecting lower probability tokens, more creative/unexpected outputs\n\nUsage guidelines:\n- Low temperature (near 0) = data extraction, factual tasks, consistent outputs\n- High temperature (near 1) = creative writing, brainstorming, jokes, marketing\n\nImplementation = add temperature parameter to model call function, defaults to 1.0 for creativity\n\nKey insight = higher temperature increases chance of variation but doesn't guarantee it, may need multiple attempts to see creative differences\n</note>\n\n<note title=\"Response Streaming\">\nResponse Streaming = technique to provide immediate feedback to users during long AI response generation times instead of showing loading spinners for 10-30 seconds.\n\nHow it works: Server sends user message to Claude → Claude immediately sends initial response (no text content, just acknowledgment) → Claude sends stream of events containing text chunks → Server receives events and sends text chunks to frontend → User sees text appear progressively chunk by chunk.\n\nEvent types: message_start, content_block_start, content_block_delta (contains actual text), content_block_stop, message_delta, message_stop.\n\nImplementation: \n- Basic streaming: client.messages.create(model, max_tokens, messages, stream=True) returns iterator of events\n- Text-focused streaming: client.messages.stream() with context manager provides cleaner access to text content via stream.text_stream\n- Final message assembly: stream.get_final_message() collects all chunks into complete message for database storage\n\nKey benefit: Users see immediate progressive response instead of waiting for complete generation, improving user experience in chat interfaces.\n</note>\n\n<note title=\"Controlling Model Output\">\nModel output control = two main techniques beyond prompt modification: pre-filling assistant messages and stop sequences.\n\nPre-filling assistant messages = manually adding assistant message at end of message list to steer response direction. Claude treats pre-filled content as already authored and continues from that point. Example: adding \"coffee is better because\" forces Claude to justify coffee preference. Response continues from end of pre-filled text, not complete replacement.\n\nImplementation: Add assistant message with partial content after user message in messages list. Claude sees this as its own previous response and builds upon it.\n\nStop sequences = forcing Claude to halt generation when specific string appears. Provides list of strings that trigger immediate response termination when generated. Generated stop sequence text excluded from final output.\n\nImplementation: Add stop_sequences parameter to API call with list of trigger strings. Generation stops immediately upon encountering any listed sequence.\n\nUse cases: Pre-filling controls response direction/stance. Stop sequences control response length/format by terminating at specific points.\n</note>\n\n<note title=\"Structured Data\">\nStructured Data Generation = combining stop sequences + assistant message prefilling to get raw output without commentary.\n\nProblem: Claude naturally adds headers/footers/explanations when generating structured data (JSON, Python, lists). Users often want just the raw content for copy/paste functionality.\n\nSolution Pattern:\n1. Pre-fill assistant message with opening delimiter (e.g., \"\\`\\`\\`json\")\n2. Set stop sequence to match closing delimiter (e.g., \"\\`\\`\\`\")\n3. Claude assumes it already wrote the opening, generates only the content, stops at closing delimiter\n\nResult: Raw structured data with no additional commentary.\n\nExample Flow:\n- User: \"generate event bridge rule as JSON\"\n- Assistant prefill: \"\\`\\`\\`json\"\n- Stop sequence: \"\\`\\`\\`\"\n- Output: Just the JSON content between delimiters\n\nKey Benefits:\n- Eliminates unwanted markdown formatting\n- Removes explanatory text\n- Enables direct copy/paste functionality\n- Works for any structured data type (JSON, code, lists)\n\nImplementation: text.strip() or JSON.loads() can clean up remaining newlines/formatting.\n\nUse Case: Any scenario requiring precise structured output without Claude's natural helpful commentary.\n</note>\n\n<note title=\"Prompt Evaluation\">\nPrompt Engineering = techniques for writing/editing prompts to help Claude understand requests and desired response format.\n\nPrompt Evaluation = automated testing of prompts using objective metrics to measure effectiveness.\n\nThree paths after writing a prompt:\n1. Test once/twice, deploy to production (trap)\n2. Test with custom inputs, minor tweaks for corner cases (trap)  \n3. Run through evaluation pipeline for objective scoring (recommended)\n\nOptions 1 and 2 are common traps - engineers don't test prompts sufficiently before production use.\n\nBest practice = Use evaluation pipeline to get objective performance scores, then iterate on prompt based on results before deployment.\n\nEvaluation comes before engineering in the learning sequence - measure effectiveness first, then learn improvement techniques.\n</note>\n\n<note title=\"A Typical Eval Workflow\">\nTypical Eval Workflow = 5-step iterative process for prompt optimization\n\nStep 1: Initial Prompt Draft = Write basic prompt with input variables (example: \"Please answer the user's question [user_input]\")\n\nStep 2: Evaluation Dataset = Create collection of test inputs (3 examples minimum, real-world uses hundreds/thousands). Can be hand-crafted or AI-generated.\n\nStep 3: Prompt Execution = Feed each dataset input through prompt template to create complete prompts, then send to Claude for responses.\n\nStep 4: Grading = Use grader system to score each question-answer pair (typically 1-10 scale). Average all scores for overall prompt performance metric.\n\nStep 5: Iteration = Modify prompt based on results, repeat entire process, compare scores to determine better version.\n\nKey Principles:\n- No standardized methodology across industry\n- Multiple implementation options (open source packages, paid solutions, custom builds)\n- Can start simple in Jupyter notebook before scaling\n- Objective scoring enables data-driven prompt improvement\n- Process repeats until satisfactory performance achieved\n\nWorkflow enables systematic prompt optimization through measurable performance comparison rather than subjective evaluation.\n</note>\n\n<note title=\"Generating Test Datasets\">\nCustom Prompt Evaluation Workflow = Building system to evaluate AWS code generation prompts\n\nGoal = Help users write AWS-specific code by outputting one of three formats: Python, JSON configuration, or raw regular expressions\n\nPrompt Structure = \"Please provide a solution to the following task: [user task]\" with no additional explanation/headers/footers\n\nTest Dataset = Array of JSON objects with \"task\" property describing desired AWS tasks\n\nDataset Generation Methods = Manual assembly or automatic generation using Claude Haiku (faster model recommended)\n\nImplementation Process:\n1. Create generate_dataset() function with large prompt asking Claude to generate test cases\n2. Use pre-filling technique with assistant message starting \"\\`\\`\\`json\"\n3. Set stop sequence as \"\\`\\`\\`\" to ensure clean JSON output\n4. Parse response with JSON.loads()\n5. Save dataset to file using JSON.dump() for later evaluation use\n\nDataset Structure = [{task: \"description\"}, {task: \"description\"}, ...]\n\nKey Technique = Pre-filling + stop sequences for reliable JSON parsing from LLM responses\n\nNext Step = Use saved dataset file to evaluate prompt performance across all test cases\n</note>\n\n<note title=\"Running the Eval\">\nTest case = individual record from generated dataset that gets merged with prompt and fed to Claude\n\nrun_prompt function = takes test case, merges with prompt (simple \"please solve the following task\" + test case), sends to Claude, returns output. Currently lacks formatting instructions so returns verbose output.\n\nrun_test_case function = takes individual case, calls run_prompt, grades result (currently hardcoded score of 10), returns dictionary with output/test case/score.\n\nrun_eval function = loads dataset, loops through each test case calling run_test_case, assembles all results into list.\n\nEval pipeline workflow = dataset → merge with prompt → send to Claude → grade results → collect outputs\n\nCurrent limitations = no grading logic implemented (hardcoded scores), no output formatting instructions in prompt, relatively slow execution time (31 seconds with Haiku model).\n\nResults format = array of objects containing Claude output, original test case, and score for each test case.\n\nNext step = implement actual graders to replace hardcoded scoring system.\n</note>\n\n<note title=\"Model Based Grading\">\nModel Based Grading = using AI models to evaluate outputs from other AI models by providing objective scoring signals\n\nThree grader types:\n- Code graders = programmatic checks (length, syntax, readability, word presence)\n- Model graders = additional API calls to evaluate quality, instruction following, completeness \n- Human graders = manual evaluation (flexible but time-consuming)\n\nKey requirements:\n- All graders must return objective signals (typically 1-10 scores)\n- Define evaluation criteria upfront\n- For model graders: use detailed prompts requesting strengths/weaknesses/reasoning to avoid middling scores (models default to ~6 without reasoning requirements)\n\nImplementation pattern:\n1. Create grading function that takes test case + model output\n2. Use structured prompts with role definition + clear evaluation instructions\n3. Extract JSON responses with scores/reasoning\n4. Calculate average scores across test cases for final metric\n\nCommon evaluation criteria = format compliance, syntax validation, task completion accuracy\n\nModel graders offer high flexibility for subjective quality assessment while code graders handle objective validation checks.\n</note>\n\n<note title=\"Code Based Grading\">\nCode Based Grading = system to validate model outputs contain only valid code (Python/JSON/RegEx) without explanations\n\nCore Components:\n- validate_json() = tries JSON parsing, returns 10 if valid, 0 if error\n- validate_python() = tries AST parsing, returns 10 if valid, 0 if error  \n- validate_regex() = tries regex compilation, returns 10 if valid, 0 if error\n- grade_syntax() = dispatcher function that calls appropriate validator based on test case format\n\nImplementation Steps:\n1. Add validator functions that attempt parsing/compilation\n2. Update dataset to include \"format\" key specifying expected output type (JSON/Python/RegEx)\n3. Update prompt template to explicitly request only code without commentary\n4. Use pre-filled Assistant message with code blocks and stop sequences\n5. Merge syntax score with model score by averaging\n\nDataset Format = must include format field indicating expected code type for each test case\n\nPrompt Engineering = use \"\\`\\`\\`code\" as pre-filled Assistant message since exact format unknown beforehand\n\nScore Calculation = (model_score + syntax_score) / 2\n\nKey Insight = validates syntax through try/catch parsing rather than complex rule-based validation\n</note>\n\n<note title=\"Prompt Engineering\">\nPrompt Engineering = improving prompts to get more reliable, higher quality outputs from language models.\n\nGoal: Generate one-day meal plans for athletes based on height, weight, physical goal, and dietary restrictions.\n\nProcess:\n1. Write initial prompt (poor first attempt)\n2. Evaluate prompt performance \n3. Apply prompt engineering techniques iteratively\n4. Re-evaluate after each improvement\n5. Monitor performance improvements\n\nInitial prompt example: \"What should this person eat?\" + basic parameter interpolation (height, weight, goal, restrictions).\n\nEvaluation setup uses Prompt Evaluator class with configurable concurrency (start low like 3, reduce to 1 if rate limits hit). Extra criteria can specify output requirements like caloric totals, macro breakdowns, exact foods/portions/timing.\n\nExpected pattern: Initial scores are poor, improve systematically through technique application. Output evaluation creates HTML report showing test case results, scores, and reasoning.\n\nKey insight: Start simple, measure performance, then incrementally apply engineering techniques while continuously evaluating improvements.\n</note>\n\n<note title=\"Being Clear and Direct\">\nBeing Clear and Direct = technique for improving prompt effectiveness by focusing on the first line of prompts.\n\nFirst line importance = most critical part of prompt that sets the foundation for AI response.\n\nStructure = use action verb + simple direct language + clear task description.\n\nAction verbs = write, generate, create, identify, analyze - tells AI exactly what to do.\n\nTask specification = include details about expected output format and content requirements.\n\nExamples:\n- \"Write three paragraphs about how solar panels work\" = action verb + output format + topic\n- \"Identify three countries that use geothermal energy and for each include generation stats\" = action verb + quantity + specific requirements\n\nImplementation = replace vague openings with direct commands that specify both the action and expected deliverable.\n\nResults = significant improvement in prompt performance (example showed increase from 2.32 to 3.92 grade).\n</note>\n\n<note title=\"Being Specific\">\nBeing Specific = adding guidelines or steps to direct model output in particular direction\n\nTwo types of guidelines:\n- Type A: Control output attributes (length, structure, qualities)\n- Type B: Provide steps for model to follow (forces consideration of specific elements, improves reasoning quality)\n\nBoth types often combined in professional prompts.\n\nWhen to use:\n- Type A: Almost always - list desired output qualities\n- Type B: Complex problems requiring broader consideration of viewpoints/data beyond model's natural scope\n\nExample improvement: Adding guidelines improved evaluation score from 3.92 to 7.86, demonstrating significant impact on output quality.\n</note>\n\n<note title=\"Structure with XML Tags\">\nXML Tags for Prompt Structure = technique to organize and clarify different content sections within prompts using custom XML-style tags.\n\nPurpose = helps language models distinguish between different types of interpolated content when large amounts of text are inserted into prompts.\n\nImplementation = wrap content sections with descriptive tags like <sales_records></sales_records> or <my_code></my_code> rather than dumping raw text.\n\nTag naming = use specific, descriptive names (sales_records > records > data) to provide context about content nature.\n\nUse cases = separating code from documentation, marking athlete information, organizing sales data, any scenario with multiple content types in single prompt.\n\nBenefits = reduces ambiguity about what text serves what purpose, improves model comprehension of prompt structure, can lead to quality improvements especially with simpler models.\n\nBest practice = even for shorter content sections, XML tags can clarify that content represents external input or specific information categories.\n</note>\n\n<note title=\"Providing Examples\">\nOne-shot/multi-shot prompting = providing examples within prompts to guide model behavior. One-shot = single example, multi-shot = multiple examples.\n\nImplementation = wrap examples in XML tags with sample input and ideal output sections. Structure clearly separates example content from main prompt.\n\nCorner cases = use multi-shot prompting to handle edge cases like sarcasm detection. Add context explaining why specific scenarios need special attention.\n\nComplex outputs = examples especially effective for demonstrating intricate JSON structures or formatting requirements.\n\nPrompt evaluation integration = extract high-scoring test cases from evaluation results to use as examples. Include reasoning explanation of why output is ideal to reinforce desired behavior patterns.\n\nEffectiveness = consistently improves model performance by providing concrete behavioral templates and output format guidance.\n</note>\n\n<note title=\"Introducing Tool Use\">\nTool use = mechanism allowing Claude to access external information beyond its training data\n\nCore limitation: Claude only knows information from training data, lacks real-time/current information\n\nTool use flow:\n1. Send initial request to Claude + instructions for external data access\n2. Claude determines if external data needed, requests specific information\n3. Server runs code to fetch requested data from external sources\n4. Send follow-up request to Claude with retrieved data\n5. Claude generates final response using original prompt + external data\n\nWeather example:\n- User asks current weather in San Francisco\n- Without tools: Claude responds \"no access to current weather data\"\n- With tools: Claude requests weather data → server calls weather API → Claude receives data → provides current weather response\n\nKey concept: Tools enable Claude to augment responses with real-time external information through structured data retrieval process.\n</note>\n\n<note title=\"Project Overview\">\nProject goal = teach Claude to set time-based reminders via tools in Jupyter notebook\n\nTarget interaction = user says \"Set reminder for doctor's appointment, week from Thursday\" → Claude responds \"I will remind you at that point in time\"\n\nThree core problems identified:\n1. Claude lacks precise current time knowledge (knows date, not exact time)\n2. Claude sometimes fails at time-based calculations (e.g., \"379 days from January 13th, 1973\")\n3. Claude has no mechanism to actually set reminders (understands concept but cannot execute)\n\nSolution approach = three dedicated tools:\n1. Get current datetime tool (date + time)\n2. Add duration to datetime tool (handles time calculations)\n3. Reminder setting tool (executes actual reminder creation)\n\nImplementation = one tool at a time, eventually combining multiple tools for complete functionality.\n</note>\n\n<note title=\"Tool Functions\">\nTool Functions = Python functions executed automatically when Claude needs extra information to help users\n\nKey Components:\n- Tool function = plain Python function that retrieves specific data (e.g., current datetime, weather)\n- Executed when Claude determines it needs additional information to answer user queries\n\nBest Practices:\n1. Use descriptive function/argument names\n2. Validate inputs and raise errors for invalid data\n3. Include meaningful error messages that help Claude correct mistakes\n\nError Handling Benefits:\n- Claude sees exact error messages when tool calls fail\n- Can retry tool calls with corrected parameters based on error feedback\n- Example: \"location cannot be empty\" helps Claude retry with valid location\n\nImplementation Pattern:\n\\`\\`\\`\ndef get_current_datetime(date_format=\"%Y%m%d %H:%M:%S\"):\n    if not date_format:\n        raise ValueError(\"date format cannot be empty\")\n    return datetime.now().strftime(date_format)\n\\`\\`\\`\n\nTool functions should focus on single responsibility (getting datetime, weather, etc.) with proper input validation and clear error messaging for Claude's retry logic.\n</note>\n\n<note title=\"Tool Schemas\">\nTool Schemas = JSON configuration objects that describe tool functions for language models\n\nJSON Schema = data validation specification (not LM-specific) used to validate JSON data. LM community adopted it for tool calling because it's widely understood and convenient.\n\nTool Schema Structure:\n- name: tool function name\n- description: 3-4 sentences explaining what tool does, when to use, what data it returns\n- input_schema: actual JSON schema describing function arguments (type, description for each parameter)\n\nBest Practice: Use 3-4 sentence descriptions for both tool and individual arguments to help Claude understand purpose and usage.\n\nSchema Generation Trick: Ask Claude to write JSON schema for your function + attach Anthropic tool use documentation for best practices. Results in high-quality schemas.\n\nNaming Convention: function_name + function_name_schema for easy tracking.\n\nType Safety: Wrap schema in ToolParam import from anthropic.types to prevent type errors during implementation.\n</note>\n\n<note title=\"Handling Message Blocks\">\n**Step 3: Calling Claude with Tool Schema**\n\nAPI Request Structure = Include tools parameter with JSON schema list to inform Claude of available tools\n\n**Multi-Block Message Response**\n\nMessage Content Types = Text blocks (user display) + Tool use blocks (function calls)\n\nTool Use Block Contents = Function name + input arguments for execution\n\n**Critical Message History Management**\n\nConversation Persistence = Manual tracking required - Claude stores no history\n\nMulti-Block Handling = Must preserve entire content list including all blocks (text + tool use) when appending to message history\n\nImplementation = messages.append({\"role\": \"assistant\", \"content\": response.content})\n\n**Helper Function Updates Needed**\n\nCurrent Limitation = add_user_message and add_assistant_message only support single text blocks\n\nRequired Fix = Update helpers to handle multiple content blocks per message\n</note>\n\n<note title=\"Sending Tool Results\">\nStep 4: Execute tool function that Claude requested. Extract arguments from tool use block using response.content[1].input, then call function with **kwargs syntax to convert dictionary to keyword arguments.\n\nStep 5: Send follow-up request to Claude with full conversation history plus new user message containing tool result block.\n\nTool result block structure:\n- tool_use_id = matches ID from original tool use block (enables Claude to match requests to results when multiple tools called)\n- content = tool function output converted to string \n- is_error = false (default, set true if tool execution failed)\n\nTool use ID system: When Claude makes multiple tool calls, each gets unique ID. Tool results must reference matching IDs so Claude can correlate which result belongs to which request.\n\nComplete flow: Original user message → Assistant message with tool use block → Execute tool function → User message with tool result block → Final assistant response using tool output.\n\nMust include tool schema in all requests once tools introduced, even in follow-up calls.\n</note>\n\n<note title=\"Multi-Turn Conversations with Tools\">\nMulti-Turn Tool Conversations = system allowing AI to make multiple sequential tool calls within single conversation\n\nProblem: Users submit queries requiring multiple tools. Example: \"What day is 103 days from today?\" needs get_current_datetime tool, then add_duration_to_datetime tool.\n\nProcess Flow:\n1. Claude requests first tool (get_current_datetime)\n2. System executes tool, returns result\n3. Claude requests second tool (add_duration_to_datetime) \n4. System executes tool, returns result\n5. Claude provides final answer\n\nImplementation Pattern:\n- run_conversation function takes initial message list\n- While loop: call Claude, check response\n- If response contains tool_use blocks: execute tools, add results to conversation, continue loop\n- If response contains no tool_use: return final answer to user\n\nRequired Refactoring:\n1. Update add_user_message/add_assistant_message helpers to handle multiple message blocks (text + tool_use blocks)\n2. Modify chat function to accept tool schemas list, return entire message instead of just text\n3. Add text_from_message helper to extract text from multi-block messages\n4. Implement conversation loop supporting sequential tool calls\n\nKey Concept: Unpredictable tool chains require flexible conversation management assuming multiple tool calls per query.\n</note>\n\n<note title=\"Implementing Multiple Turns\">\nMultiple Turns = Loop that keeps calling Claude until it stops requesting tools, indicated by stop_reason field\n\nStop Reason = Field in Claude's response indicating why text generation stopped. \"tool_use\" value means Claude wants to call a tool.\n\nRun Conversation Function = Main loop that:\n1. Calls Claude with messages + available tools\n2. Adds assistant response to message history\n3. Checks stop_reason - if not \"tool_use\", breaks loop\n4. If tool_use, calls run_tools function\n5. Adds tool results as user message\n6. Repeats until Claude gives final response\n\nRun Tools Function = Processes multiple tool calls from single message:\n1. Filters message.content for tool_use blocks only\n2. Iterates through each tool request\n3. Runs appropriate tool function using tool request name/input\n4. Creates tool_result blocks with: type=\"tool_result\", tool_use_id=original_id, content=JSON_encoded_output, is_error=false/true\n5. Returns list of all tool result blocks\n\nRun Tool Function = Helper that maps tool names to actual tool functions using if statements. Enables easy scaling to multiple tools.\n\nError Handling = Try/catch around tool execution, setting is_error=true and including error message in content when tools fail.\n\nMessage Flow = User message → Assistant response with tool_use → Tool results as user message → Assistant response (repeat until no tool_use) → Final response.\n\nMulti-block Messages = Single Claude message can contain multiple blocks (text + multiple tool_use blocks). Code must handle extracting and processing all tool_use blocks separately.\n</note>\n\n<note title=\"Using Multiple Tools\">\nAdding multiple tools to Claude involves updating two key functions after implementing tool schemas and functions.\n\n**Process:**\n1. Add tool schemas to tools list in RunConversation function\n2. Add corresponding function calls to RunTool function using if/elif statements\n3. Tool functions execute and return results when Claude requests them\n\n**Example Implementation:**\n- RunTool checks tool_name, calls appropriate function with **tool_input\n- Pattern: if tool_name == \"ToolName\": return ToolFunction(**tool_input)\n\n**Multi-tool Usage:**\nClaude can chain multiple tools in sequence (e.g., AddDurationToDateTime → SetReminder). Each tool call creates separate message blocks with tool_use and tool_result pairs in conversation history.\n\n**Key Pattern:**\nOnce initial framework established, adding new tools = add schema + add function + update RunTool switch statement. Process becomes standardized and scalable.\n</note>\n\n<note title=\"The Batch Tool\">\nBatch Tool = workaround to make Claude call multiple tools in parallel within single Assistant message\n\nProblem: Claude can technically send multiple tool use blocks in one response but rarely does so in practice. Instead sends separate sequential Assistant messages with single tool uses, creating unnecessary request rounds.\n\nSolution: Implement batch tool that acts as abstraction layer. Claude calls batch tool instead of individual tools directly.\n\nBatch Tool Schema: Takes \"invocations\" parameter = list of objects, each representing another tool to call (tool name + arguments)\n\nImplementation: \n- Add batch tool to available tools list\n- When Claude calls batch tool, extract invocations list\n- Iterate through each invocation, parse JSON arguments, call run_tool() for each\n- Return combined results as single tool result\n\nResult: Claude uses batch tool to parallelize multiple tool calls instead of making separate sequential calls. Reduces request rounds from N+1 to 2 (initial request + batch response).\n\nKey insight: This \"tricks\" Claude into parallel execution by providing higher-level abstraction it's more likely to use than native multiple tool use blocks.\n</note>\n\n<note title=\"Tools for Structured Data\">\nTools for Structured Data:\n\nAlternative method to extract structured JSON from Claude using tools instead of message pre-fill/stop sequences. More reliable but more complex setup.\n\nCore concept: Create JSON schema spec for tool where inputs match desired output structure. Claude calls tool with structured arguments containing extracted data.\n\nWorkflow:\n1. Write prompt asking Claude to analyze data and call provided tool\n2. Provide JSON schema defining tool inputs (matches desired output structure)\n3. Claude responds with tool_use block containing structured arguments\n4. Extract JSON from tool arguments - no tool_result response needed\n\nKey requirement: Force Claude to call specific tool using tool_choice parameter = {\"type\": \"tool\", \"name\": \"tool_name\"}\n\nSetup steps:\n- Define schema with tool inputs matching desired JSON structure\n- Update chat function to accept tool_choice parameter\n- Pass tool_choice to client.messages.create()\n- Access structured data via response.content[0].input\n\nAdvantage = More reliable structured output\nDisadvantage = More complex setup than prompt-based methods\n\nBoth techniques useful depending on scenario complexity requirements.\n</note>\n\n<note title=\"The Text Edit Tool\">\nText Editor Tool = Claude's built-in tool for file/text manipulation with wide text editor capabilities (open/read files, edit ranges, add/replace text, create files, undo).\n\nOnly JSON schema built into Claude, not implementation. Developers must write tool function implementation to handle Claude's text editor requests.\n\nSchema stub required based on model version:\n- Claude 3.7: specific date format\n- Claude 3.5: different date format\nSmall schema automatically expands to full schema in Claude.\n\nCapabilities = replicate code editor functionality (file operations, refactoring, testing file creation). Use cases = scenarios without access to full-featured code editors but need programmatic file manipulation.\n\nImplementation class needed with methods: view (file/directory contents), string_replace, create_file, etc.\n</note>\n\n<note title=\"The Web Search Tool\">\nWeb Search Tool = built-in Claude tool for searching web to find up-to-date/specialized information\n\nKey Implementation:\n- Requires minimal schema: type=\"web_search_20250305\", name=\"web_search\", max_uses=5\n- No custom implementation needed - Claude handles search execution\n- max_uses = limit on total searches (single search can return multiple results)\n\nResponse Structure:\n- Text blocks = Claude's framing/answers\n- Server tool use blocks = search query inputs\n- Web search tool result blocks = search results with title/URL\n- Citation blocks = supporting text for Claude's statements\n\nSchema Configuration:\n- allowed_domains = constrains search to specific domains (e.g., \"NIH.gov\" for medical content)\n- Improves result quality by limiting to authoritative sources\n\nUI Rendering Best Practice:\n- Display text blocks as plain text\n- Highlight web search result blocks and citations separately\n- Show source domains, titles, URLs, and cited text for transparency\n- Helps users understand information sourcing\n\nUse Case = current events, specialized knowledge, fact-checking with credible sources\n</note>\n\n<note title=\"Introducing Retrieval Augmented Generation\">\nRAG = Retrieval Augmented Generation, technique for querying large documents with LLMs.\n\nProblem: How to extract specific information from large documents (100-1000 pages) for LLM processing.\n\nOption 1 (Direct approach): Feed entire document text into prompt.\n- Limitations: Token limits, decreased effectiveness with long prompts, higher cost, slower processing.\n\nOption 2 (RAG approach): \n- Step 1: Break document into small chunks\n- Step 2: Find most relevant chunks for user question, include only those in prompt\n\nRAG advantages: LLM focuses on relevant content, scales to large/multiple documents, smaller prompts = faster/cheaper processing.\n\nRAG disadvantages: Complex preprocessing required, need search mechanism for chunk retrieval, must define \"relevance\", no guarantee chunks contain complete context, multiple chunking strategies possible (equal portions vs header-based).\n\nKey challenge: Determining optimal chunking strategy and search mechanism for specific use case.\n\nRAG trades simplicity for scalability and efficiency but requires careful implementation evaluation.\n</note>\n\n<note title=\"Text Chunking Strategies\">\nText Chunking = process of dividing source documents into smaller text segments for RAG pipelines\n\nCore Problem: How text is chunked significantly impacts RAG quality. Poor chunking leads to wrong context retrieval (e.g., medical text about \"bugs\" retrieved for software engineering questions due to word overlap)\n\nThree Main Strategies:\n\nSize-based Chunking = dividing documents into equal-length strings\n- Pros: easiest to implement, most common in production\n- Cons: cuts off words mid-sentence, lacks context\n- Solution: overlap strategy = include characters from neighboring chunks to preserve context\n- Trade-off: creates text duplication but improves chunk meaning\n\nStructure-based Chunking = dividing based on document structure (headers, paragraphs, sections)\n- Works well with formatted documents (markdown, clear sections)\n- Limitation: requires structured input, fails with plain text/PDFs\n- Implementation depends on document format guarantees\n\nSemantic-based Chunking = using NLP to group related sentences/sections\n- Most advanced technique\n- Groups consecutive related content\n- Complex implementation, not covered in detail\n\nKey Parameters:\n- Chunk size = length of each text segment\n- Overlap = number of characters shared between adjacent chunks\n- Separation criteria = what defines chunk boundaries\n\nStrategy Selection depends on:\n- Document structure guarantees\n- Input format consistency\n- Use case requirements\n\nDefault recommendation: chunk by character = most reliable across document types despite suboptimal results\n</note>\n\n<note title=\"Text Embeddings\">\nText Embeddings = numerical representation of meaning in text, generated by embedding models\n\nEmbedding Model = takes text input, outputs long list of numbers (values range -1 to +1)\n\nEach Number = score representing some quality/feature of input text (actual qualities unknown but helpful to think as semantic features like happiness, topic relevance, etc.)\n\nSemantic Search = uses text embeddings to find related content by comparing numerical representations rather than exact word matches\n\nRAG Pipeline Process = extract text chunks → user submits query → find related chunks using embeddings → add as context to prompt\n\nGoogle's text-embedding-005 = specific embedding model available on Vertex AI for generating text embeddings\n\nKey Limitation = we don't know what each number in embedding actually represents, but can conceptualize them as semantic quality scores\n\nImplementation = use Google GenAI SDK, create client, pass text through model to get embedding vectors\n\nUse Case = finding text chunks related to user questions by comparing their numerical representations rather than keyword matching\n</note>\n\n<note title=\"The Full RAG Flow\">\nRAG Pipeline = Complete flow from document processing to query response through embeddings and vector search.\n\nStep 1: Text Chunking = Split source documents into separate text pieces for processing.\n\nStep 2: Embedding Generation = Convert text chunks into numerical vectors using embedding models. Embeddings = numerical representations where each dimension captures semantic meaning.\n\nStep 3: Normalization = Scale embedding vectors to unit length (magnitude = 1.0), typically handled automatically by embedding APIs.\n\nStep 4: Vector Database Storage = Store normalized embeddings in specialized database optimized for numerical vector operations.\n\nStep 5: Query Processing = User submits question, gets embedded using same model as source documents.\n\nStep 6: Similarity Search = Vector database finds most similar stored embeddings to query embedding.\n\nCosine Similarity = Measure of similarity between vectors, calculated as cosine of angle between them. Returns values -1 to 1, where 1 = very similar, -1 = very different.\n\nCosine Distance = 1 minus cosine similarity. Values near 0 = high similarity, larger values = less similarity.\n\nStep 7: Response Generation = Combine user query with most relevant retrieved text chunks in prompt, send to LLM for final response.\n\nKey Point: Preprocessing (steps 1-4) happens ahead of time. Real-time processing only involves steps 5-7 when user submits query.\n</note>\n\n<note title=\"Implementing the Rag Flow\">\nRAG Flow Implementation = Complete workflow using vector database for document retrieval\n\n5 Core Steps:\n1. Text Chunking = Split document into sections using chunk_by_section function\n2. Generate Embeddings = Create embeddings for each text chunk using generate_embedding\n3. Vector Store Population = Create vector index instance, loop through chunk-embedding pairs, insert via store.add_vector with embedding + metadata dictionary containing original text\n4. Query Processing = Generate embedding for user question using same generate_embedding function\n5. Similarity Search = Use store.search(user_embedding, num_results) to find most relevant chunks, returns documents with cosine distances\n\nKey Technical Points:\n- Vector DB stores embeddings + original text metadata (embeddings alone not useful for developers)\n- Cosine distance measures similarity (lower = more similar)\n- Multiple chunks returned for comprehensive context\n- Implementation uses class VectorIndex as sample vector database\n\nExample Query: \"What did software engineering department do last year?\" returned section 2 (distance 0.71) and methodology section (distance 0.72) as top matches.\n\nLimitation: Current implementation has scenarios where workflow doesn't work optimally, requiring additional improvements.\n</note>\n\n<note title=\"BM25 Lexical Search\">\nBM25 = Best Match 25, a lexical search algorithm used in RAG pipelines for text-based search complementing semantic search.\n\nProblem with semantic search alone: Can return irrelevant results despite good performance. Example: searching \"incident 2023 Q4 011\" returned correct section 10 but also irrelevant section 3 (financial analysis) that never mentions the incident.\n\nSolution approach: Hybrid search combining semantic search (embeddings + vector database) with lexical search (BM25) running in parallel, then merging results for better balance.\n\nBM25 algorithm steps:\n1. Tokenize user query = break into individual terms by removing punctuation and splitting on spaces\n2. Count term frequency = calculate how often each term appears across all text chunks\n3. Assign importance weights = rare terms get higher importance, common terms (like \"a\") get lower importance\n4. Rank chunks = prioritize chunks containing higher-weighted terms more frequently\n\nKey advantage: BM25 prioritizes chunks with rare, specific terms (like \"incident 2023\") over generic common words, leading to more relevant results for exact term matches.\n\nImplementation: Both semantic and lexical search systems use similar APIs (add_document, search functions) enabling easy integration for hybrid search approach.\n\nNext step: Merge both search systems to get combined benefits of semantic understanding and exact term matching.\n</note>\n\n<note title=\"A Multi-Index Rag Pipeline\">\nMulti-Index RAG Pipeline = system combining semantic search (vector index) and lexical search (BM25 index) for improved retrieval accuracy.\n\nCore Architecture:\n- Vector Index + BM25 Index = both have identical APIs (add_document, search methods)\n- Retriever Class = wrapper that forwards queries to both indexes and merges results\n- Unified Interface = enables easy addition of new search methodologies\n\nReciprocal Rank Fusion = technique for merging results from different search methods:\n- Formula: Score = sum of (1/(1+rank)) for each search method\n- Process: collect all results → record ranks from each method → apply formula → sort by highest score\n- Example: if doc ranks 1st in vector search and 2nd in BM25, gets higher combined score than doc ranking 2nd and 3rd\n\nBenefits:\n- Improved relevance over single-method search\n- Modular design allows adding new search indexes\n- Maintains consistent API across all components\n\nKey Result: Hybrid approach produces better search results than either semantic or lexical search alone, particularly for complex queries requiring both conceptual understanding and keyword matching.\n</note>\n\n<note title=\"Reranking Results\">\nReranking = post-processing technique to improve retrieval accuracy by using an LLM to reorder search results based on relevance.\n\nProcess: After hybrid retrieval (vector + BM25), pass top results to Claude with prompt asking to rank documents by relevance to user query. Claude returns reordered list with most relevant documents first.\n\nImplementation details: Use document IDs instead of full text for efficiency - assign random IDs to chunks, ask Claude to return ordered IDs rather than full content. Uses XML formatting for documents in prompt, JSON response with pre-fill and stop sequences.\n\nTrade-offs: Increases search accuracy but adds latency due to LLM call. Particularly effective when semantic understanding needed (example: \"engineering team\" query correctly prioritized software engineering section over cybersecurity).\n\nPrompt structure: User question + candidate documents + instruction to return N most relevant documents in decreasing relevance order.\n</note>\n\n<note title=\"Contextual Retrieval\">\nContextual Retrieval = RAG improvement technique that adds document context to text chunks before vector storage.\n\nProblem: Document chunking removes context from original source, reducing retrieval accuracy.\n\nSolution: Pre-processing step using LLM to generate contextual information for each chunk.\n\nProcess:\n1. Take individual chunk + original source document\n2. Send to LLM (Claude) with prompt asking to situate chunk within larger document context\n3. LLM generates brief contextual description\n4. Combine generated context + original chunk = contextualized chunk\n5. Store contextualized chunk in vector/BM25 index\n\nLarge Document Handling: When source document too large for single prompt, use selective context:\n- Include starter chunks (1-3) for document summary/abstract\n- Include chunks immediately before target chunk for local context\n- Skip middle chunks that provide less relevant context\n\nImplementation: add_context() function takes chunk + source text, generates context via LLM, concatenates context with original chunk.\n\nBenefit: Maintains document relationships and cross-references within individual chunks, improving retrieval accuracy for complex documents with interconnected sections.\n</note>\n\n<note title=\"Extended Thinking\">\nExtended Thinking = Claude feature that allows reasoning time before generating final response\n\nKey mechanics:\n- Displays separate thinking process in chat UIs\n- User charged for tokens generated during thinking phase\n- Increases accuracy for complex tasks but adds cost and latency\n\nWhen to enable:\n- Use prompt evals first\n- Enable only when accuracy insufficient after prompt optimization efforts\n\nTechnical implementation:\n- Response contains thinking block + text block\n- Thinking block includes cryptographic signature to prevent tampering\n- Claude verifies thinking text hasn't been modified when reused in conversations\n- Minimum thinking budget = 1024 tokens\n- max_tokens must exceed thinking_budget (recommended significant buffer)\n\nSpecial cases:\n- Redacted thinking blocks occur when thinking text flagged by safety systems\n- Redacted content provided in encrypted form to preserve context\n- Test string available to force redacted thinking blocks for testing\n\nCode requirements:\n- Add thinking parameter (default false)\n- Add thinking_budget parameter (minimum 1024)\n- Include thinking object in API params with type=\"enabled\" and budget_tokens value\n- Ensure max_tokens >> thinking_budget for adequate response generation\n</note>\n\n<note title=\"Image Support\">\nClaude Vision Capabilities = ability to send up to 100 images per request to Claude for analysis, comparison, counting, and other visual tasks.\n\nTechnical Requirements = image size/dimension limits exist, images consume tokens based on pixel dimensions (height x width), costs calculated via specific equation.\n\nImage Integration = use image blocks in user messages containing either raw base64 image data or URLs to online images. Multiple image blocks allowed per message.\n\nCritical Success Factor = sophisticated prompting techniques essential for accurate results. Simple prompts typically fail.\n\nPrompting Techniques for Images = provide analysis steps, guidelines, one-shot/multi-shot examples. Structure prompts with specific sequential instructions rather than basic requests.\n\nExample Enhancement Methods = \n- Step-by-step analysis (identify objects, count methodically, verify count with different strategy, compare results)\n- One-shot prompting (show example image with correct answer, then present target image)\n\nMessage Structure = alternate image parts and text parts within single user message for examples and instructions.\n\nReal-world Application Example = wildfire risk assessment using satellite imagery to evaluate tree density, emergency access, roof overhang, and assign numerical risk scores.\n\nKey Takeaway = image accuracy depends entirely on prompt engineering quality, not just image clarity.\n</note>\n\n<note title=\"PDF Support\">\nClaude PDF Support = ability to read and process PDF file content directly\n\nImplementation = nearly identical to image processing code with key changes:\n- File type: \"document\" instead of \"image\" \n- Media type: \"application/pdf\" instead of \"image/png\"\n- Variable naming: file_bytes instead of image_bytes\n\nPDF capabilities = extract text, images, charts, tables, and other document elements\n\nUsage pattern = attach PDF file, modify existing image processing code with type/media changes, send to Claude with text prompt\n\nKey benefit = single tool for comprehensive PDF content extraction and analysis\n</note>\n\n<note title=\"Citations\">\nCitations = Claude feature allowing AI to reference specific source documents when generating responses\n\nCitation purpose = Inform users that AI response comes from actual source rather than just AI memory/training\n\nCitation types:\n- Citation page location = For PDF documents, includes cited text + document index + document title + start/end pages\n- Citation char location = For plain text, includes character position within text block\n\nImplementation = Add citations field with enabled:true to API request along with source document\n\nResponse structure = Content becomes list of text blocks, some containing citations arrays with location data\n\nUser interface benefit = Build popups/overlays showing users exactly which source text supports each AI statement\n\nSupported formats = PDF documents and plain text sources\n\nKey value = Users can verify AI interpretations by checking original source material, ensuring transparency and accuracy\n</note>\n\n<note title=\"Prompt Caching\">\nPrompt caching = feature that speeds up Claude's response and reduces text generation costs by reusing computational work from previous requests.\n\nNormal request process: User sends message → Claude performs extensive internal calculations on input → generates output → discards all computational work → ready for next request.\n\nProblem: When follow-up requests contain previously seen messages, Claude must redo identical calculations it already performed and discarded.\n\nSolution: Prompt caching stores computational work in temporary data store instead of discarding it. When identical input appears in subsequent requests, Claude reuses cached calculations rather than recomputing them.\n\nBenefits: Faster response times, lower costs due to avoiding redundant processing of repeated content.\n\nKey requirement: Input text must be exactly identical to previously cached content for reuse to occur.\n</note>\n\n<note title=\"Rules of Prompt Caching\">\nPrompt Caching = system where Claude saves processing work from initial request to reuse in follow-up requests with identical content\n\nCache Duration = 5 minutes temporary storage\n\nActivation = manual cache breakpoint required in message blocks, not enabled by default\n\nText Block Format = must use longhand format {type: \"text\", text: \"content\", cache_control: {...}} instead of shorthand string assignment to add cache control\n\nCache Scope = all content cached up to and including breakpoint, not content after breakpoint\n\nContent Identity Rule = follow-up requests must have identical content up to breakpoint or cache invalidated\n\nBreakpoint Locations = text blocks, image blocks, tool use, tool results, tool schemas, system prompts\n\nProcessing Order = tools → system prompt → messages (joined together behind scenes)\n\nMultiple Breakpoints = up to 4 total breakpoints allowed for granular caching control\n\nMinimum Cache Length = 1024 tokens required for content to be cached\n\nUse Cases = repetitive requests with same content, stable system prompts, unchanging tool schemas\n</note>\n\n<note title=\"Prompt Caching in Action\">\nPrompt Caching Implementation:\n\nCache Breakpoints = multiple cache points in single request (tools + system prompt + messages)\n\nTool Schema Caching:\n- Add cache_control field to LAST tool schema only\n- Best practice = copy tools list first, clone last tool, add cache_control type=\"ephemeral\"\n- Prevents accidental modification of original tool schemas\n\nSystem Prompt Caching:\n- Convert system string to list with text block containing cache_control type=\"ephemeral\"\n- Format: [{\"type\": \"text\", \"text\": system_content, \"cache_control\": {\"type\": \"ephemeral\"}}]\n\nCache Order = tools → system prompt → messages\n\nUsage Patterns:\n- cache_creation_input_tokens = tokens written to cache (first use)\n- cache_read_input_tokens = tokens read from cache (subsequent identical requests)\n- Any change to cached content invalidates cache, forces new cache_creation\n\nToken Counts Example:\n- Base message: ~14 tokens\n- Tool schemas: ~1.7K tokens  \n- System prompt: ~6.3K tokens\n\nCache invalidation = any modification to tools/system prompt creates completely new cache breakpoint\n\nUse Cases = identical content across requests (same tools, system prompts, message sequences)\n</note>\n\n<note title=\"Introducing MCP\">\nMCP = Model Context Protocol, communication layer providing Claude with context and tools without requiring developers to write tedious code.\n\nMCP Architecture = client-server model where server contains tools, resources, and prompts.\n\nPrimary Problem Solved = shifts burden of defining and running tools from developer's server to MCP server. Instead of authoring tool schemas and functions yourself, MCP server handles this.\n\nMCP Server = interface to outside service (like GitHub) that wraps functionality into pre-built tools. Eliminates need for developers to author/maintain tool implementations.\n\nWho Creates MCP Servers = anyone, but often service providers make official implementations (e.g., AWS releasing their own MCP server).\n\nMCP vs Direct API Calls = MCP saves development time by providing pre-built tool schemas and function implementations rather than requiring custom authoring.\n\nMCP vs Tool Use = complementary concepts, not identical. MCP focuses on who does the work (pre-built vs custom), while both involve tool usage. Common misconception stems from not understanding MCP's delegation aspect.\n\nKey Benefit = reduces developer burden of creating/maintaining integrations for complex services with extensive functionality.\n</note>\n\n<note title=\"MCP Clients\">\nMCP Client = communication interface between your server and MCP server, provides access to server's tools\n\nTransport agnostic = client/server can communicate via multiple protocols (stdin/stdout, HTTP, WebSockets, etc.)\n\nCommunication method = message exchange following MCP spec\n\nKey message types:\n- list tools request = client asks server for available tools\n- list tools result = server responds with tool list  \n- call tool request = client asks server to run specific tool with arguments\n- call tool result = server returns tool execution results\n\nTypical flow:\n1. User queries your server\n2. Server requests tool list from MCP client\n3. MCP client sends list tools request to MCP server\n4. MCP server responds with available tools\n5. Server sends user query + tools to Claude\n6. Claude requests tool execution\n7. Server asks MCP client to run tool\n8. MCP client sends call tool request to MCP server\n9. MCP server executes tool (e.g., GitHub API call)\n10. Results flow back through chain: MCP server → MCP client → your server → Claude → user\n\nMCP client role = intermediary that translates between your server and MCP server, doesn't execute tools directly\n</note>\n\n<note title=\"Project Setup\">\nProject Setup = CLI-based chatbot implementation to understand MCP client-server interaction\n\nComponents = MCP client + custom MCP server in single project\nDocuments = fake documents stored in memory only\nServer tools = read document contents, update document contents\n\nImportant note = typically projects implement either client OR server, not both. This project does both for learning purposes.\n\nSetup steps:\n- Download CLI project.zip starter code\n- Extract and open in code editor\n- Check readme.md for setup directions\n- Add API key to .env file\n- Install dependencies (with/without UV)\n- Run project: \"uv run main.py\" or \"python main.py\"\n- Verify chat prompt appears and responds to queries\n\nProject structure = all files in project directory, ready for feature additions\n</note>\n\n<note title=\"Defining Tools with MCP\">\nMCP Server Implementation = Creating server with Python SDK using single line of code (mcp package)\n\nTool Definition Syntax = Use @mcp.tool decorator with name, description, and typed arguments instead of manual JSON schemas\n\nPython MCP SDK Benefits = Auto-generates JSON schemas from decorators and field types, eliminates manual schema writing\n\nExample Tools Created = read_doc_contents (takes doc_id string, returns document content from in-memory docs dictionary) and edit_document (takes doc_id, old_string, new_string for find/replace operations)\n\nTool Structure = Decorator specifies metadata, function implements logic, Field() from pydantic adds argument descriptions\n\nError Handling = Check if doc_id exists in docs dictionary, raise ValueError if not found\n\nDocument Storage = In-memory dictionary with doc IDs as keys, content as values\n\nImplementation Pattern = @mcp.tool decorator → function definition → typed parameters with Field descriptions → validation logic → core functionality\n</note>\n\n<note title=\"The Server Inspector\">\nMCP Inspector = in-browser debugger for testing MCP servers without connecting to applications\n\nAccess: Run \\`mcp dev [server-file.py]\\` in terminal with Python environment activated → generates local server URL → open in browser\n\nKey features:\n- Connect button = starts MCP server\n- Top menu bar = shows resources, prompts, tools sections\n- Tools section = lists available tools from server\n- Right panel = manual tool invocation interface\n\nTesting workflow:\n1. Click Connect to start server\n2. Navigate to Tools → List Tools\n3. Select specific tool to test\n4. Input required parameters in right panel\n5. Click Run Tool to execute and verify output\n\nTool testing example:\n- read_doc_contents tool: input document ID → returns document contents\n- edit_document tool: input doc ID + old string + new string → modifies document\n- Verify edits by re-running read tool\n\nPurpose = live development and debugging of MCP servers during implementation phase. UI may change as tool is in active development but core functionality remains similar.\n</note>\n\n<note title=\"Implementing a Client\">\nMCP Client Implementation:\n\nMCP Client = wrapper class around client session for resource management and cleanup\nClient Session = actual connection to MCP server from Python SDK, requires resource cleanup\nClient Purpose = exposes server functionality to codebase, bridges server tools to application code\n\nKey Functions:\n- list_tools() = await self.session.list_tools(), returns result.tools\n- call_tool() = await self.session.call_tool(tool_name, tool_input), executes specific tool with Claude-provided parameters\n\nImplementation Pattern:\nClient wraps session → manages connection lifecycle → exposes server tools → enables Claude integration\n\nTesting: Direct file execution tests connection and tool listing functionality\n\nUsage Flow: Application calls client.list_tools() → sends to Claude → Claude requests tool execution → client.call_tool() → returns results to Claude\n\nResource Management: Client handles session cleanup via async context managers (connect, cleanup, async enter/exit functions)\n</note>\n\n<note title=\"Defining Resources\">\nMCP Resources = mechanism for MCP servers to expose data to clients for read operations\n\nResource Types:\n- Direct/Static Resources = fixed URI, always same address (e.g., docs://documents)\n- Templated Resources = parameterized URI with wildcards (e.g., documents/{doc_id})\n\nResource Flow:\n1. Client sends read resource request with URI to MCP server\n2. Server matches URI to defined resource function\n3. Server executes function and returns data via read resource result\n\nImplementation:\n- Use @mcp.resource decorator with URI and MIME type\n- MIME type = hint about data format (application/json, text/plain, etc.)\n- Templated resource parameters become function keyword arguments\n- MCP SDK auto-serializes return values to strings\n\nExample Use Case:\n- Resource 1: Return list of document names for autocomplete\n- Resource 2: Return specific document content by ID\n- Enables @ mention functionality where users reference documents that get auto-inserted into prompts\n\nKey Points:\n- One resource per distinct read operation\n- Resources expose data, don't modify it\n- Client responsible for deserializing returned data\n- Templated resources enable dynamic content selection\n</note>\n\n<note title=\"Accessing Resources\">\nMCP Resource Access Implementation:\n\nResource fetching = MCP client function reads resources from MCP server by URI, parses content based on MIME type, returns data\n\nKey imports = json module, AnyURL from pydantic for type handling\n\nFunction flow = await self.session.read_resource(AnyURL(uri)) → extract first content from results.contents[0] → check MIME type → parse accordingly\n\nContent parsing logic = if resource is TextResourceContents and mime_type == \"application/json\" → return json.loads(resource.text), else return resource.text as plain text\n\nResponse structure = result.contents list containing resource objects with type and mime_type properties\n\nIntegration = read_resource function called by other application components to fetch document contents for prompts\n\nTesting workflow = CLI shows resource list → user selects with arrow keys/space → resource content sent directly to Claude in prompt without requiring tool calls\n\nCore concept = Resources expose server information to clients, enabling direct content access rather than tool-based retrieval\n</note>\n\n<note title=\"Defining Prompts\">\nMCP Prompts = pre-defined, tested prompts exposed by MCP servers for specialized tasks\n\nPurpose = Allow server authors to create high-quality, domain-specific prompts rather than users writing generic prompts manually\n\nImplementation:\n- Use @prompt decorator with name and description\n- Function receives parameters (like document ID)\n- Returns list of messages (user/assistant format)\n- Import: from mcp.server.fastmcp.prompts import BaseMessage\n\nExample structure:\n\\`\\`\\`\n@prompt(name=\"format\", description=\"rewrites document in markdown\")\ndef format_document(doc_id: str) -> list[BaseMessage]:\n    return [BaseMessage.user(prompt_text)]\n\\`\\`\\`\n\nKey benefit = Server authors can create specialized, tested prompts for their domain (document management, etc.) that clients can use via slash commands or direct invocation\n\nFlow = Client requests prompt → Server returns message list → Client sends to LLM → LLM executes using available tools\n\nTesting = Use MCP development inspector to test prompts before deployment\n</note>\n\n<note title=\"Prompts in the Client\">\n**Prompts in MCP Client Implementation**\n\nClient-side prompt functions:\n- \\`list_prompts()\\` = await self.session.list_prompts(), return result.props\n- \\`get_prompt()\\` = await self.session.get_prompt(prompt_name, arguments), return result.messages\n\n**Prompt workflow:**\n1. Define prompt in MCP server with variables (e.g., document_id)\n2. Client calls get_prompt with prompt name + arguments dictionary\n3. Arguments interpolated into prompt function as keyword arguments\n4. Returns messages fed directly to Claude\n\n**Example usage:**\nFormat command → select document → prompt with document ID passed to Claude → Claude uses tools to fetch document → reformats with requested styling\n\n**Key concept:**\nPrompts = server-defined templates that clients can invoke with runtime arguments, enabling dynamic prompt generation with variable substitution.\n</note>\n\n<note title=\"MCP Review\">\nMCP Server Primitives = three core components with distinct control patterns and purposes.\n\nTools = model-controlled actions. Claude decides when to execute based on conversation needs. Purpose: add capabilities to AI model (e.g., JavaScript execution for calculations). Implementation: define tools in MCP server for model consumption.\n\nResources = app-controlled data access. Application code decides when to fetch and use data. Purpose: provide data for UI elements or prompt augmentation (e.g., autocomplete options, document listings, Google Drive integration). Implementation: fetch resources to populate app interfaces.\n\nPrompts = user-controlled workflows. Users trigger through UI buttons, menu options, or slash commands. Purpose: implement predefined workflows and optimized conversation starters. Implementation: create prompt templates for common user actions.\n\nControl Pattern Summary:\n- Tools serve the model\n- Resources serve the app \n- Prompts serve the users\n\nUsage Guidelines: Need AI capabilities → implement tools. Need app data → use resources. Need user workflows → create prompts.\n</note>\n\n<note title=\"Agents and Workflows\">\nWorkflows and agents = strategies for handling user tasks that can't be completed by Claude in a single request.\n\nDecision rule: Use workflows when you know exact task steps ahead of time. Use agents when task details are uncertain.\n\nWorkflow = series of calls to Claude for specific problems where all steps are predetermined and plannable.\n\nAgent = letting Claude figure out how to complete tasks using provided tools (like the tools examples in previous course modules).\n\nExample workflow: Image-to-3D model converter\n- Step 1: Claude describes uploaded metal part image in detail\n- Step 2: Claude uses CADQuery Python library to create 3D model from description\n- Step 3: Generate rendering of model as image\n- Step 4: Claude compares rendering to original image\n- Step 5: If inaccurate, return to Step 2 with feedback; if accurate, output step file\n\nThis follows evaluator-optimizer pattern:\n- Producer = creates output (Claude + CADQuery modeling)\n- Evaluator = judges output quality (Claude comparing images)\n- Loop continues until evaluator accepts output\n\nKey point: Workflows are just proven patterns other engineers have used successfully. You still must write the actual implementation code yourself.\n</note>\n\n<note title=\"Parallelization Workflows\">\nParallelization Workflows = breaking one complex task into multiple parallel subtasks, then aggregating results\n\nCore Pattern:\n- Split complex analysis into specialized parallel requests\n- Each subtask focuses on one specific aspect\n- Aggregator step combines all results into final output\n\nExample Implementation:\n- Original approach: single large prompt asking Claude to analyze image and recommend best material (metal/polymer/ceramic/etc.)\n- Parallelized approach: separate simultaneous requests, each evaluating image suitability for ONE material type\n- Final step: feed all individual analyses back to Claude for final recommendation\n\nKey Benefits:\n1. Focus = each request handles single task vs. juggling multiple considerations\n2. Optimization = can improve/evaluate each specialized prompt independently  \n3. Scalability = easy to add new subtasks without affecting existing ones\n\nStructure: Input → Multiple Parallel Subtasks → Aggregator → Final Output\n\nBest for scenarios where complex decision requires evaluating multiple independent criteria or options simultaneously.\n</note>\n\n<note title=\"Chaining Workflows\">\nChaining Workflows = breaking large tasks into sequential distinct steps rather than single complex prompts.\n\nKey structure: One main task → Multiple sequential subtasks → Each step feeds into next step.\n\nExample workflow: User topic input → Twitter trending search → Claude selects best topic → Claude researches topic → Claude writes script → AI creates video → Post to social media.\n\nPrimary benefit = allows AI to focus on one task at a time rather than juggling multiple requirements simultaneously.\n\nCritical use case: Large prompts with many constraints where AI consistently violates some requirements despite repeated instructions.\n\nSolution pattern: \n1. Send initial complex prompt with all constraints\n2. Accept imperfect output that violates some constraints  \n3. Follow-up prompt asking AI to fix specific violations in previous output\n4. AI focuses solely on correction tasks\n\nWhy this works = AI performs better when addressing focused corrections rather than managing numerous simultaneous constraints in single prompt.\n\nMost useful when = AI consistently ignores certain constraints despite clear instructions in complex prompts.\n</note>\n\n<note title=\"Routing Workflows\">\nRouting Workflows = workflow pattern that categorizes user input to determine appropriate processing pipeline\n\nProcess:\n1. User provides input/topic\n2. Routing step = AI call to categorize input into predefined genres/categories\n3. Based on category, input gets forwarded to specialized processing pipeline\n4. Each pipeline has customized prompts/tools for that category type\n\nExample: Social media video script generation\n- Programming topic → Educational category → Educational script prompt (clear explanations, definitions, examples)\n- Surfing topic → Entertainment category → Entertainment script prompt (trendy language, engaging hooks)\n\nKey components:\n- Predefined categories/genres for classification\n- Category-specific prompts tailored to expected output style\n- Single routing decision determines entire downstream processing\n- Each route can have different workflows/tools specialized for that task type\n\nPurpose = Ensures output matches nature/style appropriate for input type rather than one-size-fits-all approach\n</note>\n\n<note title=\"Agents and Tools\">\nAgents = AI systems that use tools to complete tasks when exact steps are unknown, unlike workflows which need precise predetermined steps.\n\nKey difference: Workflows = known step sequences, Agents = flexible tool combination for unknown step sequences.\n\nAgent advantages: Flexibility to solve wide variety of tasks using same toolset. Claude creates plans dynamically using available tools.\n\nTool abstraction principle: Agents need abstract/general tools rather than hyper-specialized ones. Example: Claude code uses bash, web fetch, file write (abstract) rather than specific refactor or install tools.\n\nTool combination examples: With get_current_datetime, add_duration, set_reminder tools, Claude can handle \"what time is it\" (single tool) or \"remind me gym next Wednesday\" (multiple tools + planning).\n\nDynamic interaction capability: Agents can request additional user information when needed, like asking warranty purchase date to calculate expiration.\n\nBest practice: Provide small set of abstract tools that can be combined creatively rather than many specialized single-purpose tools.\n\nReal example: Social media video agent with bash/FFmpeg, generate_image, text_to_speech, post_media tools enables both simple \"create video\" and complex \"generate sample cover first for approval\" workflows.\n</note>\n\n<note title=\"Environment Inspection\">\nEnvironment Inspection = agents examining their environment state after (or before) taking actions to understand results and progress.\n\nCore concept: Agents need feedback beyond tool return values to understand action outcomes and current state.\n\nComputer use example: Claude takes screenshot after every action (typing, clicking) because it cannot predict how actions change the environment. Button clicks might navigate pages or open menus - screenshot reveals new state.\n\nCode editing example: Before modifying files, agents must read current file contents to understand existing state.\n\nSocial media video agent applications:\n- Use Whisper CPP via bash tool to generate timestamped captions, verify audio placement\n- Use FFmpeg to extract video screenshots at intervals, verify visual output\n- Inspect generated videos to ensure task completion quality\n\nBenefits: Better progress tracking, error handling, unexpected result management through environment state awareness.\n</note>\n\n<note title=\"Workflows vs Agents\">\nWorkflows = pre-defined series of calls to Claude for tasks with known step sequences. Agents = flexible systems using basic tools that Claude combines creatively for unknown tasks.\n\nKey differences:\n\nTask approach: Workflows divide big tasks into smaller, specific subtasks for focused execution. Agents handle varied challenges through creative tool combination.\n\nAccuracy: Workflows achieve higher accuracy due to focused, specific steps. Agents have lower successful completion rates due to delegated complexity.\n\nTesting: Workflows easier to test/evaluate with known step sequences. Agents harder to test due to unpredictable execution paths.\n\nFlexibility: Workflows require specific inputs, fixed sequences. Agents adapt to varied user queries, can request additional input, flexible UX.\n\nReliability: Workflows more reliable for consistent task completion. Agents more experimental but less dependable.\n\nRecommendation: Prioritize workflows when possible for reliable problem-solving. Use agents only when flexibility truly required. Users want 100% working products over fancy but unreliable agents.\n</note>\n</notes>"
};

const aiFluencyForEducators = {
  path: "/ai-fluency-for-educators",
  previewId: "77953",
  usesAlternateLayout: true,
  llmContentKey: "aif4ed",
  title: "AI Fluency for Educators",
  subtitle:
    "This course empowers faculty, instructional designers, and educational leaders to apply AI Fluency into their own teaching practice and institutional strategy.",
  overview: {
    description:
      "<p>At Anthropic, we believe that empowering people with AI, and ensuring that AI makes safe contributions to society, requires engaging with a wide range of human perspectives and experiences. Responsible AI development and engagement isn't something any single discipline or viewpoint can fully address. It demands collaborative approaches that span a wide range of technical, creative, business, scientific, and educational domains. That's why  we partnered with educators who bring complementary expertise to create this course on AI collaboration for educators.</p>  <p>This course empowers faculty, instructional designers, and educational leaders to apply the 4D Framework to their own educational practice. Participants will learn to apply the 4D Framework to their own teaching practice, using AI as a thinking partner to enhance course design, create coherent learning materials, and develop authentic assessments while modeling responsible AI collaboration for their students.</p>  <p>This course is the result of a long partnership between Anthropic and professors Rick Dakan from Ringling College of Art and Design and Joseph Feller from University College Cork. It builds on their experiences in training fellow educators and teaching the AI Fluency Framework to their students, and on feedback and questions that arose from our earlier course: AI Fluency: Framework & Foundations  This course answers the question: how can I apply this framework to my personal teaching practice?</p>  <p>The work was supported in part by the Higher Education Authority (Ireland) through the National Forum for the Enhancement of Teaching and Learning.</p>  <h4>Recommended prerequisites</h4>  <p>This course lightly covers the foundational AI Fluency concepts. However, for deeper understanding, participants should complete AI Fluency: Framework & Foundations before beginning this educator-focused curriculum.</p>  <p>It is also recommended that learners have active teaching or curriculum development responsibilities as well as access to an AI chat tool for hands-on practice. Examples in this course will use Claude.ai, but any chatbot will work.</p>",
  },
  stats: {
    lectureCount: 4,
    videoHours: 0.4,
  },
  coreLearningAreas: [],

  sections: [
    {
      id: "section-1",
      title: "Introduction and AI Fluency Framework",
      lessonCount: 2,
      description:
        "A summary of Delegation, Description, Discernment, and Diligence applied to student contexts",
      screenshots: [
        {
          id: "s1-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-educators/01-01-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-educators/01-01.webp",
          alt: "Section 1 screenshot 1",
        },
        {
          id: "s1-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-educators/01-02-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-educators/01-02.webp",
          alt: "Section 1 screenshot 2",
        },
        {
          id: "s1-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-educators/01-03-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-educators/01-03.webp",
          alt: "Section 1 screenshot 3",
        },
      ],
    },
    {
      id: "section-2",
      title: "AI Fluency Framework applications for educators",
      lessonCount: 2,
      description:
        "Applying the 4D framework to course design and learning materials.",
      screenshots: [
        {
          id: "s2-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-educators/02-01-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-educators/02-01.webp",
          alt: "Section 2 screenshot 1",
        },
        {
          id: "s2-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-educators/02-02-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-educators/02-02.webp",
          alt: "Section 2 screenshot 2",
        },
        {
          id: "s2-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-educators/02-03-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-educators/02-03.webp",
          alt: "Section 2 screenshot 3",
        },
      ],
    },
  ],
};

if (typeof window._clpdata !== "object") {
  window._clpdata = {};
}
window._clpdata[aiFluencyForEducators["path"]] = aiFluencyForEducators;


const aiFluencyForNonprofits = {
  path: "/ai-fluency-for-nonprofits",
  previewId: "89476",
  usesAlternateLayout: true,
  title: "AI Fluency for nonprofits",
  subtitle:
    "This course empowers nonprofit professionals to develop AI fluency in order to increase organizational impact and efficiency while staying true to their mission and values.",
  overview: {
    description: `<p>At Anthropic, we believe that empowering people with AI, and ensuring that AI makes safe contributions to society, requires engaging with a wide range of human perspectives and experiences. Nonprofit professionals are uniquely positioned to leverage AI for social good—but only if they can approach it intentionally, with resilience and clear purpose.</p>

<p>This course helps nonprofit staff—whether in fundraising, communications, program delivery, operations, or leadership—build practical AI collaboration skills through the 4D Framework (Delegation, Description, Discernment, and Diligence). In this course, we'll explore how the framework applies to common non-profit tasks, and consider what it means to implement AI across a nonprofit organization.</p>

<p>This course is the result of a partnership between Anthropic and GivingTuesday, drawing on research with nonprofit professionals about their actual needs, concerns, and aspirations for AI adoption.</p>

<p>The work builds on our AI Fluency: Framework & Foundations course, adapted specifically for the nonprofit context where limited resources, multiple stakeholder accountabilities, and mission-driven work create unique considerations for AI collaboration.</p>

<h4>Recommended prerequisites</h4>

<p>This course lightly covers the foundational AI Fluency concepts. However, for deeper understanding, participants should complete <em>AI Fluency: Framework & Foundations</em> before beginning this nonprofit-focused curriculum.</p>

<p>It is also recommended that learners have access to an AI chat tool for hands-on practice. Examples in this course will use Claude.ai, but any chatbot will work.</p>`,
  },
  stats: {
    lectureCount: 9,
    videoHours: 0.9,
    quizCount: 1,
  },
  instructors: [
    {
      name: "Kelsey Kramer",
      avatar: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-nonprofits/kelsey.webp",
      description:
        "Kelsey directs partnerships for the GivingTuesday Data Commons, supporting a vibrant network of collaborators that power the Data Commons' mission to utilize data to build a more resilient social sector. She engages data and research partners, movement leaders, and strategic collaborators to build new initiatives, support ongoing research, and connect research to practice—bringing 10 years of experience from nonprofits and technology companies large and small.",
    },
    {
      name: "Zoe Ludwig",
      avatar: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-nonprofits/zoe.webp",
      description:
        "Zoe Ludwig leads Anthropic's Claude apps education, including AI Fluency work. She has a range of experience in educational roles including classroom instruction, curriculum design, and instructor led training. Prior to Anthropic, she founded and led the customer education team at Notion.",
    },
  ],
  sections: [
    {
      id: "section-1",
      title: "Introduction",
      lessonCount: 2,
      description: "Define AI Fluency and learn the four interconnected competencies—Delegation, Description, Discernment, and Diligence—that form the foundation of this course.",
      screenshots: [
        {
          id: "s1-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-nonprofits/s1-1-medium.webp",
          fullSizeUrl: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-nonprofits/s1-1.webp",
          alt: "Section 1 screenshot 1",
        },
        {
          id: "s1-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-nonprofits/s1-2-medium.webp",
          fullSizeUrl: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-nonprofits/s1-2.webp",
          alt: "Section 1 screenshot 2",
        },
        {
          id: "s1-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-nonprofits/s1-3-medium.webp",
          fullSizeUrl: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-nonprofits/s1-3.webp",
          alt: "Section 1 screenshot 3",
        },
      ],
    },
    {
      id: "section-2",
      title: "Description-Discernment",
      lessonCount: 2,
      description: "Practice crafting context-rich prompts and critically evaluating AI outputs through research and writing examples.",
      screenshots: [
        {
          id: "s2-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-nonprofits/s2-1-medium.webp",
          fullSizeUrl: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-nonprofits/s2-1.webp",
          alt: "Section 2 screenshot 1",
        },
        {
          id: "s2-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-nonprofits/s2-2-medium.webp",
          fullSizeUrl: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-nonprofits/s2-2.webp",
          alt: "Section 2 screenshot 2",
        },
        {
          id: "s2-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-nonprofits/s2-3-medium.webp",
          fullSizeUrl: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-nonprofits/s2-3.webp",
          alt: "Section 2 screenshot 3",
        },
      ],
    },
    {
      id: "section-3",
      title: "Delegation-Diligence",
      lessonCount: 2,
      description: "Make thoughtful decisions about what tasks to delegate to AI while taking responsibility for high-stakes considerations like data privacy and analytical accuracy.",
      screenshots: [
        {
          id: "s3-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-nonprofits/s3-1-medium.webp",
          fullSizeUrl: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-nonprofits/s3-1.webp",
          alt: "Section 3 screenshot 1",
        },
        {
          id: "s3-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-nonprofits/s3-2-medium.webp",
          fullSizeUrl: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-nonprofits/s3-2.webp",
          alt: "Section 3 screenshot 2",
        },
        {
          id: "s3-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-nonprofits/s3-3-medium.webp",
          fullSizeUrl: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-nonprofits/s3-3.webp",
          alt: "Section 3 screenshot 3",
        },
      ],
    },
    {
      id: "section-4",
      title: "Putting it all together",
      lessonCount: 3,
      description: "Apply all four dimensions of the 4D Framework to automate workflows and integrate AI thoughtfully into your organization.",
      screenshots: [
        {
          id: "s4-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-nonprofits/s4-1-medium.webp",
          fullSizeUrl: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-nonprofits/s4-1.webp",
          alt: "Section 4 screenshot 1",
        },
        {
          id: "s4-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-nonprofits/s4-2-medium.webp",
          fullSizeUrl: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-nonprofits/s4-2.webp",
          alt: "Section 4 screenshot 2",
        },
        {
          id: "s4-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-nonprofits/s4-3-medium.webp",
          fullSizeUrl: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-nonprofits/s4-3.webp",
          alt: "Section 4 screenshot 3",
        },
      ],
    },
  ],
};

if (typeof window._clpdata !== "object") {
  window._clpdata = {};
}
window._clpdata[aiFluencyForNonprofits["path"]] = aiFluencyForNonprofits;


const aiFluencyForStudents = {
  path: "/ai-fluency-for-students",
  previewId: "77955",
  usesAlternateLayout: true,
  llmContentKey: "aif4students",
  title: "AI Fluency for Students",
  subtitle:
    "This course empowers students to develop AI Fluency skills that enhance learning, career planning, and academic success through responsible AI collaboration.",
  overview: {
    description:
      "<p>You've probably tried AI—maybe to help with an essay, solve  a problem, or just explore what it can do. But there's a  difference between using AI and being fluent with it. This  course teaches students how to collaborate with AI  effectively, efficiently, ethically, and safely across learning  and career contexts.</p>    <p>Rather than focusing on shortcuts or prompt tricks, this  course develops lasting skills through the 4D Framework  (Delegation, Description, Discernment, and Diligence).  Students learn to use AI as a thinking partner that enhances  their learning and career development rather than replacing  their own critical thinking and creativity.</p>    <p>Through practical applications in academic work and career  planning, students discover how to leverage AI to  understand concepts more deeply, develop professional  skills, and prepare for a future where AI Fluency is essential.  The course emphasizes being 'the human in the loop'—  maintaining agency, judgment, and responsibility while  working thoughtfully with AI systems.</p>    <p>This course is the result of a long partnership between  Anthropic and professors Rick Dakan from Ringling College  of Art and Design and Joseph Feller from University College  Cork. It addresses the fundamental question students face:  how can I use AI to genuinely enhance my learning and  career success without compromising my own growth and  integrity?</p>    <p>The work was supported in part by the Higher Education  Authority (Ireland) through the National Forum for the  Enhancement of Teaching and Learning.</p>    <h4>Recommended prerequisites</h4>    <p>This course lightly covers the foundational AI Fluency  concepts. However, for deeper understanding, participants  should complete AI Fluency: Framework & Foundations  before beginning this student-focused curriculum. </p>    <p>It is also recommended that learners have access to an AI  chat tool for hands-on practice. Examples in this course will  use Claude.ai, but any chatbot will work.</p>",
  },
  stats: {
    lectureCount: 5,
    videoHours: 0.5,
  },
  coreLearningAreas: [],

  sections: [
    {
      id: "section-1",
      title: "Introduction and AI Fluency Framework",
      lessonCount: 2,
      description:
        "A summary of Delegation, Description, Discernment, and Diligence applied to student contexts",
      screenshots: [
        {
          id: "s1-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-students/01-01-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-students/01-01.webp",
          alt: "Section 1 screenshot 1",
        },
        {
          id: "s1-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-students/01-02-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-students/01-02.webp",
          alt: "Section 1 screenshot 2",
        },
        {
          id: "s1-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-students/01-03-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-students/01-03.webp",
          alt: "Section 1 screenshot 3",
        },
      ],
    },
    {
      id: "section-2",
      title: "AI Fluency Framework applications for students",
      lessonCount: 2,
      description: "Applying the 4D framework to learning and career planning.",
      screenshots: [
        {
          id: "s2-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-students/02-01-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-students/02-01.webp",
          alt: "Section 2 screenshot 1",
        },
        {
          id: "s2-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-students/02-02-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-students/02-02.webp",
          alt: "Section 2 screenshot 2",
        },
        {
          id: "s2-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-students/02-03-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-students/02-03.webp",
          alt: "Section 2 screenshot 3",
        },
      ],
    },
    {
      id: "section-3",
      title: "Conclusion",
      lessonCount: 1,
      description:
        "Developing personal commitments for responsible AI collaboration and maintaining agency.",
      screenshots: [
        {
          id: "s2-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-students/03-01-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-students/03-01.webp",
          alt: "Section 3 screenshot 1",
        },
        {
          id: "s2-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-students/03-02-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-students/03-02.webp",
          alt: "Section 3 screenshot 2",
        },
        {
          id: "s2-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-students/03-03-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-for-students/03-03.webp",
          alt: "Section 3 screenshot 3",
        },
      ],
    },
  ],
};

if (typeof window._clpdata !== "object") {
  window._clpdata = {};
}
window._clpdata[aiFluencyForStudents["path"]] = aiFluencyForStudents;


const aiFluencyFrameworkFoundationsData = {
  path: "/ai-fluency-framework-foundations",
  previewId: "68809",
  usesAlternateLayout: true,
  title: "AI Fluency: Framework & Foundations",
  subtitle:
    "Learn to collaborate with AI systems effectively, efficiently, ethically, and safely",
  overview: {
    description:
      "<p>At Anthropic, we believe that empowering people with AI, and ensuring that AI makes safe contributions to society, requires engaging with a wide range of human perspectives and experiences. Responsible AI development and engagement isn't something any single discipline or viewpoint can fully address. It demands collaborative approaches that span a wide range of technical, creative, business, scientific, and educational domains. That's why we partnered with educators who bring complementary expertise to create this foundational course on AI collaboration.</p><p>This course is the result of a long partnership between Anthropic and professors Rick Dakan from Ringling College of Art and Design and Joseph Feller from University College Cork. Rick and Joe developed the AI Fluency Framework in 2023-2024, based on their research exploring how AI tools like Claude were transforming creative and business processes. When we saw the framework, we immediately recognized a shared vision: helping people interact with AI effectively and responsibly, beyond just “cool prompts.” Their framework offered exactly the kind of multidisciplinary perspective we believe is essential for navigating AI’s impact on society. </p><p>The AI Fluency Framework they created — four interconnected competencies (Delegation, Description, Discernment, and Diligence) — enables more effective, efficient, ethical, and safe human-AI collaboration, regardless of which new AI models or tools emerge. We collaborated to develop this course based on their framework, bringing together our collective expertise in AI systems, education, creativity, and business innovation. The work was supported in part by the Higher Education Authority (Ireland) through the National Forum for the Enhancement of Teaching and Learning. </p><p>This framework has already informed undergraduate and postgraduate courses at both Ringling College and University College Cork, as well as staff training initiatives and community outreach events. Now, we’re excited to share these insights more broadly through this open online course.</p><p>Our goal is to make AI Fluency accessible and useful to everyone, no matter what stage of AI expertise you find yourself at. We hope you find it valuable in navigating the evolving landscape of AI collaboration.</p>",
  },
  stats: {
    lectureCount: 14,
    videoHours: 1.1,
    quizCount: 1,
  },
  instructors: [
    {
      name: "Drew Bent",
      description:
        "Drew leads education research at Anthropic. He previously co-founded the tutoring non-profit Schoolhouse.world with Sal Khan, which he ran from 2020-24 and now sits on the board. Prior to that, he wrote code at Khan Academy, taught high school math, and has been tutoring students for over a decade. Drew has degrees in physics & CS from MIT, and an education master's from Stanford.",
      avatar:
        "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-framework-foundations/drew_bent_o-medium.webp",
    },
    {
      name: "Rick Dakan",
      description:
        "Rick is the AI Coordinator and a professor at Ringling College of Art and Design in Sarasota, Florida where he teaches creative writing, interactive experience design, and AI courses. He also oversees the college's <i>Undergraduate Certificate in Artificial Intelligence</i> and the <i>Professional Certificate in Fundamentals of AI for Creatives</i>. He is a game designer and author of more than thirty games and books from video games and tabletop games to novels, nonfiction, and comics.",
      avatar:
        "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-framework-foundations/rick_dakan_o-medium.webp",
    },

    {
      name: "Joseph Feller",
      description:
        "Joseph is <i>Professor of Information Systems and Digital Transformation</i> at the Cork University Business School, University College Cork, Ireland. His current work focuses on AI-human hybrid creativity, innovation, and learning. His research has been published in <i>Information Systems Research</i>, <i>Journal of MIS</i>, <i>Journal of the AIS</i>, <i>Journal of Information Technology</i>, <i>Information Systems Journal</i>, <i>European Journal of Information Systems</i>, and <i>Journal of Strategic Information Systems</i>, and has been funded by the European Commission, Irish Research Council, Irish HEA, and other funding bodies.</p>",
      avatar:
        "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-framework-foundations/joseph_feller_o-medium.webp",
    },
    {
      name: "Maggie Vo",
      description:
        "Maggie founded and leads Anthropic's education team. She has a varied applied research background from Harvard University, with a career and education that spans fields such as game design, organizational behavior, tech and consumer goods, and human behavioral psychology. Maggie has held previous roles at top consulting firms and innovative startups alike. Prior to Anthropic, she worked in AI strategy consulting.</p>",
      avatar:
        "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-framework-foundations/maggie_vo_o-medium.webp",
    },
  ],
  extra: {
    title: "AI diligence statement",
    content:
      "<p>In the development of the AI Fluency: Framework and Foundations course, we engaged in extensive collaboration with Claude 3.7 from Anthropic.</p><p>The base content for this course came from:</p><ul><li>The AI Fluency Framework Practical Summary Document by Rick Dakan (at Ringling College of Art and Design) and Joseph Feller (at University College Cork) and related working documents and research notes</li><li>Slide decks and lecture transcripts from multiple university courses, guest lectures, and research talks delivered by Feller and/or Dakan</li><li>Technical/practical content provided by Maggie Vo and Drew Bent (Anthropic)</li></ul><p>Throughout this process, Claude assisted one or more of the human authors with structural development, resource and exercise design, and content drafting, critiquing, editing and rewriting. The human authors wrote, designed, edited and provided continual vision, expertise, critical judgment, and domain knowledge and made all final decisions about both content and approach.</p><p>All AI-generated and co-created content underwent thorough validation, editing, and curation by the human authors. The final materials accurately reflect the human authors' understanding, expertise, and intended pedagogical approach. While AI assistance was instrumental in producing these materials, the human authors maintain responsibility for the content.</p><p>This disclosure is made in the spirit of transparency advocated by the AI Fluency Framework and to acknowledge the evolving role of AI in educational content development and other creative and intellectual work.</p>",
  },
  coreLearningAreas: [
    {
      id: "fundamentals",
      title: "Claude Code fundamentals",
      colorAccent: "#b4c6d4",
      topics: [
        "How coding assistants work",
        "Setting up Claude Code in your projects",
        "Using /init to understand your codebase",
        "Managing context with CLAUDE.md files",
        "Architecting big changes with plan mode",
      ],
    },
    {
      id: "advanced-workflows",
      title: "Advanced development workflows",
      colorAccent: "#c5bfd9",
      topics: [
        "Automated browser control",
        "Creating custom commands for repetitive tasks",
        "Extending with MCP servers",
        "GitHub Actions integration",
        "Techniques for context control",
      ],
    },
  ],

  sections: [
    {
      id: "section-1",
      title: "AI Fundamentals & Framework",
      lessonCount: 10,
      description:
        "Establish foundational understanding of generative AI systems and why developing AI fluency matters for effective collaboration. Introduces the 4D Framework as a structured approach to human-AI interaction, covering core capabilities and limitations of current AI technologies. Provides the conceptual grounding needed to approach AI tools strategically rather than reactively.",
      screenshots: [
        {
          id: "s1-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-framework-foundations/01-01-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-framework-foundations/01-01.webp",
          alt: "Three ways to interact with ai",
        },
        {
          id: "s1-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-framework-foundations/01-02-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-framework-foundations/01-02.webp",
          alt: "key takeaways",
        },
        {
          id: "s1-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-framework-foundations/01-03-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-framework-foundations/01-03.webp",
          alt: "three pillars that make ai possible",
        },
      ],
    },
    {
      id: "section-2",
      title: "Practical AI Skills",
      lessonCount: 10,
      description:
        "Develop hands-on competencies for effective AI collaboration through the four core areas of the framework: delegation, description, discernment, and diligence. Learn systematic approaches to project planning with AI, crafting effective prompts, evaluating outputs critically, and iterating through the description-discernment loop. Emphasizes practical application across creative, business, and educational contexts.",
      screenshots: [
        {
          id: "s1-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-framework-foundations/02-01-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-framework-foundations/02-01.webp",
          alt: "product descriptions",
        },
        {
          id: "s1-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-framework-foundations/02-02-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-framework-foundations/02-02.webp",
          alt: "foundational prompting tips",
        },
        {
          id: "s1-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-framework-foundations/02-03-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/ai-fluency-framework-foundations/02-03.webp",
          alt: "creation diligence",
        },
      ],
    },
  ],
};

if (typeof window._clpdata !== "object") {
  window._clpdata = {};
}
window._clpdata[aiFluencyFrameworkFoundationsData["path"]] =
  aiFluencyFrameworkFoundationsData;


const claudeCodeInActionData = {
  path: "/claude-code-in-action",
  llmContentKey: "claudecode",
  title: "Claude Code in Action",
  subtitle:
    "Practical walkthrough of using Claude Code to accelerate your development workflow",
  overview: {
    description:
      "This course covers Claude Code, a command-line AI assistant that uses language models to perform development tasks. You'll learn how Claude Code reads files, executes commands, and modifies code through its tool system, along with techniques for managing context, creating custom workflows, extending Claude Code with hooks, and integrating with external services.",
    learningObjectives: [
      "Use Claude Code's core tools for file manipulation, command execution, and code analysis",
      "Manage context effectively using /init, Claude.md files, and @ mentions",
      "Control conversation flow with a variety of hotkeys and commands",
      "Enable Plan Mode and Thinking Mode for complex tasks requiring deeper analysis",
      "Create custom commands for automating repetitive development workflows",
      "Extend Claude Code with MCP servers to add browser automation and other capabilities",
      "Set up GitHub integration for automated PR reviews and issue handling",
      "Write hooks to add additional behavior into Claude Code",
    ],
    prerequisites: [
      "Basic familiarity with command line interfaces",
      "Access to Claude Code and an API key",
    ],
    targetAudience:
      "Engineers who want to speed up their development workflow with AI assistance",
  },
  stats: {
    lectureCount: 15,
    videoHours: 1.0,
    quizCount: 1,
  },
  coreLearningAreas: [
    {
      id: "fundamentals",
      title: "Claude Code fundamentals",
      colorAccent: "#b4c6d4",
      topics: [
        "How coding assistants work",
        "Setting up Claude Code in your projects",
        "Using /init to understand your codebase",
        "Managing context with CLAUDE.md files",
        "Architecting big changes with plan mode",
      ],
    },
    {
      id: "advanced-workflows",
      title: "Advanced development workflows",
      colorAccent: "#c5bfd9",
      topics: [
        "Automated browser control",
        "Creating custom commands for repetitive tasks",
        "Extending with MCP servers",
        "GitHub Actions integration",
        "Techniques for context control",
      ],
    },
  ],

  sections: [
    {
      id: "section-1",
      title: "Claude Code in action",
      lessonCount: 10,
      description:
        "Complete guide to using Claude Code effectively. Starts with understanding how coding assistants work, then moves through setup, context management, making changes, and advanced features like MCP servers, GitHub integration, and hook implementations.",
      screenshots: [
        {
          id: "s1-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-code-in-action/01-01-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-code-in-action/01-01.webp",
          alt: "Claude Code setup and initialization",
        },
        {
          id: "s1-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-code-in-action/01-02-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-code-in-action/01-02.webp",
          alt: "Working with context and file mentions",
        },
        {
          id: "s1-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-code-in-action/01-03-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-code-in-action/01-03.webp",
          alt: "MCP servers and GitHub integration",
        },
      ],
    },
  ],
};

if (typeof window._clpdata !== "object") {
  window._clpdata = {};
}
window._clpdata[claudeCodeInActionData["path"]] = claudeCodeInActionData;


const claudeWithAmazonBedrock = {
  path: "/claude-in-amazon-bedrock",
  llmContentKey: "bedrock",
  title: "Claude with Amazon Bedrock",
  subtitle:
    "This comprehensive course covers the full spectrum of working with Anthropic models using Amazon Bedrock",
  overview: {
    description:
      "This course covers using Claude models through AWS Bedrock API, from basic requests through advanced agent implementations. You'll learn to make API calls, implement tool use, build RAG pipelines, work with MCP servers, and leverage features like Claude Code and computer use for automation.",
    learningObjectives: [
      "Make requests to Claude models via AWS Bedrock using boto3",
      "Implement multi-turn conversations, streaming responses, and structured data extraction",
      "Build and evaluate prompts using automated testing pipelines with objective scoring",
      "Create custom tools and handle multi-step tool execution workflows",
      "Design RAG systems with text chunking, embeddings, and hybrid search (semantic + BM25)",
      "Connect Claude to external services using MCP (Model Context Protocol) servers",
      "Use Claude Code for automated development workflows and parallelized task execution",
      "Configure and optimize features like prompt caching, extended thinking, and image processing",
      "Implement computer use for automated testing and UI interaction",
    ],
    prerequisites: [
      "Proficiency in Python programming",
      "Basic knowledge of handling JSON data",
      "AWS account with Bedrock access",
    ],
    targetAudience: "Devs who want to add AI features to their apps",
  },
  stats: {
    lectureCount: 85,
    videoHours: 8.0,
    quizCount: 10,
  },
  coreLearningAreas: [
    {
      id: "getting-started",
      title: "Getting started with Claude",
      colorAccent: "#b4c6d4",
      topics: [
        "Authentication and API key management",
        "Multi-turn conversation handling",
        "Temperature and output control",
        "System prompt configuration",
        "Request/response fundamentals",
      ],
    },
    {
      id: "advanced-implementation",
      title: "Advanced implementation techniques",
      colorAccent: "#c5bfd9",
      topics: [
        "Custom tool creation and schemas",
        "RAG pipelines and embeddings",
        "Prompt caching and optimization",
        "Model Context Protocol (MCP)",
        "Multi-modal features",
      ],
    },
    {
      id: "production-ready",
      title: "Production-ready development",
      colorAccent: "#b5c5c0",
      topics: [
        "Evaluation frameworks and testing",
        "Prompt engineering best practices",
        "Agent and workflow architecture",
        "Development with Claude Code",
        "Automation with Computer Use",
      ],
    },
  ],

  sections: [
    {
      id: "section-2",
      title: "Getting started with Amazon Bedrock",
      lessonCount: 16,
      description:
        "Start here for the fundamentals. Covers API authentication, basic requests, conversation management, system prompts, and structured output generation.",
      screenshots: [
        {
          id: "s2-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-in-amazon-bedrock/02-01-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-in-amazon-bedrock/02-01.webp",
          alt: "Section 2 screenshot 1",
        },
        {
          id: "s2-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-in-amazon-bedrock/02-02-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-in-amazon-bedrock/02-02.webp",
          alt: "Section 2 screenshot 2",
        },
        {
          id: "s2-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-in-amazon-bedrock/02-03-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-in-amazon-bedrock/02-03.webp",
          alt: "Section 2 screenshot 3",
        },
      ],
    },
    {
      id: "section-3",
      title: "Prompt engineering & evaluation",
      lessonCount: 16,
      description:
        "Learn to write prompts that actually work. Focuses on prompting strategies, evaluation frameworks, and systematic testing approaches.",
      screenshots: [
        {
          id: "s3-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-in-amazon-bedrock/03-01-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-in-amazon-bedrock/03-01.webp",
          alt: "Section 3 screenshot 1",
        },
        {
          id: "s3-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-in-amazon-bedrock/03-02-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-in-amazon-bedrock/03-02.webp",
          alt: "Section 3 screenshot 2",
        },
        {
          id: "s3-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-in-amazon-bedrock/03-03-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-in-amazon-bedrock/03-03.webp",
          alt: "Section 3 screenshot 3",
        },
      ],
    },
    {
      id: "section-4",
      title: "Tool use with Claude",
      lessonCount: 14,
      description:
        "Extend Claude with custom tools and functions. Build apps with function calling, multi-turn tool interactions, batch tool calling, and leverage built-in utilities.",
      screenshots: [
        {
          id: "s4-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-in-amazon-bedrock/04-01-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-in-amazon-bedrock/04-01.webp",
          alt: "Section 4 screenshot 1",
        },
        {
          id: "s4-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-in-amazon-bedrock/04-02-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-in-amazon-bedrock/04-02.webp",
          alt: "Section 4 screenshot 2",
        },
        {
          id: "s4-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-in-amazon-bedrock/04-03-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-in-amazon-bedrock/04-03.webp",
          alt: "Section 4 screenshot 3",
        },
      ],
    },
    {
      id: "section-5",
      title: "Retrieval augmented generation",
      lessonCount: 10,
      description:
        "Implementation guide for production RAG systems. Covers text chunking, embeddings, hybrid search with BM25, multi-index architectures, reranking, and contextual retrieval.",
      screenshots: [
        {
          id: "s5-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-in-amazon-bedrock/05-01-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-in-amazon-bedrock/05-01.webp",
          alt: "Section 5 screenshot 1",
        },
        {
          id: "s5-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-in-amazon-bedrock/05-02-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-in-amazon-bedrock/05-02.webp",
          alt: "Section 5 screenshot 2",
        },
        {
          id: "s5-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-in-amazon-bedrock/05-03-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-in-amazon-bedrock/05-03.webp",
          alt: "Section 5 screenshot 3",
        },
      ],
    },
    {
      id: "section-6",
      title: "Model Context Protocol (MCP)",
      lessonCount: 12,
      description:
        "The protocol for building modular AI applications. Define custom tools and resources, implement MCP servers and clients, handle the full integration lifecycle.",
      screenshots: [
        {
          id: "s6-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-in-amazon-bedrock/06-01-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-in-amazon-bedrock/06-01.webp",
          alt: "Section 6 screenshot 1",
        },
        {
          id: "s6-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-in-amazon-bedrock/06-02-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-in-amazon-bedrock/06-02.webp",
          alt: "Section 6 screenshot 2",
        },
        {
          id: "s6-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-in-amazon-bedrock/06-03-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-in-amazon-bedrock/06-03.webp",
          alt: "Section 6 screenshot 3",
        },
      ],
    },
    {
      id: "section-7",
      title: "Claude Code & Computer Use",
      lessonCount: 8,
      description:
        "Two powerful Anthropic tools in action. Claude Code accelerates development workflows, Computer Use automates UI interactions. Includes MCP integration patterns.",
      screenshots: [
        {
          id: "s7-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-in-amazon-bedrock/07-01-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-in-amazon-bedrock/07-01.webp",
          alt: "Section 7 screenshot 1",
        },
        {
          id: "s7-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-in-amazon-bedrock/07-02-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-in-amazon-bedrock/07-02.webp",
          alt: "Section 7 screenshot 2",
        },
        {
          id: "s7-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-in-amazon-bedrock/07-03-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-in-amazon-bedrock/07-03.webp",
          alt: "Section 7 screenshot 3",
        },
      ],
    },
  ],
};

if (typeof window._clpdata !== "object") {
  window._clpdata = {};
}
window._clpdata[claudeWithAmazonBedrock["path"]] = claudeWithAmazonBedrock;


const claudeWithGoogleCloudsVertexAi = {
  path: "/claude-with-google-vertex",
  llmContentKey: "vertex",
  title: "Claude with Google Cloud's Vertex AI",
  subtitle:
    "This comprehensive course covers the full spectrum of working with Anthropic models using Google Cloud's Vertex AI",
  overview: {
    description:
      "This course covers using Claude models through Google Cloud's Vertex AI platform. You'll implement core API features like streaming and tool use, build systematic prompt evaluation pipelines, understand RAG systems, and investigate agent architectures.",
    learningObjectives: [
      "Set up and authenticate Claude through Vertex AI using the Anthropic SDK",
      "Select appropriate Claude models based on intelligence, speed, and cost trade-offs",
      "Write and systematically evaluate prompts using objective scoring metrics",
      "Implement tool calling for web search, file operations, and custom functionality",
      "Build RAG pipelines with text chunking, embeddings, and hybrid search",
      "Use advanced features like extended thinking, citations, and prompt caching",
      "Connect Claude to external services using MCP (Model Context Protocol)",
      "Design workflows for known task sequences and agents for flexible problem-solving",
    ],
    prerequisites: [
      "Proficiency in Python programming",
      "Basic knowledge of handling JSON data",
      "A Google Cloud account with Vertex AI access",
    ],
    targetAudience: "Devs who want to add AI features to their apps",
  },
  stats: {
    lectureCount: 85,
    videoHours: 8.0,
    quizCount: 10,
  },
  coreLearningAreas: [
    {
      id: "getting-started",
      title: "Getting started with Google Cloud's Vertex AI",
      colorAccent: "#b4c6d4",
      topics: [
        "Authentication and API key management",
        "Multi-turn conversation handling",
        "Temperature and output control",
        "System prompt configuration",
        "Request/response fundamentals",
      ],
    },
    {
      id: "advanced-implementation",
      title: "Advanced implementation techniques",
      colorAccent: "#c5bfd9",
      topics: [
        "Custom tool creation and schemas",
        "RAG pipelines and embeddings",
        "Prompt caching and optimization",
        "Model Context Protocol (MCP)",
        "Multi-modal features",
      ],
    },
    {
      id: "production-ready",
      title: "Production-ready development",
      colorAccent: "#b5c5c0",
      topics: [
        "Evaluation frameworks and testing",
        "Prompt engineering best practices",
        "Agent and workflow architecture",
        "Development with Claude Code",
        "Automation with Computer Use",
      ],
    },
  ],

  sections: [
    {
      id: "section-2",
      title: "Getting started with Claude",
      lessonCount: 16,
      description:
        "Start here for the fundamentals. Covers API authentication, basic requests, conversation management, system prompts, and structured output generation.",
      screenshots: [
        {
          id: "s2-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/02-01-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/02-01.webp",
          alt: "Section 2 screenshot 1",
        },
        {
          id: "s2-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/02-02-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/02-02.webp",
          alt: "Section 2 screenshot 2",
        },
        {
          id: "s2-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/02-03-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/02-03.webp",
          alt: "Section 2 screenshot 3",
        },
      ],
    },
    {
      id: "section-3",
      title: "Prompt engineering & evaluation",
      lessonCount: 16,
      description:
        "Learn to write prompts that actually work. Focuses on prompting strategies, evaluation frameworks, and systematic testing approaches.",
      screenshots: [
        {
          id: "s3-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/03-01-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/03-01.webp",
          alt: "Section 3 screenshot 1",
        },
        {
          id: "s3-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/03-02-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/03-02.webp",
          alt: "Section 3 screenshot 2",
        },
        {
          id: "s3-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/03-03-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/03-03.webp",
          alt: "Section 3 screenshot 3",
        },
      ],
    },
    {
      id: "section-4",
      title: "Tool use with Claude",
      lessonCount: 14,
      description:
        "Extend Claude with custom tools and functions. Build apps with function calling, multi-turn tool interactions, batch tool calling, and leverage built-in utilities.",
      screenshots: [
        {
          id: "s4-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/04-01-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/04-01.webp",
          alt: "Section 4 screenshot 1",
        },
        {
          id: "s4-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/04-02-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/04-02.webp",
          alt: "Section 4 screenshot 2",
        },
        {
          id: "s4-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/04-03-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/04-03.webp",
          alt: "Section 4 screenshot 3",
        },
      ],
    },
    {
      id: "section-5",
      title: "Retrieval augmented generation",
      lessonCount: 10,
      description:
        "Implementation guide for production RAG systems. Covers text chunking, embeddings, hybrid search with BM25, multi-index architectures, reranking, and contextual retrieval.",
      screenshots: [
        {
          id: "s5-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/05-01-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/05-01.webp",
          alt: "Section 5 screenshot 1",
        },
        {
          id: "s5-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/05-02-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/05-02.webp",
          alt: "Section 5 screenshot 2",
        },
        {
          id: "s5-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/05-03-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/05-03.webp",
          alt: "Section 5 screenshot 3",
        },
      ],
    },
    {
      id: "section-6",
      title: "Model Context Protocol (MCP)",
      lessonCount: 12,
      description:
        "The protocol for building modular AI applications. Define custom tools and resources, implement MCP servers and clients, handle the full integration lifecycle.",
      screenshots: [
        {
          id: "s6-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/06-01-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/06-01.webp",
          alt: "Section 6 screenshot 1",
        },
        {
          id: "s6-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/06-02-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/06-02.webp",
          alt: "Section 6 screenshot 2",
        },
        {
          id: "s6-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/06-03-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/06-03.webp",
          alt: "Section 6 screenshot 3",
        },
      ],
    },
    {
      id: "section-7",
      title: "Claude Code & Computer Use",
      lessonCount: 8,
      description:
        "Two powerful Anthropic tools in action. Claude Code accelerates development workflows, Computer Use automates UI interactions. Includes MCP integration patterns.",
      screenshots: [
        {
          id: "s7-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/07-01-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/07-01.webp",
          alt: "Section 7 screenshot 1",
        },
        {
          id: "s7-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/07-02-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/07-02.webp",
          alt: "Section 7 screenshot 2",
        },
        {
          id: "s7-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/07-03-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/07-03.webp",
          alt: "Section 7 screenshot 3",
        },
      ],
    },
    {
      id: "section-8",
      title: "Agents and workflows",
      lessonCount: 11,
      description:
        "Architecture patterns for autonomous AI systems. Understand parallel execution, operation chaining, conditional routing, and effective debugging strategies.",
      screenshots: [
        {
          id: "s8-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/08-01-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/08-01.webp",
          alt: "Section 8 screenshot 1",
        },
        {
          id: "s8-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/08-02-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/08-02.webp",
          alt: "Section 8 screenshot 2",
        },
        {
          id: "s8-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/08-03-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-google-vertex/08-03.webp",
          alt: "Section 8 screenshot 3",
        },
      ],
    },
  ],
};

if (typeof window._clpdata !== "object") {
  window._clpdata = {};
}
window._clpdata[claudeWithGoogleCloudsVertexAi["path"]] =
  claudeWithGoogleCloudsVertexAi;


const claudeWithTheAnthropicApiData = {
  path: "/claude-with-the-anthropic-api",
  llmContentKey: "1p",
  title: "Building with the Claude API",
  subtitle:
    "This comprehensive course covers the full spectrum of working with Anthropic models using the Claude API",
  overview: {
    description:
      "This course provides comprehensive coverage of the Claude API, from basic usage through advanced agent architectures. You'll learn to integrate Claude into applications, implement tool calling, build RAG pipelines, and design both deterministic workflows and flexible agent systems.",
    learningObjectives: [
      "Make API requests to Claude models and handle responses",
      "Implement multi-turn conversations, streaming, and structured output generation",
      "Build and evaluate prompts systematically using automated testing pipelines",
      "Create custom tools and integrate Claude with external services",
      "Design and implement RAG systems with hybrid search and reranking",
      "Use MCP (Model Context Protocol) to connect Claude to various data sources",
      "Understand common workflows and agent architectures",
    ],
    prerequisites: [
      "Proficiency in Python programming",
      "Basic knowledge of handling JSON data",
      "Access to an Anthropic API key",
    ],
    targetAudience:
      "Software engineers who need to integrate Claude into production applications. Whether you're building chatbots, automation tools, or AI-powered features, this course covers the implementation patterns you'll need.",
  },
  stats: {
    lectureCount: 84,
    videoHours: 8.1,
    quizCount: 10,
  },
  coreLearningAreas: [
    {
      id: "getting-started",
      title: "Getting started with Claude",
      colorAccent: "#b4c6d4",
      topics: [
        "Authentication and API key management",
        "Multi-turn conversation handling",
        "Temperature and output control",
        "System prompt configuration",
        "Request/response fundamentals",
      ],
    },
    {
      id: "advanced-implementation",
      title: "Advanced implementation techniques",
      colorAccent: "#c5bfd9",
      topics: [
        "Custom tool creation and schemas",
        "RAG pipelines and embeddings",
        "Prompt caching and optimization",
        "Model Context Protocol (MCP)",
        "Multi-modal features",
      ],
    },
    {
      id: "production-ready",
      title: "Production-ready development",
      colorAccent: "#b5c5c0",
      topics: [
        "Evaluation frameworks and testing",
        "Prompt engineering best practices",
        "Agent and workflow architecture",
        "Development with Claude Code",
        "Automation with Computer Use",
      ],
    },
  ],

  sections: [
    {
      id: "section-2",
      title: "Getting started with Claude",
      lessonCount: 16,
      description:
        "Start here for the fundamentals. Covers API authentication, basic requests, conversation management, system prompts, and structured output generation.",
      screenshots: [
        {
          id: "s2-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/02-01-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/02-01.webp",
          alt: "Section 2 screenshot 1",
        },
        {
          id: "s2-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/02-02-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/02-02.webp",
          alt: "Section 2 screenshot 2",
        },
        {
          id: "s2-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/02-03-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/02-03.webp",
          alt: "Section 2 screenshot 3",
        },
      ],
    },
    {
      id: "section-3",
      title: "Prompt engineering & evaluation",
      lessonCount: 16,
      description:
        "Learn to write prompts that actually work. Focuses on prompting strategies, evaluation frameworks, and systematic testing approaches.",
      screenshots: [
        {
          id: "s3-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/03-01-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/03-01.webp",
          alt: "Section 3 screenshot 1",
        },
        {
          id: "s3-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/03-02-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/03-02.webp",
          alt: "Section 3 screenshot 2",
        },
        {
          id: "s3-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/03-03-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/03-03.webp",
          alt: "Section 3 screenshot 3",
        },
      ],
    },
    {
      id: "section-4",
      title: "Tool use with Claude",
      lessonCount: 14,
      description:
        "Extend Claude with custom tools and functions. Build apps with function calling, multi-turn tool interactions, batch tool calling, and leverage built-in utilities.",
      screenshots: [
        {
          id: "s4-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/04-01-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/04-01.webp",
          alt: "Section 4 screenshot 1",
        },
        {
          id: "s4-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/04-02-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/04-02.webp",
          alt: "Section 4 screenshot 2",
        },
        {
          id: "s4-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/04-03-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/04-03.webp",
          alt: "Section 4 screenshot 3",
        },
      ],
    },
    {
      id: "section-5",
      title: "Retrieval augmented generation",
      lessonCount: 10,
      description:
        "Implementation guide for production RAG systems. Covers text chunking, embeddings, hybrid search with BM25, multi-index architectures, reranking, and contextual retrieval.",
      screenshots: [
        {
          id: "s5-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/05-01-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/05-01.webp",
          alt: "Section 5 screenshot 1",
        },
        {
          id: "s5-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/05-02-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/05-02.webp",
          alt: "Section 5 screenshot 2",
        },
        {
          id: "s5-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/05-03-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/05-03.webp",
          alt: "Section 5 screenshot 3",
        },
      ],
    },
    {
      id: "section-6",
      title: "Model Context Protocol (MCP)",
      lessonCount: 12,
      description:
        "The protocol for building modular AI applications. Define custom tools and resources, implement MCP servers and clients, handle the full integration lifecycle.",
      screenshots: [
        {
          id: "s6-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/06-01-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/06-01.webp",
          alt: "Section 6 screenshot 1",
        },
        {
          id: "s6-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/06-02-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/06-02.webp",
          alt: "Section 6 screenshot 2",
        },
        {
          id: "s6-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/06-03-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/06-03.webp",
          alt: "Section 6 screenshot 3",
        },
      ],
    },
    {
      id: "section-7",
      title: "Claude Code & Computer Use",
      lessonCount: 8,
      description:
        "Two powerful Anthropic tools in action. Claude Code accelerates development workflows, Computer Use automates UI interactions. Includes MCP integration patterns.",
      screenshots: [
        {
          id: "s7-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/07-01-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/07-01.webp",
          alt: "Section 7 screenshot 1",
        },
        {
          id: "s7-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/07-02-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/07-02.webp",
          alt: "Section 7 screenshot 2",
        },
        {
          id: "s7-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/07-03-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/07-03.webp",
          alt: "Section 7 screenshot 3",
        },
      ],
    },
    {
      id: "section-8",
      title: "Agents and workflows",
      lessonCount: 11,
      description:
        "Architecture patterns for autonomous AI systems. Understand parallel execution, operation chaining, conditional routing, and effective debugging strategies.",
      screenshots: [
        {
          id: "s8-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/08-01-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/08-01.webp",
          alt: "Section 8 screenshot 1",
        },
        {
          id: "s8-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/08-02-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/08-02.webp",
          alt: "Section 8 screenshot 2",
        },
        {
          id: "s8-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/08-03-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/claude-with-the-anthropic-api/08-03.webp",
          alt: "Section 8 screenshot 3",
        },
      ],
    },
  ],
};

if (typeof window._clpdata !== "object") {
  window._clpdata = {};
}
window._clpdata[claudeWithTheAnthropicApiData["path"]] =
  claudeWithTheAnthropicApiData;


const introductionToMcpData = {
  path: "/introduction-to-model-context-protocol",
  llmContentKey: "mcp_intro",
  title: "Introduction to Model Context Protocol",
  subtitle:
    "Learn how to build modular AI applications using MCP to connect Claude with external tools and data sources",
  overview: {
    description:
      "This course covers MCP, a protocol for connecting Claude to external services and data sources without manually writing tool schemas. You'll learn to build both MCP servers that expose tools, resources, and prompts, and MCP clients that consume them. The course includes a hands-on project where you implement a document management system using MCP.",
    learningObjectives: [
      "Understand MCP architecture and the client-server communication model",
      "Build MCP servers that expose tools using the Python SDK",
      "Implement MCP clients to connect your applications to MCP servers",
      "Create resources for exposing data and prompts for pre-defined workflows",
      "Test and debug MCP servers using the MCP Inspector",
      "Choose between tools, resources, and prompts based on control patterns",
      "Handle resource cleanup and async communication in MCP implementations",
    ],
    prerequisites: [
      "Basic Python programming experience",
      "Understanding of async/await patterns",
      "Familiarity with API concepts",
    ],
    targetAudience:
      "Engineers who want to integrate Claude with external tools and services without writing tons of boilerplate integration code",
  },
  stats: {
    lectureCount: 16,
    videoHours: 1.0,
    quizCount: 1,
  },
  coreLearningAreas: [
    {
      id: "mcp-fundamentals",
      title: "MCP architecture & concepts",
      colorAccent: "#b4c6d4",
      topics: [
        "MCP client-server architecture",
        "Transport-agnostic communication",
        "Message types and protocol flow",
        "Tools, resources, and prompts",
        "Comparison with direct API integration",
      ],
    },
    {
      id: "server-implementation",
      title: "Building MCP servers",
      colorAccent: "#c5bfd9",
      topics: [
        "Python SDK and FastMCP framework",
        "Tool definition with decorators",
        "Resource exposure patterns",
        "Prompt engineering for servers",
        "Testing with the MCP Inspector",
      ],
    },
    {
      id: "client-integration",
      title: "MCP client development",
      colorAccent: "#b5c5c0",
      topics: [
        "Client session management",
        "Tool discovery and execution",
        "Resource reading and parsing",
        "Prompt retrieval and usage",
        "Error handling and debugging",
      ],
    },
  ],

  sections: [
    {
      id: "section-1",
      title: "MCP fundamentals & server development",
      lessonCount: 8,
      description:
        "Start with understanding MCP's architecture and why it exists. Build your first MCP server with tools using the Python SDK, then test it with the built-in inspector.",
      screenshots: [
        {
          id: "s1-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/introduction-to-model-context-protocol/01-01-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/introduction-to-model-context-protocol/01-01.webp",
          alt: "MCP architecture diagram showing client-server communication",
        },
        {
          id: "s1-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/introduction-to-model-context-protocol/01-02-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/introduction-to-model-context-protocol/01-02.webp",
          alt: "Python code showing MCP server tool definitions",
        },
        {
          id: "s1-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/introduction-to-model-context-protocol/01-03-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/introduction-to-model-context-protocol/01-03.webp",
          alt: "MCP Inspector interface testing tools",
        },
      ],
    },
    {
      id: "section-2",
      title: "MCP client implementation & advanced features",
      lessonCount: 8,
      description:
        "Build the client side to communicate with MCP servers. Implement resources for direct data access and prompts for pre-built instructions. See how everything connects in a complete application flow.",
      screenshots: [
        {
          id: "s2-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/introduction-to-model-context-protocol/02-01-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/introduction-to-model-context-protocol/02-01.webp",
          alt: "Client implementation code with session management",
        },
        {
          id: "s2-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/introduction-to-model-context-protocol/02-02-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/introduction-to-model-context-protocol/02-02.webp",
          alt: "Resource and prompt implementation examples",
        },
        {
          id: "s2-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/introduction-to-model-context-protocol/02-03-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/introduction-to-model-context-protocol/02-03.webp",
          alt: "Complete application flow diagram with MCP integration",
        },
      ],
    },
  ],
};

if (typeof window._clpdata !== "object") {
  window._clpdata = {};
}
window._clpdata[introductionToMcpData["path"]] = introductionToMcpData;


const modelContextProtocolAdvancedTopicsData = {
  path: "/model-context-protocol-advanced-topics",
  llmContentKey: "mcp_advanced",
  title: "Model Context Protocol: Advanced Topics",
  subtitle:
    "Deep dive into MCP's advanced features including sampling, notifications, and transport implementations",
  overview: {
    description:
      "This course covers the technical implementation of MCP servers and clients, from basic message passing to production deployment strategies. You'll learn how MCP enables language models like Claude to interact with external tools and data sources through standardized protocols, transports, and message formats.",
    learningObjectives: [
      "Implement MCP servers with tool functions, logging, and progress notifications",
      "Handle bidirectional communication between MCP clients and servers",
      "Configure file system access using the roots permission model",
      "Work with both stdio and HTTP transports for local and remote deployments",
      "Implement sampling callbacks to enable server-initiated LLM requests",
      "Debug message flows using JSON-RPC message types",
      "Deploy scalable MCP servers using stateless HTTP configurations",
      "Troubleshoot common issues when transitioning from development to production",
    ],
    prerequisites: [
      "Basic understanding of MCP servers and clients",
      "Familiarity with async programming patterns",
    ],
    targetAudience:
      "Engineers building production MCP servers who need to understand the protocol's advanced capabilities",
  },
  stats: {
    lectureCount: 15,
    videoHours: 1.1,
    quizCount: 2,
  },
  coreLearningAreas: [
    {
      id: "advanced-features",
      title: "Advanced MCP features",
      colorAccent: "#b4c6d4",
      topics: [
        "Sampling for delegated LLM calls",
        "Real-time progress notifications",
        "Logging callbacks and debugging",
        "Roots for secure file access",
        "Resource subscription patterns",
      ],
    },
    {
      id: "transport-architecture",
      title: "Transport architecture",
      colorAccent: "#c5bfd9",
      topics: [
        "JSON message protocol specification",
        "STDIO transport for local development",
        "StreamableHTTP and SSE connections",
        "Stateless HTTP trade-offs",
        "Scaling considerations with load balancers",
      ],
    },
  ],

  sections: [
    {
      id: "section-1",
      title: "Core MCP features",
      lessonCount: 8,
      description:
        "Learn the advanced features that make MCP servers more powerful. Covers sampling to offload AI costs to clients, implementing progress notifications for better UX, and using roots to safely handle file access.",
      screenshots: [
        {
          id: "s1-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/model-context-protocol-advanced-topics/01-01-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/model-context-protocol-advanced-topics/01-01.webp",
          alt: "Section 1 screenshot 1",
        },
        {
          id: "s1-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/model-context-protocol-advanced-topics/01-02-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/model-context-protocol-advanced-topics/01-02.webp",
          alt: "Section 1 screenshot 2",
        },
        {
          id: "s1-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/model-context-protocol-advanced-topics/01-03-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/model-context-protocol-advanced-topics/01-03.webp",
          alt: "Section 1 screenshot 3",
        },
      ],
    },
    {
      id: "section-2",
      title: "Transports and communication",
      lessonCount: 7,
      description:
        "Understand how MCP messages flow between clients and servers. Explores the JSON message protocol, STDIO transport for local development, and the complexities of StreamableHTTP including when to sacrifice features for scalability.",
      screenshots: [
        {
          id: "s2-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/model-context-protocol-advanced-topics/02-01-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/model-context-protocol-advanced-topics/02-01.webp",
          alt: "Section 2 screenshot 1",
        },
        {
          id: "s2-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/model-context-protocol-advanced-topics/02-02-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/model-context-protocol-advanced-topics/02-02.webp",
          alt: "Section 2 screenshot 2",
        },
        {
          id: "s2-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/model-context-protocol-advanced-topics/02-03-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/model-context-protocol-advanced-topics/02-03.webp",
          alt: "Section 2 screenshot 3",
        },
      ],
    },
  ],
};

if (typeof window._clpdata !== "object") {
  window._clpdata = {};
}
window._clpdata[modelContextProtocolAdvancedTopicsData["path"]] =
  modelContextProtocolAdvancedTopicsData;


const teachingAiFluencyData = {
  path: "/teaching-ai-fluency",
  previewId: "78076",
  usesAlternateLayout: true,
  llmContentKey: "taif",
  title: "Teaching AI Fluency",
  subtitle:
    "This course empowers academic faculty, instructional designers, and others to teach and assess AI Fluency in instructor-led settings.",
  overview: {
    description:
      " <p>At Anthropic, we believe that empowering people with AI, and ensuring that AI makes safe contributions to society, requires engaging with a wide range of human perspectives and experiences. Responsible AI development and engagement isn't something any single discipline or viewpoint can fully address. It demands collaborative approaches that span a wide range of technical, creative, business, scientific, and educational domains. That's why  we partnered with educators who bring complementary expertise to create this course on AI collaboration for educators.</p>  <p>This course empowers faculty, instructional designers, and educational leaders to apply the 4D Framework to their own educational practice. Participants will learn to apply the 4D Framework to their own teaching practice, using AI as a thinking partner to enhance course design, create coherent learning materials, and develop authentic assessments while modeling responsible AI collaboration for their students.</p>  <p>This course is the result of a long partnership between Anthropic and professors Rick Dakan from Ringling College of Art and Design and Joseph Feller from University College Cork. It builds on their experiences in training fellow educators and teaching the AI Fluency Framework to their students, and on feedback and questions that arose from our earlier course: AI Fluency: Framework & Foundations  This course answers the question: how can I apply this framework to my personal teaching practice? </p>  <p>The work was supported in part by the Higher Education Authority (Ireland) through the National Forum for the Enhancement of Teaching and Learning.</p>  <h4>Recommended prerequisites</h4>  <p>This course lightly covers the foundational AI Fluency concepts. However, for deeper understanding, participants should complete AI Fluency: Framework & Foundations before beginning this educator-focused curriculum. </p>  <p>It is also recommended that learners have active teaching or curriculum development responsibilities as well as access to an AI chat tool for hands-on practice. Examples in this course will use Claude.ai, but any chatbot will work.</p>",
  },
  stats: {
    lectureCount: 7,
    videoHours: 0.6,
    quizCount: 1,
  },
  coreLearningAreas: [],

  sections: [
    {
      id: "section-1",
      title: "Teaching approaches",
      lessonCount: 3,
      description: "Scaffolding student AI Fluency through the 4D Framework.",
      screenshots: [
        {
          id: "s1-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/teaching-ai-fluency/01-01-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/teaching-ai-fluency/01-01.webp",
          alt: "Section 1 screenshot 1",
        },
        {
          id: "s1-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/teaching-ai-fluency/01-02-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/teaching-ai-fluency/01-02.webp",
          alt: "Section 1 screenshot 2",
        },
        {
          id: "s1-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/teaching-ai-fluency/01-03-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/teaching-ai-fluency/01-03.webp",
          alt: "Section 1 screenshot 3",
        },
      ],
    },
    {
      id: "section-2",
      title: "Assessment & Assignment",
      lessonCount: 2,
      description:
        "Designing assessments and assignments that develop and measure student AI Fluency alongside disciplinary expertise.",
      screenshots: [
        {
          id: "s2-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/teaching-ai-fluency/02-01-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/teaching-ai-fluency/02-01.webp",
          alt: "Section 2 screenshot 1",
        },
        {
          id: "s2-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/teaching-ai-fluency/02-02-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/teaching-ai-fluency/02-02.webp",
          alt: "Section 2 screenshot 2",
        },
        {
          id: "s2-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/teaching-ai-fluency/02-03-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/teaching-ai-fluency/02-03.webp",
          alt: "Section 2 screenshot 3",
        },
      ],
    },
    {
      id: "section-3",
      title: "AI's impact and your discipline",
      lessonCount: 2,
      description:
        "Building institutional capacity for responsible and meaningful AI integration in pedagogy, assessment, and curricula across disciplines.",
      screenshots: [
        {
          id: "s2-1",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/teaching-ai-fluency/03-01-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/teaching-ai-fluency/03-01.webp",
          alt: "Section 3 screenshot 1",
        },
        {
          id: "s2-2",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/teaching-ai-fluency/03-02-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/teaching-ai-fluency/03-02.webp",
          alt: "Section 3 screenshot 2",
        },
        {
          id: "s2-3",
          url: "https://d7juhi4i8fsw0.cloudfront.net/images/teaching-ai-fluency/03-03-medium.webp",
          fullSizeUrl:
            "https://d7juhi4i8fsw0.cloudfront.net/images/teaching-ai-fluency/03-03.webp",
          alt: "Section 3 screenshot 3",
        },
      ],
    },
  ],
};

if (typeof window._clpdata !== "object") {
  window._clpdata = {};
}
window._clpdata[teachingAiFluencyData["path"]] = teachingAiFluencyData;


// Modal Carousel for Course Landing Page Screenshots

function renderScreenshot(screenshot, sectionId) {
  if (screenshot.url) {
    return `
      <div class="clp__screenshot-wrapper">
        <img 
          src="${screenshot.url}" 
          alt="${screenshot.alt || "Screenshot"}" 
          class="clp__screenshot-image clp__screenshot-clickable"
          loading="lazy"
          data-section-id="${sectionId}"
          data-screenshot-id="${screenshot.id}"
          data-full-size-url="${screenshot.fullSizeUrl || screenshot.url}"
        />
      </div>
    `;
  } else {
    return `
      <div class="clp__image-placeholder">${
        screenshot.alt || "Screenshot"
      }</div>
    `;
  }
}

function generateModalCarousel() {
  return `
    <div id="clp__modal-overlay" class="clp__modal-overlay" style="display: none;">
      <div class="clp__modal-content">
        <button class="clp__modal-close" aria-label="Close modal">✕</button>
        <div class="clp__modal-header">
          <h3 class="clp__modal-title"></h3>
          <p class="clp__modal-description"></p>
          <span class="clp__preview-label">Course preview images</span>
        </div>
        <div class="clp__modal-body">
          <button class="clp__modal-nav clp__modal-prev" aria-label="Previous image"><span>‹</span></button>
          <div class="clp__modal-image-container">
            <img class="clp__modal-image" src="" alt="" />
          </div>
          <button class="clp__modal-nav clp__modal-next" aria-label="Next image"><span>›</span></button>
        </div>
        <div class="clp__modal-footer">
          <div class="clp__modal-footer-content">
            <span class="clp__modal-counter"></span>
            <div class="clp__modal-enroll">
              <p class="clp__modal-enroll-text">Ready to start learning?</p>
              <a href="" class="clp__modal-enroll-btn">Enroll in Course</a>
            </div>
          </div>
        </div>
      </div>
    </div>
  `;
}

function initializeModalCarousel(courseData, enrollHref = '') {
  // Add modal to DOM if not already present
  if (!document.getElementById("clp__modal-overlay")) {
    document.body.insertAdjacentHTML("beforeend", generateModalCarousel());
  }

  const modal = document.getElementById("clp__modal-overlay");
  const modalImage = modal.querySelector(".clp__modal-image");
  const modalTitle = modal.querySelector(".clp__modal-title");
  const modalDescription = modal.querySelector(".clp__modal-description");
  const modalCounter = modal.querySelector(".clp__modal-counter");
  const modalEnrollBtn = modal.querySelector(".clp__modal-enroll-btn");
  const closeBtn = modal.querySelector(".clp__modal-close");
  const prevBtn = modal.querySelector(".clp__modal-prev");
  const nextBtn = modal.querySelector(".clp__modal-next");

  // Set the enroll button href
  if (modalEnrollBtn && enrollHref) {
    modalEnrollBtn.href = enrollHref;
  }

  let currentSectionIndex = 0;
  let currentImageIndex = 0;
  let allSections = courseData.sections.filter(
    (section) => section.screenshots && section.screenshots.length > 0
  );

  function updateModal() {
    if (allSections.length === 0) return;

    const currentSection = allSections[currentSectionIndex];
    const currentImage = currentSection.screenshots[currentImageIndex];

    modalTitle.textContent = currentSection.title;
    modalDescription.textContent = currentSection.description;
    modalImage.src = currentImage.fullSizeUrl || currentImage.url;
    modalImage.alt = currentImage.alt || "Screenshot";

    // Update counter
    const totalImages = allSections.reduce(
      (sum, section) => sum + section.screenshots.length,
      0
    );
    let currentImageNumber = 1;
    for (let i = 0; i < currentSectionIndex; i++) {
      currentImageNumber += allSections[i].screenshots.length;
    }
    currentImageNumber += currentImageIndex;
    modalCounter.textContent = `${currentImageNumber} of ${totalImages}`;
  }

  function nextImage() {
    const currentSection = allSections[currentSectionIndex];

    if (currentImageIndex < currentSection.screenshots.length - 1) {
      currentImageIndex++;
    } else {
      // Move to next section
      if (currentSectionIndex < allSections.length - 1) {
        currentSectionIndex++;
        currentImageIndex = 0;
      } else {
        // Wrap to first section
        currentSectionIndex = 0;
        currentImageIndex = 0;
      }
    }
    updateModal();
  }

  function prevImage() {
    if (currentImageIndex > 0) {
      currentImageIndex--;
    } else {
      // Move to previous section
      if (currentSectionIndex > 0) {
        currentSectionIndex--;
        currentImageIndex =
          allSections[currentSectionIndex].screenshots.length - 1;
      } else {
        // Wrap to last section
        currentSectionIndex = allSections.length - 1;
        currentImageIndex =
          allSections[currentSectionIndex].screenshots.length - 1;
      }
    }
    updateModal();
  }

  function openModal(sectionId, screenshotId) {
    // Find the section and image
    const sectionIndex = allSections.findIndex(
      (section) => section.id === sectionId
    );
    if (sectionIndex === -1) return;

    const section = allSections[sectionIndex];
    const imageIndex = section.screenshots.findIndex(
      (screenshot) => screenshot.id === screenshotId
    );
    if (imageIndex === -1) return;

    currentSectionIndex = sectionIndex;
    currentImageIndex = imageIndex;

    updateModal();
    modal.style.display = "flex";
    document.body.style.overflow = "hidden";
  }

  function closeModal() {
    modal.style.display = "none";
    document.body.style.overflow = "";
  }

  // Event listeners
  closeBtn.addEventListener("click", closeModal);
  nextBtn.addEventListener("click", nextImage);
  prevBtn.addEventListener("click", prevImage);

  // Click outside modal to close
  modal.addEventListener("click", (e) => {
    if (e.target === modal) {
      closeModal();
    }
  });

  // Keyboard navigation
  document.addEventListener("keydown", (e) => {
    if (modal.style.display === "flex") {
      switch (e.key) {
        case "Escape":
          closeModal();
          break;
        case "ArrowLeft":
          prevImage();
          break;
        case "ArrowRight":
          nextImage();
          break;
      }
    }
  });

  // Add click handlers to screenshot images
  document.addEventListener("click", (e) => {
    if (e.target.classList.contains("clp__screenshot-clickable")) {
      const sectionId = e.target.dataset.sectionId;
      const screenshotId = e.target.dataset.screenshotId;
      openModal(sectionId, screenshotId);
    }
  });
}

// Add to window for browser usage
if (typeof window !== "undefined") {
  window.renderScreenshot = renderScreenshot;
  window.generateModalCarousel = generateModalCarousel;
  window.initializeModalCarousel = initializeModalCarousel;
}


// Course Landing Page HTML Generator

function generateCourseLandingPage(courseData, videoElement, enrollHref) {
  const html = `
    <div class="clp__container">
      ${generateHeroSection(courseData, videoElement, enrollHref)}
      ${generateMainContent(courseData)}
    </div>
  `;
  return html.trim();
}

function generateHeroSection(courseData, videoElement, enrollHref) {
  // Determine what to show for video
  let videoContent;
  if (videoElement) {
    // If we have an existing video element, use it directly
    videoContent = `<div class="clp__video-wrapper">${videoElement.outerHTML}</div>`;
  } else {
    // Otherwise, show a fallback placeholder
    videoContent = `
      <div class="clp__video-container">
        <div class="clp__video-title">Course Overview</div>
        <div class="clp__video-placeholder">
          <div class="clp__play-button"></div>
        </div>
      </div>
    `;
  }

  return `
    <section class="clp__hero">
      <div class="clp__hero-content">
        <div class="clp__hero-text">
          <div class="clp__breadcrumb">
            <a href="https://www.anthropic.com/learn" class="clp__breadcrumb-link">Anthropic Academy</a>
            <span class="clp__breadcrumb-separator"> / </span>
            <a href="https://anthropic.skilljar.com/" class="clp__breadcrumb-link">Courses</a>
          </div>
          <h1 class="clp__title">${courseData.title}</h1>
          <p class="clp__subtitle">${courseData.subtitle}</p>
          
          <div class="clp__hero-actions">
            <a href="${
              enrollHref || ""
            }" class="clp__enroll-btn">Enroll in Course</a>
            <span class="clp__free-tag">FREE</span>
          </div>
          
          <div class="clp__signin-text">
            Already registered? <a href="/auth/login?next=${encodeURIComponent(
              courseData.path
            )}" class="clp__signin-link">Sign In</a>
          </div>
          
          <div class="clp__share-buttons-inline">
            <a href="#" class="clp__share-btn-inline" onclick="shareOnX(event)">
              <svg class="clp__share-icon-inline" viewBox="0 0 24 24" fill="currentColor">
                <path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/>
              </svg>
              <span>Share on X</span>
            </a>
            <a href="#" class="clp__share-btn-inline" onclick="shareOnLinkedIn(event)">
              <svg class="clp__share-icon-inline" viewBox="0 0 24 24" fill="currentColor">
                <path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/>
              </svg>
              <span>Share on LinkedIn</span>
            </a>
          </div>
        </div>
        
        <div class="clp__hero-right">
          ${videoContent}
          ${courseData.stats ? generateStatsSection(courseData.stats) : ""}
        </div>
      </div>
    </section>
  `;
}

function generateStatsSection(stats) {
  return `
    <div class="clp__stats-container">
      <div class="clp__stat-item">
        <span class="clp__stat-value">${stats.lectureCount}</span>
        <span class="clp__stat-label">lectures</span>
      </div>
      <div class="clp__stat-item">
        <span class="clp__stat-value">${stats.videoHours}</span>
        <span class="clp__stat-label">${
          stats.videoHours === 1 ? "hour" : "hours"
        } of video</span>
      </div>
      ${
        stats.quizCount > 0
          ? `
      <div class="clp__stat-item">
        <span class="clp__stat-value">${stats.quizCount}</span>
        <span class="clp__stat-label">${
          stats.quizCount > 1 ? "quizzes" : "quiz"
        }</span>
      </div>
      `
          : ""
      }
      <div class="clp__stat-item">
        <span class="clp__stat-value">✓</span>
        <span class="clp__stat-label">Certificate of completion</span>
      </div>
    </div>
  `;
}

function generateMainContent(courseData) {
  return `
    <div class="clp__main-content">
    ${generateAboutThisCourse(courseData)}
    ${generateCourseSections(courseData.sections)}
    ${courseData.instructors ? generateInstructorsSection(courseData.instructors) : ''}
    ${courseData.extra ? generateExtraSection(courseData.extra) : ''}
    </div>
  `;
}

function generateCourseSections(sections) {
  return `
    <section class="clp__sections-container">
      <h2 class="clp__section-title">Course sections</h2>
      
      ${sections
        .map(
          (section) => `
        <div class="clp__section-block">
          <div class="clp__section-header">
            <div class="clp__section-info">
              <h3 class="clp__section-name">${section.title}</h3>
              <span class="clp__lesson-count">${
                section.lessonCount
              } lessons</span>
            </div>
          </div>
          
          <p class="clp__section-description">${section.description}</p>
          
          ${
            section.screenshots && section.screenshots.length > 0
              ? `
            <span class="clp__preview-label">Course preview images</span>
            <div class="clp__image-placeholders">
              ${section.screenshots
                .map((screenshot) => renderScreenshot(screenshot, section.id))
                .join("")}
            </div>
          `
              : ""
          }
        </div>
      `
        )
        .join("")}
    </section>
  `;
}

function generateInstructorsSection(instructors) {
  return `
    <section class="clp__instructors-container">
      <h2 class="clp__instructors-title">About the instructors</h2>
      
      <div class="clp__instructors-card">
        <div class="clp__instructors-grid">
          ${instructors
            .map(
              (instructor) => `
            <div class="clp__instructor-item">
              <img src="${instructor.avatar}" alt="${instructor.name}" class="clp__instructor-avatar" />
              <div class="clp__instructor-info">
                <h3 class="clp__instructor-name">${instructor.name}</h3>
                <p class="clp__instructor-description">${instructor.description}</p>
              </div>
            </div>
          `
            )
            .join("")}
        </div>
      </div>
    </section>
  `;
}

function generateExtraSection(extra) {
  return `
    <section class="clp__extra-container">
      <h2 class="clp__extra-title">${extra.title}</h2>
      
      <div class="clp__extra-card">
        <div class="clp__extra-content">${extra.content}</div>
      </div>
    </section>
  `;
}

function generateAboutThisCourse(courseData) {
  return `
    <div>
      <div class="clp__course-details">
        <h2 class="clp__details-title">About this course</h2>
        
        <p class="clp__details-text">${courseData.overview.description}</p>
        
        ${courseData.overview.learningObjectives ? `
          <h3 class="clp__details-subtitle">Learning objectives</h3>
          <p class="clp__details-text">By the end of this course, you'll be able to:</p>
          <ul class="clp__details-list">
            ${courseData.overview.learningObjectives
              .map(
                (objective) => `
              <li>${objective}</li>
            `
              )
              .join("")}
          </ul>
        ` : ''}
        
        ${courseData.overview.prerequisites ? `
          <h3 class="clp__details-subtitle">Prerequisites</h3>
          <ul class="clp__details-list">
            ${courseData.overview.prerequisites
              .map(
                (prereq) => `
              <li>${prereq}</li>
            `
              )
              .join("")}
          </ul>
        ` : ''}
        
        ${courseData.overview.targetAudience ? `
          <h3 class="clp__details-subtitle">Who this course is for</h3>
          <p class="clp__details-text">${courseData.overview.targetAudience}</p>
        ` : ''}
      </div>
      
      
    </div>
  `;
}

// Share functions
function shareOnX(e) {
  e.preventDefault();
  const text =
    document.querySelector(".clp__title").textContent +
    " - " +
    document.querySelector(".clp__subtitle").textContent;
  const url = window.location.href;
  window.open(
    `https://twitter.com/intent/tweet?text=${encodeURIComponent(
      text
    )}&url=${encodeURIComponent(url)}`,
    "_blank"
  );
}

function shareOnLinkedIn(e) {
  e.preventDefault();
  const url = window.location.href;
  window.open(
    `https://www.linkedin.com/sharing/share-offsite/?url=${encodeURIComponent(
      url
    )}`,
    "_blank"
  );
}

// Export the main function
if (typeof module !== "undefined" && module.exports) {
  module.exports = { generateCourseLandingPage };
}

// Add to window for browser usage
if (typeof window !== "undefined") {
  window.generateCourseLandingPage = generateCourseLandingPage;
  window.shareOnX = shareOnX;
  window.shareOnLinkedIn = shareOnLinkedIn;
}


// Find the course data matching the current path
function getMatchingCourse() {
  const currentPath = window.location.pathname;
  const courses = Object.values(window._clpdata || {});

  return courses.find((course) => currentPath.includes(course.path));
}

// The AI Fluency series of courses hide the .lesson-top header
// on the lesson page, which is where the Chat with Claude
// button is usually located. If this is a AIF course then
// we will instead prepend the chatbutton to the course text content
function useAlternateLayout() {
  const course = getMatchingCourse();
  return course?.usesAlternateLayout || false;
}

function renderChatButton() {
  let chatButtonParent;
  if (useAlternateLayout()) {
    chatButtonParent = document.querySelector(".course-text-content");
  } else {
    chatButtonParent = document.querySelector(".lesson-top");
  }

  if (!chatButtonParent) {
    return null;
  }

  // Check if chat button already exists
  if (chatButtonParent.querySelector(".lp__chat-combo")) {
    console.warn("Chat button already exists, skipping");
    return null;
  }

  // Find matching course and get LLM content using its key
  const course = getMatchingCourse();
  const llmKey = course?.llmContentKey;
  const attachmentContent =
    llmKey && window.__chatData ? window.__chatData[llmKey] : "";

  // Return early if no attachment content found
  if (!attachmentContent) {
    return null;
  }

  // Create combo button container
  const comboContainer = document.createElement("div");
  comboContainer.className = "lp__chat-combo";

  // Create main button
  const mainButton = document.createElement("button");
  mainButton.className = "lp__chat-button-main";
  mainButton.innerHTML = `
    <div class="lp__chat-button-content">
      <svg fill="currentColor" fill-rule="evenodd" height="1em" viewBox="0 0 24 24" width="1em" xmlns="http://www.w3.org/2000/svg" class="lp__chat-icon">
        <title>Anthropic</title>
        <path d="M13.827 3.52h3.603L24 20h-3.603l-6.57-16.48zm-7.258 0h3.767L16.906 20h-3.674l-1.343-3.461H5.017l-1.344 3.46H0L6.57 3.522zm4.132 9.959L8.453 7.687 6.205 13.48H10.7z"></path>
      </svg>
      <span>Open in Claude</span>
    </div>
  `;

  // Create dropdown button
  const dropdownButton = document.createElement("button");
  dropdownButton.className = "lp__chat-button-dropdown";
  dropdownButton.innerHTML = `
    <svg width="8" height="24" viewBox="0 -9 3 24" class="lp__dropdown-arrow">
      <path d="M0 0L3 3L0 6" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round"></path>
    </svg>
  `;

  // Create dropdown menu
  const dropdownMenu = document.createElement("div");
  dropdownMenu.className = "lp__chat-dropdown-menu";
  dropdownMenu.innerHTML = `
    <div class="lp__chat-dropdown-item" data-action="claude">
      <div class="lp__chat-icon-container">
        <svg fill="currentColor" fill-rule="evenodd" height="1em" viewBox="0 0 24 24" width="1em" xmlns="http://www.w3.org/2000/svg" class="lp__chat-icon">
          <title>Anthropic</title>
          <path d="M13.827 3.52h3.603L24 20h-3.603l-6.57-16.48zm-7.258 0h3.767L16.906 20h-3.674l-1.343-3.461H5.017l-1.344 3.46H0L6.57 3.522zm4.132 9.959L8.453 7.687 6.205 13.48H10.7z"></path>
        </svg>
      </div>
      <div class="lp__chat-text">
        <div class="lp__chat-title">
          Open in Claude
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lp__external-link">
            <path d="M7 7h10v10"></path>
            <path d="M7 17 17 7"></path>
          </svg>
        </div>
        <div class="lp__chat-subtitle">Ask questions about this course</div>
      </div>
    </div>
    <div class="lp__chat-dropdown-item" data-action="copy">
      <div class="lp__chat-icon-container">
        <svg width="18" height="18" viewBox="0 0 18 18" fill="none" xmlns="http://www.w3.org/2000/svg" class="lp__chat-icon">
          <path d="M14.25 5.25H7.25C6.14543 5.25 5.25 6.14543 5.25 7.25V14.25C5.25 15.3546 6.14543 16.25 7.25 16.25H14.25C15.3546 16.25 16.25 15.3546 16.25 14.25V7.25C16.25 6.14543 15.3546 5.25 14.25 5.25Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path>
          <path d="M2.80103 11.998L1.77203 5.07397C1.61003 3.98097 2.36403 2.96397 3.45603 2.80197L10.38 1.77297C11.313 1.63397 12.19 2.16297 12.528 3.00097" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path>
        </svg>
      </div>
      <div class="lp__chat-text">
        <div class="lp__chat-title">Copy notes</div>
        <div class="lp__chat-subtitle">Copy full course notes for LLMs</div>
      </div>
    </div>
  `;

  // Assemble combo button
  comboContainer.appendChild(mainButton);
  comboContainer.appendChild(dropdownButton);
  comboContainer.appendChild(dropdownMenu);

  // Add event listeners
  mainButton.onclick = () => openInClaude();

  dropdownButton.onclick = (e) => {
    e.stopPropagation();
    dropdownMenu.classList.toggle("lp__chat-dropdown-open");
    dropdownButton.classList.toggle("lp__dropdown-open");
  };

  // Add dropdown item listeners
  dropdownMenu.addEventListener("click", (e) => {
    const item = e.target.closest(".lp__chat-dropdown-item");
    if (!item) return;

    const action = item.dataset.action;
    if (action === "copy") {
      copyTranscript();
    } else if (action === "claude") {
      openInClaude();
    }

    dropdownMenu.classList.remove("lp__chat-dropdown-open");
    dropdownButton.classList.remove("lp__dropdown-open");
  });

  // Close dropdown when clicking outside
  document.addEventListener("click", (e) => {
    if (!comboContainer.contains(e.target)) {
      dropdownMenu.classList.remove("lp__chat-dropdown-open");
      dropdownButton.classList.remove("lp__dropdown-open");
    }
  });

  // Action functions
  function copyTranscript() {
    const mainButtonText = mainButton.querySelector("span");
    const dropdownItemText = dropdownMenu.querySelector(
      '[data-action="copy"] .lp__chat-title'
    );

    navigator.clipboard
      .writeText(attachmentContent)
      .then(() => {
        // Change text to "Copied!"
        mainButtonText.textContent = "Copied!";
        dropdownItemText.textContent = "Copied!";

        // Revert after 3 seconds
        setTimeout(() => {
          mainButtonText.textContent = "Open in Claude";
          dropdownItemText.textContent = "Copy notes";
        }, 3000);
      })
      .catch((err) => {
        console.error("Failed to copy transcript:", err);
      });
  }

  function openInClaude() {
    const q = encodeURIComponent("Explain this concept:");
    const attachment = encodeURIComponent(attachmentContent);
    const linkUrl = `https://claude.ai/remix#q=${q}&attachment=${attachment}`;
    window.open(linkUrl, "_blank", "noopener,noreferrer");
  }

  if (useAlternateLayout()) {
    chatButtonParent.prepend(comboContainer);
  } else {
    chatButtonParent.appendChild(comboContainer);
  }

  return comboContainer;
}


const VIOLET = "#cbcadb";
const MINT = "#bcd1ca";

class ResourceLinks {
  images = {
    quill: {
      imgAlt:
        "Ornate quill pen resting on a detailed hand, positioned against a textured background",
      imgSrc:
        "https://www-cdn.anthropic.com/images/4zrzovbb/website/33dbe8f783d4835a838b4c4ae85d3c04e352fee1-1000x1000.svg",
    },
    head: {
      imgAlt:
        "Stylized hand and head silhouette with interconnected node and abstract geometric elements",
      imgSrc:
        "https://www-cdn.anthropic.com/images/4zrzovbb/website/46e4aa7ea208ed440d5bd9e9e3a0ee66bc336ff1-1000x1000.svg",
    },
  };

  resources = [
    // AI Fluency
    {
      title: "AI Fluency vocabulary guide",
      description:
        "A reference for key terms you'll encounter, written in plain language. No need to memorize; just refer to it when needed.",
      img: "quill",
      attachmentTitle: "AI Fluency vocabulary cheat sheet.pdf",
      color: VIOLET,
    },
    {
      title: "The AI Fluency Framework summary (8.5x11)",
      description:
        "Quick visual reference for the four core competencies (the “4Ds”) and key concepts. Can be printed as a reference poster.",
      img: "quill",
      attachmentTitle: "1.2 AI Fluency Summary One-Pager.pdf",
      color: VIOLET,
    },
    {
      title: "The AI Fluency Framework summary (16x9)",
      description:
        "Quick visual reference for the four core competencies (the “4Ds”) and key concepts. Can be used in delivering presentations.",
      img: "quill",
      attachmentTitle: "1.2 AI Fluency Summary 16x9.pdf",
      color: MINT,
    },
    {
      title: "Overview of Generative AI",
      description: "Quick reference guide for understanding generative AI.",
      img: "quill",
      attachmentTitle: "DD1 Handout_ Overview of Generative AI.pdf",
      color: VIOLET,
    },
    {
      title: "Delegation summary slide (8.5x11)",
      description:
        "A concise summary of the Delegation competency that you can print as a quick reference.",
      img: "quill",
      attachmentTitle: "1.3 Delegation Summary.pdf",
      color: VIOLET,
    },
    {
      title: "Delegation summary slide (16x9)",
      description:
        "A concise summary of the Delegation competency that you can use in delivering presentations.",
      img: "quill",
      attachmentTitle: "1.3 Delegation Summary 16x9.pdf",
      color: MINT,
    },
    {
      title: "Description summary slide (8.5x11)",
      description:
        "A concise summary of the Delegation competency that you can print as a quick reference.",
      img: "quill",
      attachmentTitle: "1.5 Description Summary.pdf",
      color: VIOLET,
    },
    {
      title: "Description summary slide (16x9)",
      description:
        "A concise summary of the Delegation competency that you can use in presentations.",
      img: "quill",
      attachmentTitle: "1.5 Description Summary 16x9.pdf",
      color: MINT,
    },
    {
      title: "6 techniques for effective prompt engineering",
      description:
        "See how vague prompts can be transformed into effective ones with these real-world examples.",
      img: "quill",
      attachmentTitle: "DD2 Handout_ 6 Effective Prompting Techniques.pdf",
      color: VIOLET,
    },
    {
      title: "Discernment summary slide (8.5x11)",
      description:
        "A concise summary of the Discernment competency that you can print as a quick reference poster.",
      img: "quill",
      attachmentTitle: "1.6 Discernment Summary.pdf",
      color: VIOLET,
    },
    {
      title: "Discernment summary slide (16x9)",
      description:
        "A concise summary of the Discernment competency that you can use in presentations.",
      img: "quill",
      attachmentTitle: "1.6 Discernment Summary 16x9.pdf",
      color: MINT,
    },
    {
      title: "Diligence summary slide (8.5x11)",
      description:
        "A concise summary of the Diligence competency that you can print as a quick reference.",
      img: "quill",
      attachmentTitle: "1.8 Diligence Summary.pdf",
      color: VIOLET,
    },
    {
      title: "Diligence summary slide (16x9)",
      description:
        "A concise summary of the Diligence competency that you can use in presentations.",
      img: "quill",
      attachmentTitle: "1.8 Diligence Summary 16x9.pdf",
      color: MINT,
    },
  ];

  init() {
    const anchors = document.querySelectorAll(
      "#details-pane-summary-content .sj-text-downloads + ul a"
    );
    const resourceCards = [];

    anchors.forEach((anchor) => {
      const span = anchor.querySelector("span");
      if (!span) return;

      const spanText = span.textContent.trim();
      const matchedResource = this.resources.find(
        (resource) => resource.attachmentTitle === spanText
      );

      if (matchedResource) {
        const href = anchor.href;
        const img = this.images[matchedResource.img];

        const resourceHtml = this.createResource({
          href,
          description: matchedResource.description,
          title: matchedResource.title,
          imgAlt: img.imgAlt,
          imgSrc: img.imgSrc,
          bgColor: matchedResource.color,
        });

        resourceCards.push(resourceHtml);
      }
    });

    if (resourceCards.length > 0) {
      // Remove the original ul element
      const ul = document.querySelector(
        "#details-pane-summary-content .sj-text-downloads + ul"
      );
      if (ul) {
        ul.remove();
      }

      // Create new container and add resource cards
      const downloadsHeader = document.querySelector(".sj-text-downloads");
      if (downloadsHeader) {
        const resourceContainer = document.createElement("div");
        resourceContainer.className = "resource__container";
        resourceContainer.innerHTML = resourceCards.join("");

        // Insert as sibling after the downloads header
        downloadsHeader.parentNode.insertBefore(
          resourceContainer,
          downloadsHeader.nextSibling
        );
      }
    }
  }

  createResource({ href, description, title, imgAlt, imgSrc, bgColor }) {
    return `
<div class="resource__wrapper" style="background-color: ${bgColor};">
  <div class="resource__image">
    <img alt="${imgAlt}" loading="lazy" width="1000" height="1000" decoding="async" data-nimg="1" src="${imgSrc}" style="color: transparent;">
  </div>
  <div class="resource__content">
    <div class="resource__text_wrapper">
      <h3 class="resource__header">${title}</h3>
      <p class="resource__text">${description}</p>
    </div>
    <a class="resource__link" href="${href}" rel="noopener" target="_blank" aria-label="Download">
      <span>Download</span>
    </a>
  </div>
</div>
`;
  }
}


renderChatButton();

const resourceLinks = new ResourceLinks();
resourceLinks.init();

// Check if current path matches a course with usesAlternateLayout flag
function shouldUseAlternateLayout() {
  const currentPath = window.location.pathname;
  const courses = Object.values(window._clpdata || {});

  return courses.some((course) => {
    if (!course.usesAlternateLayout) return false;
    const matchesPath = currentPath.includes(course.path);
    const matchesPreview =
      course.previewId && currentPath.includes(`preview/${course.previewId}`);
    return matchesPath || matchesPreview;
  });
}

// Apply specific CSS based on course layout type
if (shouldUseAlternateLayout()) {
  const style = document.createElement("style");
  style.textContent = `
    #lp-wrapper #lp-content #details-pane.columns #details-pane-inner #details-pane-content #details-pane-summary-content {
      flex-direction: column !important;
    }
  `;
  document.head.appendChild(style);
} else {
  const style = document.createElement("style");
  style.textContent = `
    #lesson-body #lesson-main #lesson-main-inner .lesson-top {
      display: flex !important;
    }
  `;
  document.head.appendChild(style);
}

const song = document.querySelector("#song");
if (song) {
  song.addEventListener("click", () => {
    const songWrapper = document.querySelector("#song-wrapper");

    if (songWrapper) {
      const existingAudio = songWrapper.querySelector("audio");
      if (existingAudio) {
        existingAudio.play();
        return;
      }

      const audio = document.createElement("audio");
      audio.controls = true;
      audio.autoplay = false;
      audio.id = "karaoke-audio";
      audio.src = "https://ant-lm-sj-s3.s3.us-east-1.amazonaws.com/script.js";
      audio.style.marginTop = "16px";
      audio.style.width = "100%";
      audio.style.maxWidth = "400px";

      songWrapper.appendChild(audio);

      const lines = [
        {
          text: "Welcome to our alpha test, my friend.",
          start: 6738,
          end: 11257,
        },
        {
          text: "It's long, but you'll make it till the end.",
          start: 11257,
          end: 15559,
        },
        {
          text: "We need your feedback, oh, it's so divine.",
          start: 15559,
          end: 20498,
        },
        {
          text: "To make this certification really shine.",
          start: 20498,
          end: 27357,
        },
        {
          text: "Don't use Claude, don't use Google.",
          start: 27357,
          end: 30753,
        },
        {
          text: "Don't even ask your precious Poodle.",
          start: 30753,
          end: 33960,
        },
        {
          text: "We're watching you, but not really, though.",
          start: 33960,
          end: 36653,
        },
        {
          text: "Just pretend that we're in the know.",
          start: 36653,
          end: 38357,
        },
        { text: "No external help, it's the rule.", start: 38357, end: 40360 },
        {
          text: "The pacing, the difficulty, the clarity, too.",
          start: 40360,
          end: 43648,
        },
        {
          text: "We need to know if this exam's killing you.",
          start: 43648,
          end: 47057,
        },
        {
          text: "Take it by yourself, serious and stern.",
          start: 47057,
          end: 50359,
        },
        { text: "Give it a try, help us learn.", start: 50359, end: 53660 },
        {
          text: "Your manager won't see your score.",
          start: 53660,
          end: 56758,
        },
        {
          text: "We promise this, we swear what's more.",
          start: 56758,
          end: 59951,
        },
        { text: "Your results are anonymized.", start: 59951, end: 63348 },
        {
          text: "So don't feel stressed or victimized.",
          start: 63348,
          end: 66649,
        },
        {
          text: "There's no cheating now, be strong and true.",
          start: 66649,
          end: 70058,
        },
        {
          text: "Leave feedback when you're finally through.",
          start: 70058,
          end: 73360,
        },
        { text: "This alpha test is just for fun.", start: 73360, end: 78555 },
        { text: "And data collection, everyone.", start: 78555, end: 81749 },
        {
          text: "Remember, don't ask your dog for help.",
          start: 81749,
          end: 86660,
        },
      ];

      let lyricsDiv = null;
      let currentLineIndex = -1;
      let isInitialized = false;

      function initializeLyrics() {
        if (isInitialized) return;

        lyricsDiv = document.createElement("div");
        lyricsDiv.id = "karaoke-lyrics";
        document.body.appendChild(lyricsDiv);
        isInitialized = true;
      }

      function updateDisplay() {
        if (!lyricsDiv) return;

        const currentTime = audio.currentTime * 1000;

        // Find current line
        let newLineIndex = -1;
        for (let i = 0; i < lines.length; i++) {
          if (currentTime >= lines[i].start && currentTime <= lines[i].end) {
            newLineIndex = i;
            break;
          }
        }

        // Update display if line changed
        if (newLineIndex !== currentLineIndex) {
          currentLineIndex = newLineIndex;

          lyricsDiv.innerHTML = "";

          // Show 3 lines: previous, current, next
          const linesToShow = [
            currentLineIndex - 1,
            currentLineIndex,
            currentLineIndex + 1,
          ];

          linesToShow.forEach((idx) => {
            if (idx >= 0 && idx < lines.length) {
              const lineDiv = document.createElement("div");
              lineDiv.textContent = lines[idx].text;
              lineDiv.dataset.index = idx;

              if (idx === currentLineIndex) {
                lineDiv.className = "karaoke-line karaoke-current";
              } else if (idx < currentLineIndex) {
                lineDiv.className = "karaoke-line karaoke-past";
              } else {
                lineDiv.className = "karaoke-line karaoke-upcoming";
              }

              lyricsDiv.appendChild(lineDiv);
            }
          });
        }
      }

      audio.addEventListener("timeupdate", updateDisplay);

      // Hide lyrics when paused, show when playing
      audio.addEventListener("pause", () => {
        if (lyricsDiv) {
          lyricsDiv.classList.add("karaoke-hidden");
        }
      });

      audio.addEventListener("play", () => {
        // Initialize lyrics div on first play
        if (!isInitialized) {
          initializeLyrics();
        }

        if (lyricsDiv) {
          lyricsDiv.classList.remove("karaoke-hidden");
        }
      });
    }
  });
}


const coursePaths = Object.keys(window._clpdata || {});

function revealDefaultContent() {
  const el = document.querySelector("#skilljar-content");
  if (el) {
    el.classList.add("reveal");
  }
}

function applyEnhancedStyling() {
  const currentPath = window.location.pathname;

  const isExactMatch = coursePaths.some((title) => currentPath === title);

  if (!isExactMatch) {
    revealDefaultContent();
    return;
  }

  // True if we're on any course landing page, registered or not
  if (window.skilljarCourse) {
    // True if we have already registered for the course
    if (window.skilljarCourseProgress) {
      revealDefaultContent();
    } else {
      // Apply course landing page template
      applyCourseLandingPageTemplate(currentPath);
    }
  } else {
    revealDefaultContent();
  }
}

function applyCourseLandingPageTemplate(path) {
  // Get the course data for the current path
  const courseData = window._clpdata && window._clpdata[path];

  if (!courseData) {
    console.error("No course data found for path:", path);
    revealDefaultContent();
    return;
  }

  // Find the main content container
  const mainContent = document.querySelector("#skilljar-content");

  if (mainContent) {
    // Extract the existing video player before replacing content
    const existingVideoWrapper = mainContent.querySelector(
      ".dp-promo-image-wrapper"
    );
    let videoElement = null;

    if (existingVideoWrapper) {
      // Clone the video element to preserve event listeners and state
      videoElement = existingVideoWrapper.cloneNode(true);
    }

    // Extract the enroll button href
    const enrollButton = document.querySelector("#purchase-button");
    let enrollHref = "";

    if (enrollButton && enrollButton.href) {
      enrollHref = enrollButton.href;
    }

    // Generate the HTML with the video element and enroll href
    const html = window.generateCourseLandingPage(
      courseData,
      videoElement,
      enrollHref
    );

    // Replace the content
    mainContent.innerHTML = html;

    initializeModalCarousel(courseData, enrollHref);

    // Reveal the content
    const el = document.querySelector("#skilljar-content");
    if (el) {
      el.classList.add("reveal");
    }
  } else {
    console.error("Could not find main content container");
    revealDefaultContent();
  }
}

applyEnhancedStyling();


// Inject this code into the Skilljar iframe page

// v Don't touch this comment
// ^ DONT TOUCH THAT COMMENT

// STYLE ELEMENT OBSERVER
// Function to set up MutationObserver for style elements in head
function setupStyleObserver() {
  // Remove any existing PROD_STYLES elements
  document.head.querySelectorAll("style").forEach(function (style) {
    if (
      style.textContent.includes("PROD_STYLES") &&
      style.id !== "parent-injected-styles"
    ) {
      style.remove();
    }
  });

  const observer = new MutationObserver(function (mutations) {
    mutations.forEach(function (mutation) {
      mutation.addedNodes.forEach(function (node) {
        if (
          node.nodeName === "STYLE" &&
          node.textContent.includes("PROD_STYLES") &&
          node.id !== "parent-injected-styles"
        ) {
          node.remove();
        }
      });
    });
  });

  // Start observing the head element for style additions
  observer.observe(document.head, {
    childList: true,
    subtree: false,
  });
}

// SKILLJAR REDIRECT AFTER LOGOUT
// FIXME: get skilljar to override their server side redirect settings

// 1. Hide the page immediately if we expect a redirect
(function () {
  // Check if we're coming from a logout on a lesson page
  const redirectUrl = sessionStorage.getItem("redirect_after_logout");

  if (redirectUrl) {
    // Add a style to hide the page content immediately
    const style = document.createElement("style");
    style.textContent = `
          body { 
              visibility: hidden !important; 
              opacity: 0 !important;
          }
      `;
    document.head.appendChild(style);

    // Check if user is logged out and redirect
    setTimeout(function () {
      if (typeof skilljarUser === "undefined") {
        sessionStorage.removeItem("redirect_after_logout");
        window.location.replace(redirectUrl);
      } else {
        // User is still logged in, show the page
        document.body.style.visibility = "visible";
        document.body.style.opacity = "1";
        sessionStorage.removeItem("redirect_after_logout");
      }
    }, 50); // Very short delay to check login status
  }
})();

// 2. Intercept logout clicks on lesson pages
document.addEventListener("click", function (event) {
  // Current URL pattern for lesson pages
  const currentUrl = window.location.href;
  const lessonPattern =
    /^(https:\/\/anthropic-poc\.skilljar\.com\/[^\/]+)\/\d+$/;
  const isLessonPage = lessonPattern.test(currentUrl);

  if (!isLessonPage) return;

  // Check for logout button
  const logoutButton = event.target.closest(
    'a[href*="/logout"], a[href*="/sign-out"], .logout-button, .sign-out'
  );

  if (logoutButton) {
    event.preventDefault();

    // Extract course URL
    const match = currentUrl.match(lessonPattern);
    if (match) {
      const courseUrl = match[1];
      sessionStorage.setItem("redirect_after_logout", courseUrl);

      // Add loading state to current page
      document.body.style.opacity = "0.5";
      document.body.insertAdjacentHTML(
        "beforeend",
        `
              <div style="position: fixed; top: 50%; left: 50%; transform: translate(-50%, -50%); 
                          background: white; padding: 20px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); 
                          border-radius: 8px; z-index: 9999;">
                  Signing out...
              </div>
          `
      );
    }

    // Proceed with logout
    window.location.href = logoutButton.href;
  }
});

// VIDEO PLAYBACK RATE

// Video playback rate persistence script
(function () {
  "use strict";

  const STORAGE_KEY = "video-playback-rate";

  // Function to save playback rate to session storage
  function savePlaybackRate(rate) {
    try {
      sessionStorage.setItem(STORAGE_KEY, rate.toString());
    } catch (e) {
      console.error("Failed to save playback rate:", e);
    }
  }

  // Function to get saved playback rate from session storage
  function getSavedPlaybackRate() {
    try {
      const saved = sessionStorage.getItem(STORAGE_KEY);
      return saved ? parseFloat(saved) : null;
    } catch (e) {
      console.error("Failed to retrieve playback rate:", e);
      return null;
    }
  }

  // Function to setup video player
  function setupVideoPlayer(video) {
    // Skip if already setup
    if (video.dataset.playbackRateSetup) return;
    video.dataset.playbackRateSetup = "true";

    // Listen for rate changes
    video.addEventListener("ratechange", function () {
      savePlaybackRate(video.playbackRate);
    });

    // Listen for play event to restore saved rate
    video.addEventListener(
      "play",
      function () {
        const savedRate = getSavedPlaybackRate();
        if (savedRate && savedRate !== video.playbackRate) {
          video.playbackRate = savedRate;
        }
      },
      { once: true }
    ); // Only set rate on first play

    // Also set rate if video is already playing
    if (!video.paused) {
      const savedRate = getSavedPlaybackRate();
      if (savedRate && savedRate !== video.playbackRate) {
        video.playbackRate = savedRate;
      }
    }
  }

  // Function to find and setup all video players
  function findAndSetupVideos() {
    const videos = document.querySelectorAll("video");
    videos.forEach(setupVideoPlayer);
  }

  // Setup when DOM is ready
  if (document.readyState === "loading") {
    document.addEventListener("DOMContentLoaded", findAndSetupVideos);
  } else {
    findAndSetupVideos();
  }

  // Watch for dynamically added videos
  const observer = new MutationObserver(function (mutations) {
    mutations.forEach(function (mutation) {
      mutation.addedNodes.forEach(function (node) {
        if (node.nodeName === "VIDEO") {
          setupVideoPlayer(node);
        } else if (node.querySelectorAll) {
          const videos = node.querySelectorAll("video");
          videos.forEach(setupVideoPlayer);
        }
      });
    });
  });

  // Start observing
  observer.observe(document.body, {
    childList: true,
    subtree: true,
  });
})();

// FA-CHECK ICON DISPLAY FIX
// Function to change fa-check icons from display:block to display:inline
function fixFaCheckIcons() {
  const faCheckIcons = document.querySelectorAll(
    'i.fa.fa-check[aria-hidden="true"]'
  );

  faCheckIcons.forEach(function (icon) {
    // Check if the icon has display: block style
    const computedStyle = window.getComputedStyle(icon);
    const inlineStyle = icon.style.display;

    // Check both computed style and inline style for display: block
    if (computedStyle.display === "block" || inlineStyle === "block") {
      icon.style.display = "inline";
    }
  });
}

// Run the fix when DOM is ready
if (document.readyState === "loading") {
  document.addEventListener("DOMContentLoaded", fixFaCheckIcons);
} else {
  fixFaCheckIcons();
}

// Also watch for dynamically added fa-check icons
const faCheckObserver = new MutationObserver(function (mutations) {
  mutations.forEach(function (mutation) {
    mutation.addedNodes.forEach(function (node) {
      if (node.nodeType === Node.ELEMENT_NODE) {
        // Check if the added node is a fa-check icon
        if (node.matches && node.matches('i.fa.fa-check[aria-hidden="true"]')) {
          if (
            window.getComputedStyle(node).display === "block" ||
            node.style.display === "block"
          ) {
            node.style.display = "inline";
          }
        }

        // Check for fa-check icons within the added node
        if (node.querySelectorAll) {
          const icons = node.querySelectorAll(
            'i.fa.fa-check[aria-hidden="true"]'
          );
          icons.forEach(function (icon) {
            if (
              window.getComputedStyle(icon).display === "block" ||
              icon.style.display === "block"
            ) {
              icon.style.display = "inline";
            }
          });
        }
      }
    });
  });
});

// Start observing for fa-check icons
faCheckObserver.observe(document.body, {
  childList: true,
  subtree: true,
});

// CHECKOUT PAYMENT PAGE LAYOUT FIX
// Fix layout issue on checkout payment page by changing column width from large-7 to large-6
// This addresses a visual layout problem where the payment form column is too wide
function fixCheckoutPaymentLayout() {
  // Only run on checkout payment pages
  if (!window.location.pathname.includes('/checkout/') ||
      !window.location.pathname.includes('/payment')) {
    return;
  }

  const targetDiv = document.querySelector('.large-7.columns');

  if (targetDiv && targetDiv.classList.contains('large-7')) {
    targetDiv.classList.replace('large-7', 'large-6');
  }
}

// Run the fix when DOM is ready
if (document.readyState === "loading") {
  document.addEventListener("DOMContentLoaded", fixCheckoutPaymentLayout);
} else {
  fixCheckoutPaymentLayout();
}

// Watch for dynamically added content (in case the element loads after initial page load)
if (window.location.pathname.includes('/checkout/') &&
    window.location.pathname.includes('/payment')) {
  const checkoutObserver = new MutationObserver(function (mutations) {
    const targetDiv = document.querySelector('.large-7.columns');
    if (targetDiv && targetDiv.classList.contains('large-7')) {
      fixCheckoutPaymentLayout();
      checkoutObserver.disconnect(); // Stop observing once we've made the fix
    }
  });

  checkoutObserver.observe(document.body, {
    childList: true,
    subtree: true,
  });
}

window.parent.postMessage(
  {
    type: "injectReady",
  },
  "*"
);

})();
</script>
        
    

    
        <div id="course_completion_code">
            
        </div>
    

    
        <script src="./AI Fluency_ Framework &amp; Foundations_files/axios.min.7ee4c66e95c6.js"></script>
        <script src="./AI Fluency_ Framework &amp; Foundations_files/api-service.aefb97567b43.js"></script>
    
    
        
        <script src="./AI Fluency_ Framework &amp; Foundations_files/coursePlatformSessionInactivity.944dacb51181.js"></script>
    
</div>
    <div id="clp__modal-overlay" class="clp__modal-overlay" style="display: none;">
      <div class="clp__modal-content">
        <button class="clp__modal-close" aria-label="Close modal">✕</button>
        <div class="clp__modal-header">
          <h3 class="clp__modal-title"></h3>
          <p class="clp__modal-description"></p>
          <span class="clp__preview-label">Course preview images</span>
        </div>
        <div class="clp__modal-body">
          <button class="clp__modal-nav clp__modal-prev" aria-label="Previous image"><span>‹</span></button>
          <div class="clp__modal-image-container">
            <img class="clp__modal-image" src="https://anthropic.skilljar.com/ai-fluency-framework-foundations" alt="">
          </div>
          <button class="clp__modal-nav clp__modal-next" aria-label="Next image"><span>›</span></button>
        </div>
        <div class="clp__modal-footer">
          <div class="clp__modal-footer-content">
            <span class="clp__modal-counter"></span>
            <div class="clp__modal-enroll">
              <p class="clp__modal-enroll-text">Ready to start learning?</p>
              <a href="https://anthropic.skilljar.com/checkout/17mii0ye0vhrr" class="clp__modal-enroll-btn">Enroll in Course</a>
            </div>
          </div>
        </div>
      </div>
    </div>
  





<script>
    document.addEventListener('DOMContentLoaded', function() {
      var $checkboxes = document.querySelectorAll('input[type=checkbox]');
      for (var checkbox of $checkboxes) {
        checkbox.addEventListener('keydown', handleCheckboxEnter);
      }

      
      var $checkboxesInForm = document.querySelectorAll('form input[type=checkbox]');
      for (var checkbox of $checkboxesInForm) {
        checkbox.removeEventListener('keydown', handleCheckboxEnter);
      }

      function handleCheckboxEnter(e) {
        if (e.keyCode !== 13) {
          return;
        }

        e.target.click();
      }

      document.querySelectorAll('.focus-link-v2, button, a.button, a.coursebox-container, input[type="checkbox"], .filter-group-title').forEach(function(element) {
        element.addEventListener('mousedown', function(event) {
          event.preventDefault();
        });
      });
      document.querySelectorAll('label').forEach(function(element) {
        if (element.querySelector('input[type="checkbox"]')) {
          element.addEventListener('mouseup', function() {
            setTimeout(function() {
              element.querySelector('input[type="checkbox"]').blur();
            }, 0);
          });
        }
      });
    });
</script>







<script>
    var sjlpGeneral = {
        'general_close': 'Close',
        'lesson_page_burger_title': 'Lessons',
        'storefront_course_count': ['course_count', ['{course_count} {courses_name_singular}','{course_count} {courses_name_plural}']],
        'general_error_message': 'Uh oh! Something went wrong, please try again.',
    };

    $.extend(sjlpLanguagePack, sjlpGeneral);
</script>





<iframe scrolling="no" frameborder="0" allowtransparency="true" src="./AI Fluency_ Framework &amp; Foundations_files/widget_iframe.2f70fb173b9000da126c79afe2098f02.html" title="Twitter settings iframe" style="display: none;" data-gtm-yt-inspected-8="true"></iframe><iframe id="rufous-sandbox" scrolling="no" frameborder="0" allowtransparency="true" allowfullscreen="true" style="position: absolute; visibility: hidden; display: none; width: 0px; height: 0px; padding: 0px; border: none;" data-gtm-yt-inspected-8="true" title="Twitter analytics iframe" src="./AI Fluency_ Framework &amp; Foundations_files/saved_resource.html"></iframe><div id="fb-root" class=" fb_reset"><div style="position: absolute; top: -10000px; width: 0px; height: 0px;"><div></div></div></div></body></html>